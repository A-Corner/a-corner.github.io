<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>深度学习数学基础入门 - A-Acorner 信息的一角</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="A-Acorner 信息的一角"><meta name="msapplication-TileImage" content="/img/一角favicon.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="A-Acorner 信息的一角"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="前言   为了学习，也为了看懂深度学习中的定律、函数、方程等，所以狠心的整理回顾了一下涉及到深度学习中的一些数学基础知识。 知识列表：I. 预备知识：基础数学概念回顾  集合 集合的定义与表示 (列举法、描述法) 集合的基本关系 (子集、真子集、空集、全集) 集合的基本运算 (并集、交集、补集、差集) 韦恩图 (Venn Diagram) 的理解与应用"><meta property="og:type" content="blog"><meta property="og:title" content="深度学习数学基础入门"><meta property="og:url" content="http://acorner.ac.cn/2025/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"><meta property="og:site_name" content="A-Acorner 信息的一角"><meta property="og:description" content="前言   为了学习，也为了看懂深度学习中的定律、函数、方程等，所以狠心的整理回顾了一下涉及到深度学习中的一些数学基础知识。 知识列表：I. 预备知识：基础数学概念回顾  集合 集合的定义与表示 (列举法、描述法) 集合的基本关系 (子集、真子集、空集、全集) 集合的基本运算 (并集、交集、补集、差集) 韦恩图 (Venn Diagram) 的理解与应用"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://acorner.ac.cn/img/og_image.png"><meta property="article:published_time" content="2025-02-20T06:44:10.490Z"><meta property="article:modified_time" content="2025-02-20T07:31:53.093Z"><meta property="article:author" content="ViniJack.SJX"><meta property="article:tag" content="LLM"><meta property="article:tag" content="原理"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="数学"><meta property="article:tag" content="概率"><meta property="article:tag" content="入门"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://acorner.ac.cn/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://acorner.ac.cn/2025/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"},"headline":"深度学习数学基础入门","image":["http://acorner.ac.cn/img/og_image.png"],"datePublished":"2025-02-20T06:44:10.490Z","dateModified":"2025-02-20T07:31:53.093Z","author":{"@type":"Person","name":"ViniJack.SJX"},"publisher":{"@type":"Organization","name":"A-Acorner 信息的一角","logo":{"@type":"ImageObject","url":"http://acorner.ac.cn/img/一角logo.png"}},"description":"前言   为了学习，也为了看懂深度学习中的定律、函数、方程等，所以狠心的整理回顾了一下涉及到深度学习中的一些数学基础知识。 知识列表：I. 预备知识：基础数学概念回顾  集合 集合的定义与表示 (列举法、描述法) 集合的基本关系 (子集、真子集、空集、全集) 集合的基本运算 (并集、交集、补集、差集) 韦恩图 (Venn Diagram) 的理解与应用"}</script><link rel="canonical" href="http://acorner.ac.cn/2025/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"><link rel="icon" href="/img/%E4%B8%80%E8%A7%92favicon.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link data-pjax rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-02-20T06:44:10.490Z" title="2025/2/20 14:44:10">2025-02-20</time>发表</span><span class="level-item"><time dateTime="2025-02-20T07:31:53.093Z" title="2025/2/20 15:31:53">2025-02-20</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/LLM/">LLM</a><span> / </span><a class="link-muted" href="/categories/%E5%8E%9F%E7%90%86/">原理</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></span><span class="level-item">2 小时读完 (大约21966个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">深度学习数学基础入门</h1><div class="content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>   为了学习，也为了看懂深度学习中的定律、函数、方程等，所以狠心的整理回顾了一下涉及到深度学习中的一些数学基础知识。</p>
<h1 id="知识列表："><a href="#知识列表：" class="headerlink" title="知识列表："></a>知识列表：</h1><p><strong>I. 预备知识：基础数学概念回顾</strong></p>
<ol>
<li><strong>集合</strong><ul>
<li>集合的定义与表示 (列举法、描述法)</li>
<li>集合的基本关系 (子集、真子集、空集、全集)</li>
<li>集合的基本运算 (并集、交集、补集、差集)</li>
<li>韦恩图 (Venn Diagram) 的理解与应用</li>
</ul>
</li>
</ol>
<span id="more"></span>

<ol start="2">
<li><p><strong>函数</strong></p>
<ul>
<li>函数的定义 (映射、定义域、值域、对应法则)</li>
<li>函数的表示 (解析式、图像法、列表法)</li>
<li>函数的性质 (单调性、奇偶性、周期性)</li>
<li>基本初等函数<ul>
<li>线性函数 (一次函数)</li>
<li>二次函数</li>
<li>幂函数</li>
<li>指数函数</li>
<li>对数函数</li>
<li>三角函数 (正弦、余弦、正切)</li>
<li>反三角函数 (反正弦、反余弦、反正切)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>方程与不等式</strong></p>
<ul>
<li>一元一次方程及其解法</li>
<li>一元二次方程及其解法 (公式法、因式分解法、配方法)</li>
<li>根与系数的关系 (韦达定理)</li>
<li>不等式的性质</li>
<li>一元一次不等式(组)及其解法</li>
<li>一元二次不等式及其解法</li>
<li>绝对值不等式</li>
<li>分式不等式</li>
</ul>
</li>
<li><p><strong>数列</strong></p>
<ul>
<li>数列的定义</li>
<li>等差数列<ul>
<li>定义与通项公式</li>
<li>前 n 项和公式</li>
</ul>
</li>
<li>等比数列<ul>
<li>定义与通项公式</li>
<li>前 n 项和公式</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>II. 线性代数基础</strong></p>
<ol>
<li><p><strong>向量</strong></p>
<ul>
<li>向量的定义 (几何向量、代数向量)</li>
<li>向量的表示 (坐标表示、列向量、行向量)</li>
<li>向量的运算<ul>
<li>向量加法 (平行四边形法则、三角形法则)</li>
<li>向量数乘</li>
<li>向量点积 (内积)</li>
<li>向量叉积 (外积) (仅限三维向量)</li>
</ul>
</li>
<li>向量的范数<ul>
<li>L1 范数</li>
<li>L2 范数 (欧几里得范数)</li>
<li>Lp 范数</li>
</ul>
</li>
<li>线性相关与线性无关</li>
<li>向量空间、子空间</li>
<li>基、维数、坐标</li>
</ul>
</li>
<li><p><strong>矩阵</strong></p>
<ul>
<li>矩阵的定义</li>
<li>矩阵的表示</li>
<li>特殊矩阵<ul>
<li>零矩阵</li>
<li>方阵</li>
<li>单位矩阵</li>
<li>对角矩阵</li>
<li>上三角矩阵</li>
<li>下三角矩阵</li>
<li>对称矩阵</li>
<li>反对称矩阵</li>
</ul>
</li>
<li>矩阵的运算<ul>
<li>矩阵加法</li>
<li>矩阵数乘</li>
<li>矩阵乘法 (矩阵与向量的乘法、矩阵与矩阵的乘法)</li>
<li>矩阵的转置</li>
</ul>
</li>
<li>矩阵的行列式<ul>
<li>二阶行列式</li>
<li>三阶行列式</li>
<li>n 阶行列式</li>
<li>行列式的性质</li>
</ul>
</li>
<li>矩阵的逆<ul>
<li>逆矩阵的定义</li>
<li>逆矩阵的性质</li>
<li>逆矩阵的求法 (伴随矩阵法、初等变换法)</li>
</ul>
</li>
<li>矩阵的秩</li>
</ul>
</li>
<li><p><strong>线性方程组</strong></p>
<ul>
<li>线性方程组的表示 (系数矩阵、增广矩阵)</li>
<li>线性方程组的解<ul>
<li>唯一解</li>
<li>无穷多解</li>
<li>无解</li>
</ul>
</li>
<li>高斯消元法 (初等行变换)</li>
<li>克拉默法则 (Cramer’s Rule)</li>
</ul>
</li>
<li><p><strong>特征值与特征向量</strong></p>
<ul>
<li>特征值与特征向量的定义</li>
<li>特征值与特征向量的计算</li>
<li>特征多项式</li>
<li>特征空间</li>
<li>特征值分解 (谱分解)</li>
<li>特征值与特征向量的几何意义</li>
</ul>
</li>
<li><p><strong>奇异值分解 (SVD)</strong></p>
<ul>
<li>SVD 的定义</li>
<li>SVD的计算（进阶）</li>
<li>SVD 的应用 (数据压缩、降维、推荐系统)</li>
</ul>
</li>
</ol>
<p><strong>III. 微积分基础</strong></p>
<ol start="6">
<li><p><strong>导数</strong></p>
<ul>
<li>导数的定义 (极限形式)</li>
<li>导数的几何意义 (切线斜率、瞬时变化率)</li>
<li>常见函数的导数<ul>
<li>常数函数</li>
<li>幂函数</li>
<li>指数函数</li>
<li>对数函数</li>
<li>三角函数</li>
<li>反三角函数</li>
</ul>
</li>
<li>导数法则<ul>
<li>加法法则</li>
<li>减法法则</li>
<li>乘法法则</li>
<li>除法法则</li>
<li>链式法则 (复合函数求导)</li>
</ul>
</li>
<li>高阶导数</li>
</ul>
</li>
<li><p><strong>偏导数</strong></p>
<ul>
<li>偏导数的定义</li>
<li>多元函数的偏导数</li>
<li>梯度向量</li>
<li>方向导数</li>
<li>全微分</li>
</ul>
</li>
<li><p><strong>梯度下降法</strong></p>
<ul>
<li>梯度下降法的原理</li>
<li>梯度下降法的步骤</li>
<li>学习率 (步长) 的作用</li>
<li>局部最小值、全局最小值</li>
<li>鞍点</li>
</ul>
</li>
<li><p><strong>泰勒展开</strong></p>
<ul>
<li>泰勒公式 (Taylor’s Formula)</li>
<li>麦克劳林公式 (Maclaurin’s Formula)</li>
<li>一阶泰勒展开 (线性近似)</li>
<li>二阶泰勒展开 (二次近似)</li>
</ul>
</li>
<li><p><strong>凸优化(选学)</strong><br>*   凸集<br>*   凸函数<br>*   凸优化问题<br>*   凸优化的性质（局部最优解即全局最优解）</p>
</li>
</ol>
<p><strong>IV. 概率论与数理统计基础</strong></p>
<ol start="11">
<li><p><strong>随机事件与概率</strong><br>*   随机试验<br>*   样本空间<br>*   随机事件<br>*   事件的关系与运算 (包含、相等、并、交、差、互斥、对立)<br>*   概率的定义 (古典概型、几何概型、频率定义、公理化定义)<br>*   概率的性质 (非负性、规范性、可加性)<br>*   条件概率<br>*   事件的独立性<br>*   全概率公式<br>*   贝叶斯公式 (Bayes’ Theorem)</p>
</li>
<li><p><strong>随机变量</strong><br>*   随机变量的定义<br>*   离散型随机变量</p>
<ul>
<li>概率分布列</li>
<li>伯努利分布 (0-1 分布)</li>
<li>二项分布</li>
<li>泊松分布<br>*   连续型随机变量</li>
<li>概率密度函数 (PDF)</li>
<li>均匀分布</li>
<li>指数分布</li>
<li>正态分布 (高斯分布)<br>*   随机变量的数字特征</li>
<li>期望 (均值)</li>
<li>方差</li>
<li>标准差</li>
<li>协方差</li>
<li>相关系数</li>
</ul>
</li>
<li><p><strong>最大似然估计 (MLE)</strong><br>*   似然函数<br>*   最大似然估计的原理<br>*   最大似然估计的步骤<br>*   最大似然估计的应用</p>
</li>
<li><p><strong>信息熵与交叉熵</strong></p>
<ul>
<li>信息熵定义与公式</li>
<li>交叉熵定义与公式</li>
<li>相对熵（KL散度）</li>
<li>信息熵，交叉熵，相对熵在深度学习中的应用</li>
</ul>
</li>
</ol>
<hr>
<h3 id="1-集合"><a href="#1-集合" class="headerlink" title="1. 集合"></a>1. 集合</h3><p><strong>1.1 什么是集合？</strong></p>
<ul>
<li><strong>通俗定义：</strong>  把一些东西放在一起，就形成了一个集合。这些“东西”可以是任何事物，比如数字、字母、人、物体，甚至是其他集合。</li>
<li><strong>数学定义：</strong>  具有某种特定性质的、确定的、互不相同的事物的总体，称为<strong>集合</strong> (Set)。 集合中的每个事物称为<strong>元素</strong> (Element)。</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li>所有的英文字母可以组成一个集合：{a, b, c, …, z}</li>
<li>所有小于10的正整数可以组成一个集合：{1, 2, 3, 4, 5, 6, 7, 8, 9}</li>
<li>你今天穿的所有衣物可以组成一个集合：{衬衫, 裤子, 袜子, 鞋子}</li>
<li>一个班级里所有的学生可以组成一个集合。</li>
</ul>
<p><strong>要点：</strong></p>
<ul>
<li><strong>确定性：</strong>  一个元素是否属于某个集合，必须是明确的，不能模棱两可。例如，“好人”不能组成一个集合，因为“好”的标准不确定。</li>
<li><strong>互异性：</strong>  集合中的元素必须是互不相同的。例如，{1, 2, 2, 3} 不是一个集合，应该写成 {1, 2, 3}。</li>
<li><strong>无序性：</strong> 集合中的元素没有先后顺序。{1, 2, 3} 和 {3, 1, 2} 是同一个集合。</li>
</ul>
<p><strong>1.2 集合的表示</strong></p>
<ul>
<li><strong>列举法：</strong>  把集合中的所有元素一一列举出来，写在大括号 {} 内。<ul>
<li>例如：{1, 2, 3, 4, 5}</li>
</ul>
</li>
<li><strong>描述法：</strong>  用集合中元素的共同特征来描述集合。<ul>
<li>例如：{x | x 是小于 10 的正整数} (读作“x 属于实数集，且 x 是小于 10 的正整数”)。竖线 “|” 前面表示元素的形式，后面表示元素的特征。</li>
<li>更复杂的例子：{ (x, y) | x² + y² &#x3D; 1 } (表示单位圆上的所有点)</li>
</ul>
</li>
<li><strong>特殊符号：</strong><ul>
<li><strong>N:</strong> 自然数集 (非负整数集) {0, 1, 2, 3, …}</li>
<li><strong>N+ 或 N*:</strong> 正整数集 {1, 2, 3, …}</li>
<li><strong>Z:</strong> 整数集 {…, -2, -1, 0, 1, 2, …}</li>
<li><strong>Q:</strong> 有理数集 (可以表示成两个整数之比的数)</li>
<li><strong>R:</strong> 实数集 (包括有理数和无理数)</li>
<li><strong>C:</strong> 复数集</li>
<li><strong>∅:</strong> 空集, 不包含任何元素的集合</li>
</ul>
</li>
</ul>
<p><strong>1.3 集合的基本关系</strong></p>
<ul>
<li><strong>子集 (Subset)：</strong> 如果集合 A 的所有元素都属于集合 B，那么 A 是 B 的子集，记作 A ⊆ B (或 B ⊇ A)。<ul>
<li>例如：{1, 2} ⊆ {1, 2, 3}</li>
<li>任何集合都是它本身的子集：A ⊆ A</li>
<li>空集是任何集合的子集：∅ ⊆ A</li>
</ul>
</li>
<li><strong>真子集 (Proper Subset)：</strong> 如果 A 是 B 的子集，且 A ≠ B (A 中至少有一个元素不属于 B)，那么 A 是 B 的真子集，记作 A ⊂ B (或 B ⊃ A)。<ul>
<li>例如：{1, 2} ⊂ {1, 2, 3}</li>
</ul>
</li>
<li><strong>空集 (Empty Set)：</strong> 不包含任何元素的集合，记作 ∅。<ul>
<li>空集是任何集合的子集。</li>
<li>空集是唯一的。</li>
</ul>
</li>
<li><strong>全集 (Universal Set)：</strong> 在某个特定问题中，所有研究的对象都属于一个最大的集合，这个集合称为全集，通常记作 U。<ul>
<li>例如，在研究整数时，全集可以是整数集 Z。</li>
</ul>
</li>
</ul>
<p><strong>1.4 集合的基本运算</strong></p>
<ul>
<li><strong>并集 (Union)：</strong> 由所有属于集合 A 或属于集合 B 的元素组成的集合，记作 A ∪ B。<ul>
<li>A ∪ B &#x3D; {x | x ∈ A 或 x ∈ B}</li>
<li>例如：{1, 2, 3} ∪ {3, 4, 5} &#x3D; {1, 2, 3, 4, 5}</li>
</ul>
</li>
<li><strong>交集 (Intersection)：</strong> 由所有既属于集合 A 又属于集合 B 的元素组成的集合，记作 A ∩ B。<ul>
<li>A ∩ B &#x3D; {x | x ∈ A 且 x ∈ B}</li>
<li>例如：{1, 2, 3} ∩ {3, 4, 5} &#x3D; {3}</li>
</ul>
</li>
<li><strong>补集 (Complement)：</strong> 如果 A 是全集 U 的一个子集，由所有不属于 A 的元素组成的集合，称为 A 在 U 中的补集 (或余集)，记作 ∁UA 或 A’。<ul>
<li>∁UA &#x3D; {x | x ∈ U 且 x ∉ A}</li>
<li>例如：如果 U &#x3D; {1, 2, 3, 4, 5}，A &#x3D; {1, 2, 3}，那么 ∁UA &#x3D; {4, 5}</li>
</ul>
</li>
<li><strong>差集(Difference):</strong> 所有属于A且不属于B的元素组成的集合, 记作A\B<ul>
<li>A\B &#x3D; {x | x∈A 且 x∉B}</li>
<li>例如：{1,2,3} \ {3,4} &#x3D; {1,2}</li>
</ul>
</li>
</ul>
<p><strong>1.5 韦恩图 (Venn Diagram)</strong></p>
<ul>
<li>韦恩图是用平面上的圆形 (或其他封闭曲线) 来表示集合的一种图形。</li>
<li>可以直观地表示集合之间的关系 (子集、真子集) 和运算 (并集、交集、补集)。</li>
<li>在表示集合运算时非常有用。</li>
</ul>
<p>以下是使用Mermaid代码绘制韦恩图的示例：</p>
<pre class="mermaid">graph LR
    subgraph U
    A((A))
    B((B))
    end
    A --> |A ∪ B| C{并集}
    A --> |A ∩ B| D{交集}
    A --> |∁UA| E{补集}</pre>


<p>集合是数学中最基本的概念之一，也是学习深度学习数学基础的起点。理解集合的概念、表示方法、关系和运算，对于后续理解概率论等概念至关重要。</p>
<hr>
<h3 id="2-函数"><a href="#2-函数" class="headerlink" title="2. 函数"></a>2. 函数</h3><p><strong>函数</strong>。 函数是现代数学、乃至整个科学的基石。在深度学习中，从数据到预测结果的映射，本质上就是一个复杂的函数。</p>
<p><strong>2.1 什么是函数？</strong></p>
<ul>
<li><p><strong>通俗定义:</strong>  函数就像一个“黑盒子”，你给它一个输入（称为自变量），它会根据某种规则进行处理，然后给你一个输出（称为因变量）。</p>
</li>
<li><p><strong>数学定义:</strong>  设 A 和 B 是两个非空集合，如果按照某种确定的对应关系 f，对于集合 A 中的任意一个元素 x，在集合 B 中都有唯一确定的元素 y 与之对应，那么就把这种对应关系 f 叫做定义在集合 A 上的一个<strong>函数</strong> (Function)，记作 y &#x3D; f(x)。</p>
<ul>
<li>x 称为<strong>自变量</strong> (Independent Variable)。</li>
<li>y 称为<strong>因变量</strong> (Dependent Variable)。</li>
<li>A 称为函数的<strong>定义域</strong> (Domain)，即所有可能的输入 x 的集合。</li>
<li>所有可能的输出 y 的集合称为函数的<strong>值域</strong> (Range)。</li>
<li>f 是<strong>对应法则</strong>，它规定了输入 x 如何映射到输出 y。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li><strong>自动售货机:</strong> 你投入硬币 (输入)，售货机根据你的选择给你相应的商品 (输出)。</li>
<li><strong>计算器:</strong> 你输入一个数字和一个运算符号 (如 +2)，计算器会计算出结果 (输出)。</li>
<li><strong>身高预测:</strong> 根据父母的身高 (输入)，可以预测子女的身高 (输出) —— 当然，这只是一个粗略的估计。</li>
<li><strong>y&#x3D;2x+1</strong>。这是一个常见的数学函数。例如输入2，输出5。</li>
</ul>
<p><strong>要点：</strong></p>
<ul>
<li><strong>一一对应：</strong> 函数要求对于每一个输入 x，都必须有唯一的输出 y 与之对应。一个 x 不能对应多个 y（但多个 x 可以对应同一个 y）。</li>
<li><strong>定义域和值域：</strong> 定义域是所有可能的输入的集合，值域是所有可能的输出的集合。</li>
</ul>
<p><strong>2.2 函数的表示</strong></p>
<ul>
<li><p><strong>解析式 (公式法)：</strong> 用数学表达式来表示函数。</p>
<ul>
<li>例如：y &#x3D; 2x + 1,  f(x) &#x3D; x²,  g(t) &#x3D; sin(t)</li>
</ul>
</li>
<li><p><strong>图像法：</strong> 用图形来表示函数。在平面直角坐标系中，以自变量 x 为横坐标，因变量 y 为纵坐标，画出函数的图像。</p>
<ul>
<li>例如，y &#x3D; 2x + 1 的图像是一条直线。</li>
<li>例如，y &#x3D; x² 的图像是抛物线。</li>
</ul>
<p>下面是使用Matplotlib绘制这两个函数的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 x 值</span></span><br><span class="line">x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">100</span>)  <span class="comment"># 从 -5 到 5 生成 100 个点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 y 值</span></span><br><span class="line">y1 = <span class="number">2</span> * x + <span class="number">1</span></span><br><span class="line">y2 = x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))  <span class="comment"># 设置图形大小</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 创建子图，2 行 1 列，当前为第 1 个子图</span></span><br><span class="line">plt.plot(x, y1)</span><br><span class="line">plt.title(<span class="string">&#x27;y = 2x + 1&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 创建子图，2 行 1 列，当前为第 2 个子图</span></span><br><span class="line">plt.plot(x, y2)</span><br><span class="line">plt.title(<span class="string">&#x27;y = x²&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()  <span class="comment"># 自动调整子图布局</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>列表法：</strong> 用表格来表示函数。表格中列出一些自变量的值和对应的因变量的值。</p>
<ul>
<li>例如：<table>
<thead>
<tr>
<th>x</th>
<th>y &#x3D; 2x + 1</th>
</tr>
</thead>
<tbody><tr>
<td>-1</td>
<td>-1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<p><strong>2.3 函数的性质</strong></p>
<ul>
<li><strong>单调性 (Monotonicity)：</strong> 描述函数值随自变量变化而变化的情况。<ul>
<li><strong>单调递增：</strong> 在定义域内，如果 x₁ &lt; x₂，则 f(x₁) &lt; f(x₂)，那么函数是单调递增的。</li>
<li><strong>单调递减：</strong> 在定义域内，如果 x₁ &lt; x₂，则 f(x₁) &gt; f(x₂)，那么函数是单调递减的。</li>
</ul>
</li>
<li><strong>奇偶性 (Parity)：</strong> 描述函数关于 y 轴或原点对称的情况。<ul>
<li><strong>偶函数：</strong> 对于定义域内的任意 x，都有 f(-x) &#x3D; f(x)，那么函数是偶函数 (图像关于 y 轴对称)。<ul>
<li>例如：f(x) &#x3D; x²</li>
</ul>
</li>
<li><strong>奇函数：</strong> 对于定义域内的任意 x，都有 f(-x) &#x3D; -f(x)，那么函数是奇函数 (图像关于原点对称)。<ul>
<li>例如：f(x) &#x3D; x³</li>
</ul>
</li>
</ul>
</li>
<li><strong>周期性 (Periodicity)：</strong> 描述函数值周期性重复出现的情况。<ul>
<li>对于定义域内的任意 x，如果存在一个常数 T ≠ 0，使得 f(x + T) &#x3D; f(x)，那么函数是周期函数，T 称为函数的周期。</li>
<li>例如：f(x) &#x3D; sin(x) 是周期函数，周期为 2π。</li>
</ul>
</li>
</ul>
<p><strong>2.4 基本初等函数</strong></p>
<p>在深度学习中经常遇到的一些基本函数：</p>
<ul>
<li><strong>线性函数 (一次函数):</strong>  y &#x3D; kx + b (k 和 b 是常数，k ≠ 0)。图像是一条直线。</li>
<li><strong>二次函数:</strong>  y &#x3D; ax² + bx + c (a, b, c 是常数, a ≠ 0)。图像是抛物线。</li>
<li><strong>幂函数:</strong>  y &#x3D; xᵃ (a 是常数)。</li>
<li><strong>指数函数:</strong>  y &#x3D; aˣ (a 是常数, a &gt; 0 且 a ≠ 1)。<ul>
<li>当 a &gt; 1 时，指数函数单调递增。</li>
<li>当 0 &lt; a &lt; 1 时，指数函数单调递减。</li>
<li>一个非常重要的指数函数：自然指数函数 y &#x3D; eˣ，其中 e ≈ 2.71828… 是一个无理数 (自然对数的底)。</li>
</ul>
</li>
<li><strong>对数函数:</strong>  y &#x3D; logₐx (a 是常数, a &gt; 0 且 a ≠ 1)。对数函数是指数函数的反函数。<ul>
<li>当 a &gt; 1 时，对数函数单调递增。</li>
<li>当 0 &lt; a &lt; 1 时，对数函数单调递减。</li>
<li>自然对数函数 y &#x3D; ln x，是以 e 为底的对数函数 (logₑx)。</li>
</ul>
</li>
<li><strong>三角函数:</strong>  正弦函数 y &#x3D; sin(x), 余弦函数 y &#x3D; cos(x), 正切函数 y &#x3D; tan(x)。</li>
<li>**反三角函数:**反正弦函数，反余弦函数，反正切函数</li>
</ul>
<p>函数是描述输入和输出之间关系的数学工具，对函数的理解，是构建深度学习模型的基础。</p>
<hr>
<h3 id="3-方程与不等式"><a href="#3-方程与不等式" class="headerlink" title="3. 方程与不等式"></a>3. 方程与不等式</h3><p><strong>方程与不等式</strong>。 方程与不等式是处理数量关系的重要工具，在深度学习中，它们被用于模型参数的求解、优化问题的约束条件等方面。</p>
<p><strong>3.1 方程</strong></p>
<ul>
<li><p><strong>什么是方程？</strong> 含有未知数的等式叫做<strong>方程</strong> (Equation)。</p>
<ul>
<li><strong>未知数：</strong> 通常用字母 x, y, z 等表示。</li>
<li><strong>等式：</strong> 用等号 “&#x3D;” 连接的式子。</li>
<li><strong>方程的解：</strong> 使方程左右两边相等的未知数的值。</li>
<li><strong>解方程：</strong> 求出方程的解的过程。</li>
</ul>
</li>
<li><p><strong>一元一次方程：</strong> 只含有一个未知数，并且未知数的次数是 1 的方程。</p>
<ul>
<li>一般形式：ax + b &#x3D; 0 (a, b 是常数, a ≠ 0)</li>
<li>解法：移项、合并同类项、系数化为 1。</li>
<li>例如：2x + 3 &#x3D; 7  &#x3D;&gt;  2x &#x3D; 4  &#x3D;&gt;  x &#x3D; 2</li>
</ul>
</li>
<li><p><strong>一元二次方程：</strong> 只含有一个未知数，并且未知数的最高次数是 2 的方程。</p>
<ul>
<li>一般形式：ax² + bx + c &#x3D; 0 (a, b, c 是常数, a ≠ 0)</li>
<li><strong>解法：</strong><ul>
<li><strong>公式法：</strong> 求根公式  x &#x3D; (-b ± √(b² - 4ac)) &#x2F; 2a<ul>
<li>判别式 Δ &#x3D; b² - 4ac<ul>
<li>Δ &gt; 0：方程有两个不相等的实数根。</li>
<li>Δ &#x3D; 0：方程有两个相等的实数根 (重根)。</li>
<li>Δ &lt; 0：方程没有实数根 (有两个共轭复数根)。</li>
</ul>
</li>
</ul>
</li>
<li><strong>因式分解法：</strong> 将方程左边分解成两个一次因式的乘积。<ul>
<li>例如：x² - 5x + 6 &#x3D; 0  &#x3D;&gt;  (x - 2)(x - 3) &#x3D; 0  &#x3D;&gt;  x &#x3D; 2 或 x &#x3D; 3</li>
</ul>
</li>
<li><strong>配方法：</strong> 将方程左边配成一个完全平方的形式。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>根与系数的关系 (韦达定理)：</strong> 对于一元二次方程 ax² + bx + c &#x3D; 0 (a ≠ 0)，如果方程有两个根 x₁ 和 x₂，那么：</p>
<ul>
<li>x₁ + x₂ &#x3D; -b&#x2F;a</li>
<li>x₁ * x₂ &#x3D; c&#x2F;a</li>
</ul>
</li>
</ul>
<p><strong>3.2 不等式</strong></p>
<ul>
<li><strong>什么是不等式？</strong> 用不等号 (&gt;, &lt;, ≥, ≤, ≠) 连接的式子叫做<strong>不等式</strong> (Inequality)。</li>
<li><strong>不等式的性质：</strong><ul>
<li>不等式的两边同时加上或减去同一个数，不等号的方向不变。</li>
<li>不等式的两边同时乘以或除以同一个正数，不等号的方向不变。</li>
<li>不等式的两边同时乘以或除以同一个负数，不等号的方向改变。</li>
</ul>
</li>
<li><strong>一元一次不等式：</strong> 只含有一个未知数，并且未知数的次数是 1 的不等式。<ul>
<li>一般形式：ax + b &gt; 0, ax + b &lt; 0, ax + b ≥ 0, ax + b ≤ 0 (a, b 是常数, a ≠ 0)</li>
<li>解法：与解一元一次方程类似，但要注意乘以或除以负数时，不等号的方向要改变。</li>
</ul>
</li>
<li><strong>一元二次不等式：</strong> 只含有一个未知数，并且未知数的最高次数是 2 的不等式。<ul>
<li>一般形式：ax² + bx + c &gt; 0, ax² + bx + c &lt; 0, ax² + bx + c ≥ 0, ax² + bx + c ≤ 0 (a, b, c 是常数, a ≠ 0)</li>
<li>解法：通常先求出对应的一元二次方程的根，然后根据二次函数的图像 (抛物线) 来确定不等式的解集。</li>
<li>也可通过因式分解，转换成一元一次不等式组求解。</li>
</ul>
</li>
<li><strong>绝对值不等式：</strong> 含有绝对值符号的不等式。<ul>
<li>|x| &lt; a (a &gt; 0)  &lt;&#x3D;&gt;  -a &lt; x &lt; a</li>
<li>|x| &gt; a (a &gt; 0)  &lt;&#x3D;&gt;  x &lt; -a 或 x &gt; a</li>
</ul>
</li>
<li><strong>分式不等式：</strong> 分母中含有未知数的不等式。<ul>
<li>解法：通常先将分式不等式转化为整式不等式，然后求解。要注意分母不能为 0。</li>
</ul>
</li>
</ul>
<p>方程和不等式是解决各种数学问题的基本工具。了解其定义、类型和解法，对于理解深度学习中的优化过程非常重要。许多深度学习问题最终会转化为求解方程或不等式，或者在满足一定约束条件（不等式）下进行优化。</p>
<hr>
<h3 id="4-数列"><a href="#4-数列" class="headerlink" title="4. 数列"></a>4. 数列</h3><p><strong>数列</strong>。数列在深度学习中也有一定的应用，比如在处理时间序列数据、循环神经网络 (RNN) 中，就会涉及到数列的概念。</p>
<p><strong>4.1 什么是数列？</strong></p>
<ul>
<li><strong>定义:</strong>  按照一定次序排列的一列数叫做<strong>数列</strong> (Sequence)。<ul>
<li>数列中的每一个数叫做数列的<strong>项</strong> (Term)。</li>
<li>第一项通常称为<strong>首项</strong>，记作 a₁。</li>
<li>第 n 项称为数列的<strong>通项</strong>，记作 aₙ。</li>
<li>数列可以表示为 a₁, a₂, a₃, …, aₙ, …</li>
<li>数列可以是有穷的 (有限项)，也可以是无穷的 (无限项)。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li>1, 2, 3, 4, 5, … (正整数数列)</li>
<li>2, 4, 6, 8, 10, … (偶数数列)</li>
<li>1, 1&#x2F;2, 1&#x2F;3, 1&#x2F;4, 1&#x2F;5, … (倒数数列)</li>
<li>1, -1, 1, -1, 1, … (摆动数列)</li>
</ul>
<p><strong>4.2 等差数列</strong></p>
<ul>
<li><strong>定义:</strong>  如果一个数列从第 2 项起，每一项与它的前一项的差都等于同一个常数，那么这个数列叫做<strong>等差数列</strong> (Arithmetic Sequence 或 Arithmetic Progression)。<ul>
<li>这个常数叫做等差数列的<strong>公差</strong> (Common Difference)，通常用 d 表示。</li>
<li>通项公式：aₙ &#x3D; a₁ + (n - 1)d</li>
</ul>
</li>
<li><strong>等差数列求和：</strong><ul>
<li>前 n 项和公式：Sₙ &#x3D; (a₁ + aₙ)n &#x2F; 2 &#x3D; na₁ + n(n - 1)d &#x2F; 2</li>
<li>推导：可以利用“倒序相加”的方法推导。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li>1, 3, 5, 7, 9, … (公差 d &#x3D; 2)</li>
<li>10, 7, 4, 1, -2, … (公差 d &#x3D; -3)</li>
</ul>
<p><strong>4.3 等比数列</strong></p>
<ul>
<li><strong>定义:</strong>  如果一个数列从第 2 项起，每一项与它的前一项的比都等于同一个常数，那么这个数列叫做<strong>等比数列</strong> (Geometric Sequence 或 Geometric Progression)。<ul>
<li>这个常数叫做等比数列的<strong>公比</strong> (Common Ratio)，通常用 q 表示 (q ≠ 0)。</li>
<li>通项公式：aₙ &#x3D; a₁ * qⁿ⁻¹</li>
</ul>
</li>
<li><strong>等比数列求和：</strong><ul>
<li>前 n 项和公式：<ul>
<li>当 q ≠ 1 时，Sₙ &#x3D; a₁(1 - qⁿ) &#x2F; (1 - q)</li>
<li>当 q &#x3D; 1 时，Sₙ &#x3D; na₁</li>
</ul>
</li>
<li>推导：可以利用“错位相减”的方法推导。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li>2, 4, 8, 16, 32, … (公比 q &#x3D; 2)</li>
<li>1, 1&#x2F;2, 1&#x2F;4, 1&#x2F;8, 1&#x2F;16, … (公比 q &#x3D; 1&#x2F;2)</li>
</ul>
<p>数列是按照一定规律排列的数的集合。等差数列和等比数列是两种最基本的数列，它们在数学和实际应用中都有广泛的应用。理解数列的概念、通项公式和求和公式，对于后续学习会有帮助。</p>
<hr>
<h3 id="II-线性代数基础"><a href="#II-线性代数基础" class="headerlink" title="II. 线性代数基础"></a>II. 线性代数基础</h3><p>线性代数是深度学习的数学基础中非常重要的一个部分。深度学习中的数据通常表示为向量和矩阵，神经网络的运算也 মূলত是基于线性代数的运算。</p>
<p>我们从第一个小知识点开始：<strong>1. 向量</strong></p>
<p><strong>1.1 什么是向量？</strong></p>
<ul>
<li><strong>几何定义：</strong> 向量 (Vector) 是既有大小又有方向的量。可以用带箭头的线段来表示，箭头表示向量的方向，线段的长度表示向量的大小。<ul>
<li>在物理学中，力、速度、位移等都是向量。</li>
</ul>
</li>
<li><strong>代数定义：</strong> 向量是有序的一组数。<ul>
<li>例如：[1, 2, 3] 是一个三维向量。</li>
<li>[4, 5] 是一个二维向量。</li>
<li>[6] 是一个一维向量。</li>
</ul>
</li>
<li><strong>更广义的定义:</strong> 向量是向量空间中的元素。 (这个定义比较抽象，我们暂时可以不深究。)</li>
</ul>
<p><strong>向量的表示：</strong></p>
<ul>
<li><strong>几何表示：</strong> 在平面直角坐标系或空间直角坐标系中，用带箭头的线段表示。<ul>
<li>起点通常是原点 (0, 0) 或 (0, 0, 0)。</li>
<li>终点的坐标就是向量的坐标。</li>
</ul>
</li>
<li><strong>坐标表示：</strong><ul>
<li>二维向量：<strong>v</strong> &#x3D; [x, y] 或 <strong>v</strong> &#x3D; (x, y)</li>
<li>三维向量：<strong>v</strong> &#x3D; [x, y, z] 或 <strong>v</strong> &#x3D; (x, y, z)</li>
<li>n 维向量：<strong>v</strong> &#x3D; [v₁, v₂, …, vₙ] 或 <strong>v</strong> &#x3D; (v₁, v₂, …, vₙ)</li>
</ul>
</li>
<li><strong>列向量和行向量：</strong><ul>
<li><strong>列向量：</strong>  将向量的元素竖着排列。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[1]</span><br><span class="line">[2]</span><br><span class="line">[3]</span><br></pre></td></tr></table></figure></li>
<li><strong>行向量：</strong>  将向量的元素横着排列。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure></li>
<li>在深度学习中，通常使用列向量。</li>
</ul>
</li>
</ul>
<p><strong>1.2 向量的运算</strong></p>
<ul>
<li><strong>向量加法：</strong> 对应元素相加。<ul>
<li>几何意义：平行四边形法则或三角形法则。</li>
<li>代数表示：如果 <strong>u</strong> &#x3D; [u₁, u₂, …, uₙ]，<strong>v</strong> &#x3D; [v₁, v₂, …, vₙ]，那么 <strong>u</strong> + <strong>v</strong> &#x3D; [u₁ + v₁, u₂ + v₂, …, uₙ + vₙ]。</li>
</ul>
</li>
<li><strong>向量数乘：</strong> 向量的每个元素都乘以一个标量 (实数)。<ul>
<li>几何意义：向量的伸缩。如果标量大于 0，则向量方向不变；如果标量小于 0，则向量方向相反。</li>
<li>代数表示：如果 <strong>v</strong> &#x3D; [v₁, v₂, …, vₙ]，k 是一个标量，那么 k<strong>v</strong> &#x3D; [kv₁, kv₂, …, kvₙ]。</li>
</ul>
</li>
<li><strong>向量点积 (内积)：</strong> 两个向量对应元素相乘，然后求和。<ul>
<li>几何意义：一个向量在另一个向量上的投影长度乘以另一个向量的长度。点积的结果是一个标量。<ul>
<li>如果两个向量的点积为 0，则这两个向量正交 (垂直)。</li>
</ul>
</li>
<li>代数表示：如果 <strong>u</strong> &#x3D; [u₁, u₂, …, uₙ]，<strong>v</strong> &#x3D; [v₁, v₂, …, vₙ]，那么 <strong>u</strong> ⋅ <strong>v</strong> &#x3D; u₁v₁ + u₂v₂ + … + uₙvₙ。</li>
</ul>
</li>
<li><strong>向量叉积 (外积)：</strong> 仅适用于三维向量。<ul>
<li>几何意义：叉积的结果是一个向量，其方向垂直于两个原始向量所在的平面，其大小等于两个原始向量构成的平行四边形的面积。</li>
<li>代数表示：如果 <strong>u</strong> &#x3D; [u₁, u₂, u₃]，<strong>v</strong> &#x3D; [v₁, v₂, v₃]，那么 <strong>u</strong> × <strong>v</strong> &#x3D; [u₂v₃ - u₃v₂, u₃v₁ - u₁v₃, u₁v₂ - u₂v₁]。</li>
</ul>
</li>
</ul>
<p><strong>1.3 向量的范数</strong></p>
<ul>
<li><strong>范数 (Norm)：</strong> 衡量向量“大小”的度量。范数是一个函数，它将向量映射为一个非负实数。</li>
<li><strong>Lp 范数：</strong><ul>
<li>L1 范数：向量各个元素的绝对值之和。||<strong>v</strong>||₁ &#x3D; |v₁| + |v₂| + … + |vₙ|</li>
<li>L2 范数 (欧几里得范数)：向量各个元素的平方和的平方根。||<strong>v</strong>||₂ &#x3D; √(v₁² + v₂² + … + vₙ²)</li>
<li>Lp 范数：||<strong>v</strong>||ₚ &#x3D; (|v₁|ᵖ + |v₂|ᵖ + … + |vₙ|ᵖ)¹&#x2F;ᵖ (p ≥ 1)</li>
</ul>
</li>
</ul>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量定义</span></span><br><span class="line">v1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">v2 = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量加法:&quot;</span>, v1 + v2)  <span class="comment"># 输出：[5 7 9]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量数乘</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量数乘:&quot;</span>, <span class="number">2</span> * v1)  <span class="comment"># 输出：[2 4 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量点积</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量点积:&quot;</span>, np.dot(v1, v2))  <span class="comment"># 输出：32  (1*4 + 2*5 + 3*6 = 32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量叉积 (仅限三维向量)</span></span><br><span class="line">v3 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">v4 = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量叉积:&quot;</span>, np.cross(v3, v4))  <span class="comment"># 输出：[-3  6 -3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># L1 范数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L1 范数:&quot;</span>, np.linalg.norm(v1, <span class="built_in">ord</span>=<span class="number">1</span>))  <span class="comment"># 输出：6.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># L2 范数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L2 范数:&quot;</span>, np.linalg.norm(v1, <span class="built_in">ord</span>=<span class="number">2</span>))  <span class="comment"># 输出：3.7416573867739413</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Lp范数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L4 范数&quot;</span>, np.linalg.norm(v1, <span class="built_in">ord</span>=<span class="number">4</span>))</span><br></pre></td></tr></table></figure>


<p>向量是线性代数的基本元素，也是深度学习中表示数据的基本单位。理解向量的几何意义、代数表示、运算和范数，对于后续学习矩阵、线性变换等概念至关重要。</p>
<hr>
<h3 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2. 矩阵"></a>2. 矩阵</h3><p><strong>2.1 什么是矩阵？</strong></p>
<ul>
<li><strong>定义：</strong> 矩阵 (Matrix) 是一个由 m 行 n 列元素排列成的矩形阵列。<ul>
<li>矩阵中的元素可以是数字、符号或数学表达式。</li>
<li>m 行 n 列的矩阵称为 m × n 矩阵。</li>
<li>如果 m &#x3D; n，则称矩阵为方阵 (Square Matrix)。</li>
</ul>
</li>
</ul>
<p><strong>矩阵的表示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A = [ a₁₁ a₁₂ ... a₁ₙ ]   </span><br><span class="line">    [ a₂₁ a₂₂ ... a₂ₙ ]</span><br><span class="line">    [  .   .  ...  .  ]</span><br><span class="line">    [ aₘ₁ aₘ₂ ... aₘₙ ]</span><br></pre></td></tr></table></figure>

<ul>
<li>通常用大写字母表示矩阵，如 A, B, C。</li>
<li>aᵢⱼ 表示矩阵 A 中第 i 行第 j 列的元素。</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li><pre><code>[ 1  2 ]
[ 3  4 ]
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    是一个 2 × 2 矩阵 (方阵)。</span><br><span class="line">*   ```</span><br><span class="line">    [ 1  2  3 ]</span><br><span class="line">    [ 4  5  6 ]</span><br></pre></td></tr></table></figure>
是一个 2 × 3 矩阵。
</code></pre>
</li>
<li><pre><code>[ 1 ]
[ 2 ]
[ 3 ]
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    是一个 3 × 1 矩阵 (列向量)。</span><br><span class="line">*   ```</span><br><span class="line">    [ 1  2  3 ]</span><br></pre></td></tr></table></figure>
是一个 1 × 3 矩阵 (行向量)。
</code></pre>
</li>
</ul>
<p><strong>2.2 特殊矩阵</strong></p>
<ul>
<li><strong>零矩阵 (Zero Matrix)：</strong> 所有元素都为 0 的矩阵。</li>
<li><strong>方阵 (Square Matrix)：</strong> 行数和列数相等的矩阵。</li>
<li><strong>单位矩阵 (Identity Matrix)：</strong> 对角线上的元素都为 1，其余元素都为 0 的方阵。通常用 I 或 E 表示。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ 1  0  0 ]</span><br><span class="line">[ 0  1  0 ]</span><br><span class="line">[ 0  0  1 ]</span><br></pre></td></tr></table></figure></li>
<li><strong>对角矩阵 (Diagonal Matrix)：</strong> 除了对角线上的元素外，其余元素都为 0 的方阵。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ 2  0  0 ]</span><br><span class="line">[ 0  5  0 ]</span><br><span class="line">[ 0  0  -1 ]</span><br></pre></td></tr></table></figure></li>
<li><strong>上三角矩阵 (Upper Triangular Matrix)：</strong> 对角线以下的元素都为 0 的方阵。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[1 2 3]</span><br><span class="line">[0 4 5]</span><br><span class="line">[0 0 6]</span><br></pre></td></tr></table></figure></li>
<li><strong>下三角矩阵 (Lower Triangular Matrix)：</strong> 对角线以上的元素都为 0 的方阵。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ 1  0  0 ]</span><br><span class="line">[ 2  3  0 ]</span><br><span class="line">[ 4  5  6 ]</span><br></pre></td></tr></table></figure></li>
<li><strong>对称矩阵 (Symmetric Matrix)：</strong> 元素关于对角线对称的方阵 (aᵢⱼ &#x3D; aⱼᵢ)。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ 1  2  3 ]</span><br><span class="line">[ 2  4  5 ]</span><br><span class="line">[ 3  5  6 ]</span><br></pre></td></tr></table></figure></li>
<li><strong>反对称矩阵 (Skew-symmetric Matrix)：</strong> 元素关于对角线反对称的方阵 (aᵢⱼ &#x3D; -aⱼᵢ，且对角线元素都为 0)。</li>
</ul>
<p><strong>2.3 矩阵的运算</strong></p>
<ul>
<li><strong>矩阵加法：</strong> 对应元素相加 (只有相同大小的矩阵才能相加)。</li>
<li><strong>矩阵数乘：</strong> 矩阵的每个元素都乘以一个标量。</li>
<li><strong>矩阵乘法 (Matrix Multiplication)：</strong><ul>
<li>要求：第一个矩阵的列数必须等于第二个矩阵的行数。</li>
<li>结果：一个 m × n 矩阵乘以一个 n × p 矩阵，得到一个 m × p 矩阵。</li>
<li>计算：结果矩阵的第 i 行第 j 列的元素等于第一个矩阵的第 i 行与第二个矩阵的第 j 列的点积。</li>
<li><strong>不满足交换律：</strong> 通常 AB ≠ BA。</li>
<li><strong>满足结合律：</strong> (AB)C &#x3D; A(BC)。</li>
<li><strong>满足分配律：</strong> A(B + C) &#x3D; AB + AC, (A + B)C &#x3D; AC + BC。</li>
<li><strong>与单位矩阵相乘：</strong> AI &#x3D; IA &#x3D; A (A 是方阵)。</li>
</ul>
</li>
<li><strong>矩阵的转置(Transpose):</strong><ul>
<li>将矩阵的行和列互换。<ul>
<li>如果 A 是 m × n 矩阵，那么 A 的转置 (记作 Aᵀ 或 A’) 是一个 n × m 矩阵。</li>
<li>(Aᵀ)ᵢⱼ &#x3D; Aⱼᵢ</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵定义</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">B = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵加法:\n&quot;</span>, A + B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵数乘</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵数乘:\n&quot;</span>, <span class="number">2</span> * A)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵乘法:\n&quot;</span>, np.dot(A, B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵转置:\n&quot;</span>, A.T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单位矩阵</span></span><br><span class="line">I = np.eye(<span class="number">3</span>)  <span class="comment"># 3x3 单位矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;单位矩阵:\n&quot;</span>, I)</span><br></pre></td></tr></table></figure>



<p>矩阵是线性代数中的另一个核心概念。它们用于表示线性变换、存储数据等。理解矩阵的类型、运算（特别是矩阵乘法）对于深度学习至关重要，因为神经网络的每一层都可以看作是一个矩阵运算。</p>
<hr>
<h3 id="3-线性方程组"><a href="#3-线性方程组" class="headerlink" title="3. 线性方程组"></a>3. 线性方程组</h3><p><strong>3.1 什么是线性方程组？</strong></p>
<ul>
<li><strong>定义：</strong> 线性方程组 (System of Linear Equations) 是一组包含多个未知数、且每个方程都是线性的方程组。<ul>
<li>线性方程：未知数的次数都是 1 的方程。</li>
</ul>
</li>
</ul>
<p><strong>线性方程组的表示：</strong></p>
<ul>
<li><p><strong>一般形式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = b₁</span><br><span class="line">a₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = b₂</span><br><span class="line">...</span><br><span class="line">aₘ₁x₁ + aₘ₂x₂ + ... + aₘₙxₙ = bₘ</span><br></pre></td></tr></table></figure>
<p>其中：<br>*   x₁, x₂, …, xₙ 是未知数。<br>*   aᵢⱼ 是系数 (已知常数)。<br>*   bᵢ 是常数项 (已知常数)。</p>
</li>
<li><p><strong>矩阵形式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ax = b</span><br></pre></td></tr></table></figure>
<p>其中：<br>*   A 是 m × n 的系数矩阵 (由 aᵢⱼ 组成)。<br>*   x 是 n × 1 的未知数向量 (由 xᵢ 组成)。<br>*   b 是 m × 1 的常数项向量 (由 bᵢ 组成)。</p>
</li>
</ul>
<p><strong>举例：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2x + 3y = 7</span><br><span class="line">x - y = 1</span><br></pre></td></tr></table></figure>

<p>这是一个包含两个未知数 (x, y) 和两个方程的线性方程组。</p>
<p>用矩阵形式表示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 2  3 ] [ x ] = [ 7 ]</span><br><span class="line">[ 1 -1 ] [ y ] = [ 1 ]</span><br></pre></td></tr></table></figure>

<p><strong>3.2 线性方程组的解</strong></p>
<ul>
<li><strong>解：</strong> 满足线性方程组中所有方程的一组未知数的值。</li>
<li><strong>解的情况：</strong><ul>
<li><strong>唯一解：</strong> 方程组有且仅有一组解。</li>
<li><strong>无穷多解：</strong> 方程组有无限多组解。</li>
<li><strong>无解：</strong> 方程组没有解。</li>
</ul>
</li>
</ul>
<p><strong>3.3 高斯消元法 (Gaussian Elimination)</strong></p>
<ul>
<li><strong>高斯消元法：</strong> 一种求解线性方程组的常用方法。通过一系列的初等行变换，将增广矩阵化为行阶梯形矩阵或行最简形矩阵，从而求出方程组的解。</li>
<li><strong>增广矩阵：</strong> 将系数矩阵 A 和常数项向量 b 合并成一个矩阵 [A | b]。</li>
<li><strong>初等行变换：</strong><ol>
<li>交换两行的位置。</li>
<li>将某一行乘以一个非零常数。</li>
<li>将某一行的倍数加到另一行上。</li>
</ol>
</li>
<li><strong>行阶梯形矩阵 (Row Echelon Form)：</strong><ol>
<li>每一行的第一个非零元素 (称为主元) 必须是 1。</li>
<li>主元所在列的下方元素必须都为 0。</li>
<li>非零行必须在零行上方。</li>
</ol>
</li>
<li><strong>行最简形矩阵 (Reduced Row Echelon Form)：</strong><ol>
<li>满足行阶梯形矩阵的所有条件。</li>
<li>主元所在列的上方元素也必须都为 0。</li>
</ol>
</li>
</ul>
<p><strong>高斯消元法的步骤：</strong></p>
<ol>
<li>写出线性方程组的增广矩阵。</li>
<li>通过初等行变换，将增广矩阵化为行阶梯形矩阵。</li>
<li>如果行阶梯形矩阵中出现 [0 0 … 0 | b] (b ≠ 0) 的形式，则方程组无解。</li>
<li>否则，将行阶梯形矩阵化为行最简形矩阵。</li>
<li>从行最简形矩阵中读出方程组的解 (唯一解或无穷多解)。</li>
</ol>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性方程组的增广矩阵</span></span><br><span class="line">A = np.array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">7</span>], [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 NumPy 求解线性方程组 (如果 A 是方阵且可逆)</span></span><br><span class="line"><span class="comment"># x = np.linalg.solve(A[:, :-1], A[:, -1])</span></span><br><span class="line"><span class="comment"># print(&quot;解:&quot;, x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动实现高斯消元法 (简化版，仅适用于有唯一解的情况)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gaussian_elimination</span>(<span class="params">matrix</span>):</span><br><span class="line">    rows, cols = matrix.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">        <span class="comment"># 将主元变为 1</span></span><br><span class="line">        pivot = matrix[i, i]</span><br><span class="line">        matrix[i, :] = matrix[i, :] / pivot</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将主元所在列的下方元素变为 0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, rows):</span><br><span class="line">            factor = matrix[j, i]</span><br><span class="line">            matrix[j, :] = matrix[j, :] - factor * matrix[i, :]</span><br><span class="line">    <span class="keyword">return</span> matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求解</span></span><br><span class="line">reduced_matrix = gaussian_elimination(A.astype(<span class="built_in">float</span>)) <span class="comment">#转为float类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;行阶梯形矩阵:\n&quot;</span>, reduced_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回代求解</span></span><br><span class="line">x = np.zeros(reduced_matrix.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(reduced_matrix.shape[<span class="number">0</span>] -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">  x[i] = reduced_matrix[i, -<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, reduced_matrix.shape[<span class="number">0</span>]):</span><br><span class="line">    x[i] -= reduced_matrix[i,j] * x[j]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;解: &quot;</span>, x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>3.4 克拉默法则 (Cramer’s Rule)（选学）</strong></p>
<ul>
<li><p>克莱姆法则&#x2F;克拉默法则是一种利用行列式来求解线性方程组的方法。</p>
<p>对于一个有 n 个方程和 n 个未知数的线性方程组，如果系数矩阵 A 的行列式不为零（|A| ≠ 0），那么方程组有唯一解，解可以通过以下公式给出：</p>
<p>xi &#x3D; |Ai| &#x2F; |A|</p>
<p>其中：<br>xi 是第 i 个未知数。<br>|A| 是系数矩阵 A 的行列式。<br>|Ai| 是将系数矩阵 A 中第 i 列替换为常数项向量 b 后得到的矩阵的行列式。</p>
</li>
</ul>
<p>线性方程组是线性代数中的一个重要概念，也是解决实际问题时经常遇到的问题。高斯消元法是一种通用的求解线性方程组的方法。</p>
<hr>
<h3 id="4-特征值与特征向量"><a href="#4-特征值与特征向量" class="headerlink" title="4. 特征值与特征向量"></a>4. 特征值与特征向量</h3><p><strong>4.1 什么是特征值与特征向量？</strong></p>
<ul>
<li><p><strong>定义：</strong> 对于一个给定的方阵 A，如果存在一个非零向量 <strong>v</strong> 和一个标量 λ，使得：</p>
<p>A<strong>v</strong> &#x3D; λ<strong>v</strong></p>
<p>那么：</p>
<ul>
<li>λ 称为 A 的<strong>特征值</strong> (Eigenvalue)。</li>
<li><strong>v</strong> 称为 A 的对应于特征值 λ 的<strong>特征向量</strong> (Eigenvector)。</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>特征向量是指经过矩阵 A 变换后，方向不变或只是反向（仍然在同一条直线上）的非零向量。</li>
<li>特征值表示特征向量在变换中被缩放的比例。<ul>
<li>如果 λ &gt; 0，则特征向量被拉伸。</li>
<li>如果 λ &lt; 0，则特征向量被拉伸并反向。</li>
<li>如果 λ &#x3D; 0，则特征向量被压缩到原点（但特征向量必须是非零向量，所以这通常意味着矩阵 A 不可逆）。</li>
<li>如果 λ &#x3D; 1, 则特征向量不被拉伸或者压缩</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>4.2 特征值与特征向量的计算</strong></p>
<ol>
<li><p><strong>特征方程：</strong></p>
<ul>
<li>从 A<strong>v</strong> &#x3D; λ<strong>v</strong> 出发，可以推导出：</li>
<li>A<strong>v</strong> - λ<strong>v</strong> &#x3D; <strong>0</strong></li>
<li>(A - λI)<strong>v</strong> &#x3D; <strong>0</strong> (其中 I 是单位矩阵)</li>
<li>为了使方程有非零解，矩阵 (A - λI) 必须是奇异的 (不可逆的)，即它的行列式为 0：</li>
<li>det(A - λI) &#x3D; 0</li>
<li>这个方程称为 A 的<strong>特征方程</strong> (Characteristic Equation)。</li>
</ul>
</li>
<li><p><strong>求解特征值：</strong></p>
<ul>
<li>解特征方程 det(A - λI) &#x3D; 0，得到 A 的所有特征值 λ。</li>
</ul>
</li>
<li><p><strong>求解特征向量：</strong></p>
<ul>
<li>对于每个特征值 λ，将 λ 代入方程 (A - λI)<strong>v</strong> &#x3D; <strong>0</strong>，求解出对应的特征向量 <strong>v</strong>。</li>
<li>注意：特征向量不是唯一的，对于一个特征值，可以有无穷多个特征向量 (它们都在同一条直线上)。</li>
</ul>
</li>
</ol>
<p><strong>举例：</strong></p>
<p>求矩阵 A &#x3D; [[2, 1], [1, 2]] 的特征值和特征向量。</p>
<ol start="4">
<li><p><strong>特征方程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">det(A - λI) = det([[2-λ, 1], [1, 2-λ]]) = (2-λ)² - 1 = λ² - 4λ + 3 = 0</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>求解特征值：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(λ - 1)(λ - 3) = 0</span><br><span class="line">λ₁ = 1, λ₂ = 3</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>求解特征向量：</strong></p>
<ul>
<li>对于 λ₁ &#x3D; 1：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(A - λ₁I)v = [[1, 1], [1, 1]] [x, y] = [0, 0]</span><br></pre></td></tr></table></figure>
解得 x + y &#x3D; 0，所以特征向量可以表示为 k₁[-1, 1] (k₁ ≠ 0)。</li>
<li>对于 λ₂ &#x3D; 3：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(A - λ₂I)v = [[-1, 1], [1, -1]] [x, y] = [0, 0]</span><br></pre></td></tr></table></figure>
解得 x - y &#x3D; 0，所以特征向量可以表示为 k₂[1, 1] (k₂ ≠ 0)。</li>
</ul>
</li>
</ol>
<p><strong>4.3 特征值分解 (谱分解)</strong></p>
<ul>
<li>如果一个矩阵有n个线性无关的特征向量，则可以将矩阵进行分解</li>
</ul>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵定义</span></span><br><span class="line">A = np.array([[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算特征值和特征向量</span></span><br><span class="line">eigenvalues, eigenvectors = np.linalg.eig(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征值:&quot;</span>, eigenvalues)  <span class="comment"># 输出：[3. 1.]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征向量:\n&quot;</span>, eigenvectors)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># [[ 0.70710678 -0.70710678]</span></span><br><span class="line"><span class="comment">#  [ 0.70710678  0.70710678]]</span></span><br><span class="line"><span class="comment"># 注意：NumPy 返回的特征向量是单位向量 (长度为 1)。</span></span><br></pre></td></tr></table></figure>

<p><strong>4.4. 特征值与特征向量的几何意义</strong></p>
<ul>
<li><p><strong>特征向量：</strong> 在线性变换中，特征向量是指那些方向保持不变（或只是反向）的向量。它们定义了变换的主要方向。</p>
</li>
<li><p><strong>特征值：</strong> 特征值表示了特征向量在变换中被拉伸或压缩的比例。</p>
</li>
</ul>
<p>特征值和特征向量是线性代数中非常重要的概念，它们揭示了矩阵的内在性质。在深度学习中，特征值和特征向量被用于主成分分析 (PCA)、奇异值分解 (SVD) 等算法中。</p>
<hr>
<h3 id="5-奇异值分解-SVD"><a href="#5-奇异值分解-SVD" class="headerlink" title="5. 奇异值分解 (SVD)"></a>5. 奇异值分解 (SVD)</h3><p><strong>5.1 什么是奇异值分解？</strong></p>
<ul>
<li><p><strong>定义：</strong> 奇异值分解 (Singular Value Decomposition, SVD) 是一种重要的矩阵分解方法，可以将任意一个 m × n 的矩阵 A 分解为三个矩阵的乘积：</p>
<p>A &#x3D; UΣVᵀ</p>
<p>其中：</p>
<ul>
<li>U 是一个 m × m 的正交矩阵 (UᵀU &#x3D; I)。</li>
<li>Σ 是一个 m × n 的矩形对角矩阵，其对角线上的元素称为奇异值 (Singular Values)，通常按从大到小的顺序排列 (σ₁ ≥ σ₂ ≥ … ≥ σᵣ &gt; 0，其中 r 是矩阵 A 的秩)。</li>
<li>V 是一个 n × n 的正交矩阵 (VᵀV &#x3D; I)。</li>
<li>Vᵀ 表示 V的转置矩阵</li>
</ul>
</li>
<li><p><strong>与特征值分解的区别：</strong></p>
<ul>
<li>特征值分解只适用于方阵。</li>
<li>SVD 适用于任意形状的矩阵。</li>
</ul>
</li>
</ul>
<p><strong>理解 SVD：</strong></p>
<ul>
<li>SVD 可以看作是将一个线性变换分解为三个简单的变换：<ol>
<li><strong>旋转 (Vᵀ):</strong> 将原始空间中的向量旋转到一个新的坐标系中。</li>
<li><strong>缩放 (Σ):</strong> 在新的坐标系中，沿着坐标轴进行缩放 (奇异值的大小表示缩放的程度)。</li>
<li><strong>旋转 (U):</strong> 将缩放后的向量旋转到目标空间中。</li>
</ol>
</li>
</ul>
<p><strong>5.2 SVD 的计算 (作为进阶，这里只简要介绍步骤，不深入推导)</strong></p>
<ol>
<li><strong>计算 AᵀA 和 AAᵀ。</strong></li>
<li><strong>求 AᵀA 的特征值和特征向量：</strong><ul>
<li>AᵀA 的特征值就是奇异值的平方 (σᵢ²)。</li>
<li>AᵀA 的特征向量构成 V 的列向量。</li>
</ul>
</li>
<li><strong>求 AAᵀ 的特征值和特征向量：</strong><ul>
<li>AAᵀ 的特征值也是奇异值的平方 (σᵢ²)。</li>
<li>AAᵀ 的特征向量构成 U 的列向量。</li>
</ul>
</li>
<li><strong>求奇异值：</strong><ul>
<li>奇异值 σᵢ &#x3D; √(λᵢ)，其中 λᵢ 是 AᵀA (或 AAᵀ) 的特征值。</li>
</ul>
</li>
<li><strong>构造 Σ：</strong><ul>
<li>将奇异值按照从大到小的顺序排列在 Σ 的对角线上。</li>
</ul>
</li>
</ol>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵定义</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 奇异值分解</span></span><br><span class="line">U, S, V = np.linalg.svd(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:\n&quot;</span>, U)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;S:\n&quot;</span>, S)  <span class="comment"># NumPy 返回的是奇异值的一维数组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;V:\n&quot;</span>, V) <span class="comment"># V 已经是转置后的矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构 A</span></span><br><span class="line">Sigma = np.zeros(A.shape)</span><br><span class="line">Sigma[:A.shape[<span class="number">0</span>], :A.shape[<span class="number">0</span>]] = np.diag(S) <span class="comment">#如果A是MxN的矩阵，且M&lt;N, 则用M</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;重构 A:\n&quot;</span>, U @ Sigma @ V)</span><br></pre></td></tr></table></figure>

<p><strong>5.3 SVD 的应用</strong></p>
<ul>
<li><strong>数据压缩：</strong> 可以只保留较大的奇异值，从而实现数据的压缩。</li>
<li><strong>降维：</strong> 可以将高维数据投影到低维空间中 (主成分分析 PCA 的一种实现方式)。</li>
<li><strong>推荐系统：</strong> 可以用于发现用户和物品之间的潜在关系。</li>
<li><strong>图像处理：</strong> 可以用于图像去噪、图像压缩等。</li>
<li><strong>自然语言处理：</strong> 可以用于文本主题分析 (潜在语义分析 LSA)。</li>
</ul>
<p>奇异值分解 (SVD) 是一种强大的矩阵分解方法，它在深度学习和许多其他领域都有广泛的应用。理解SVD的原理和应用，对于深入理解深度学习算法非常有帮助。</p>
<hr>
<h1 id="III-微积分基础。"><a href="#III-微积分基础。" class="headerlink" title="III. 微积分基础。"></a><strong>III. 微积分基础</strong>。</h1><p>微积分是深度学习的另一个重要数学支柱。深度学习模型的训练过程，本质上就是一个优化问题，而微积分是解决优化问题的有力工具。</p>
<p>我们从第一个小知识点开始：<strong>1. 导数</strong></p>
<p><strong>1.1 什么是导数？</strong></p>
<ul>
<li><p><strong>直观理解：</strong> 导数 (Derivative) 描述了一个函数在某一点上的瞬时变化率。</p>
<ul>
<li>想象一下你正在开车，你的速度表显示的就是你行驶距离关于时间的瞬时变化率 (导数)。</li>
<li>如果函数图像是一条曲线，那么导数就是曲线在某一点上的切线的斜率。</li>
</ul>
</li>
<li><p><strong>数学定义：</strong><br>设函数 y &#x3D; f(x) 在点 x₀ 的某个邻域内有定义，如果极限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lim (Δx→0) [f(x₀ + Δx) - f(x₀)] / Δx</span><br></pre></td></tr></table></figure>

<p>存在，则称函数 f(x) 在点 x₀ 处<strong>可导</strong>，并称这个极限值为 f(x) 在点 x₀ 处的<strong>导数</strong>，记作 f’(x₀) 或 dy&#x2F;dx |ₓ&#x3D;ₓ₀。</p>
<ul>
<li>Δx 表示 x 的变化量 (x₀ + Δx - x₀)。</li>
<li>Δy &#x3D; f(x₀ + Δx) - f(x₀) 表示 y 的变化量。</li>
<li>Δy&#x2F;Δx 表示平均变化率 (割线的斜率)。</li>
<li>当 Δx 趋近于 0 时，平均变化率的极限就是瞬时变化率 (切线的斜率)。</li>
</ul>
</li>
</ul>
<p><strong>1.2 导数的几何意义</strong></p>
<ul>
<li>函数 y &#x3D; f(x) 在点 x₀ 处的导数 f’(x₀) 就是曲线 y &#x3D; f(x) 在点 (x₀, f(x₀)) 处的切线的斜率。</li>
</ul>
<p><strong>1.3 常见函数的导数</strong></p>
<table>
<thead>
<tr>
<th>函数 (y &#x3D; f(x))</th>
<th>导数 (y’ &#x3D; f’(x))</th>
</tr>
</thead>
<tbody><tr>
<td>C (常数)</td>
<td>0</td>
</tr>
<tr>
<td>xⁿ (n ≠ 0)</td>
<td>nxⁿ⁻¹</td>
</tr>
<tr>
<td>x</td>
<td>1</td>
</tr>
<tr>
<td>x²</td>
<td>2x</td>
</tr>
<tr>
<td>x³</td>
<td>3x²</td>
</tr>
<tr>
<td>1&#x2F;x</td>
<td>-1&#x2F;x²</td>
</tr>
<tr>
<td>√x</td>
<td>1&#x2F;(2√x)</td>
</tr>
<tr>
<td>aˣ</td>
<td>aˣ ln(a)</td>
</tr>
<tr>
<td>eˣ</td>
<td>eˣ</td>
</tr>
<tr>
<td>logₐx</td>
<td>1 &#x2F; (x ln(a))</td>
</tr>
<tr>
<td>ln x</td>
<td>1 &#x2F; x</td>
</tr>
<tr>
<td>sin x</td>
<td>cos x</td>
</tr>
<tr>
<td>cos x</td>
<td>-sin x</td>
</tr>
<tr>
<td>tan x</td>
<td>sec²x &#x3D; 1 &#x2F; cos²x</td>
</tr>
</tbody></table>
<p><strong>1.4 导数法则</strong></p>
<ul>
<li><strong>加法法则：</strong> (u(x) + v(x))’ &#x3D; u’(x) + v’(x)</li>
<li><strong>减法法则：</strong> (u(x) - v(x))’ &#x3D; u’(x) - v’(x)</li>
<li><strong>乘法法则：</strong> (u(x)v(x))’ &#x3D; u’(x)v(x) + u(x)v’(x)</li>
<li><strong>除法法则：</strong> (u(x)&#x2F;v(x))’ &#x3D; [u’(x)v(x) - u(x)v’(x)] &#x2F; v²(x)  (v(x) ≠ 0)</li>
<li><strong>链式法则 (复合函数求导)：</strong> 如果 y &#x3D; f(u) 且 u &#x3D; g(x)，那么 dy&#x2F;dx &#x3D; dy&#x2F;du * du&#x2F;dx。<ul>
<li>例如：y &#x3D; sin(x²)，求 dy&#x2F;dx。<ul>
<li>令 u &#x3D; x²，则 y &#x3D; sin(u)。</li>
<li>dy&#x2F;du &#x3D; cos(u)，du&#x2F;dx &#x3D; 2x。</li>
<li>dy&#x2F;dx &#x3D; cos(u) * 2x &#x3D; 2x cos(x²)。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>代码示例 (Python - NumPy&#x2F;SymPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 NumPy 计算数值导数 (近似)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">2</span></span><br><span class="line">h = <span class="number">0.0001</span></span><br><span class="line">numerical_derivative = (f(x + h) - f(x)) / h</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数值导数:&quot;</span>, numerical_derivative)  <span class="comment"># 输出：4.0001 (近似于 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 SymPy 计算符号导数</span></span><br><span class="line">x_sym = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># 定义符号变量</span></span><br><span class="line">f_sym = x_sym**<span class="number">2</span>  <span class="comment"># 定义符号表达式</span></span><br><span class="line">derivative = sp.diff(f_sym, x_sym)  <span class="comment"># 求导</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;符号导数:&quot;</span>, derivative)  <span class="comment"># 输出：2*x</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;符号导数在 x=2 处的值:&quot;</span>, derivative.subs(x_sym, <span class="number">2</span>))  <span class="comment"># 输出：4</span></span><br></pre></td></tr></table></figure>


<p>导数是微积分的核心概念之一，它描述了函数的变化率。理解导数的定义、几何意义、常见函数的导数以及导数法则，对于后续学习偏导数、梯度下降等概念至关重要。</p>
<hr>
<h3 id="2-偏导数"><a href="#2-偏导数" class="headerlink" title="2. 偏导数"></a>2. 偏导数</h3><p><strong>2.1 什么是偏导数？</strong></p>
<ul>
<li><p><strong>背景：</strong> 在多元函数中 (例如 z &#x3D; f(x, y)，有两个或多个自变量)，我们需要研究函数关于其中一个自变量的变化率，同时保持其他自变量不变。</p>
</li>
<li><p><strong>定义：</strong> 设函数 z &#x3D; f(x, y) 在点 (x₀, y₀) 的某个邻域内有定义，如果极限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lim (Δx→0) [f(x₀ + Δx, y₀) - f(x₀, y₀)] / Δx</span><br></pre></td></tr></table></figure>

<p>存在，则称此极限值为函数 f(x, y) 在点 (x₀, y₀) 处关于 x 的<strong>偏导数</strong> (Partial Derivative)，记作 ∂z&#x2F;∂x |<em>(x₀, y₀) 或 ∂f&#x2F;∂x |</em>(x₀, y₀) 或 fₓ(x₀, y₀)。</p>
<p>类似地，函数 f(x, y) 在点 (x₀, y₀) 处关于 y 的偏导数定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lim (Δy→0) [f(x₀, y₀ + Δy) - f(x₀, y₀)] / Δy</span><br></pre></td></tr></table></figure>

<p>记作 ∂z&#x2F;∂y |<em>(x₀, y₀) 或 ∂f&#x2F;∂y |</em>(x₀, y₀) 或 fᵧ(x₀, y₀)。</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>偏导数就是在多元函数中，固定其他所有自变量，只考虑一个自变量变化时，函数的变化率。</li>
<li>∂z&#x2F;∂x 表示函数沿着 x 轴方向的变化率。</li>
<li>∂z&#x2F;∂y 表示函数沿着 y 轴方向的变化率。</li>
</ul>
</li>
</ul>
<p><strong>2.2 偏导数的计算</strong></p>
<ul>
<li><strong>计算方法：</strong> 求偏导数时，只需将其他自变量视为常数，然后对目标自变量求导即可。</li>
</ul>
<p><strong>举例：</strong></p>
<ol>
<li><p>求函数 z &#x3D; x² + 3xy + y² 关于 x 和 y 的偏导数。</p>
<ul>
<li>∂z&#x2F;∂x &#x3D; 2x + 3y (将 y 视为常数)</li>
<li>∂z&#x2F;∂y &#x3D; 3x + 2y (将 x 视为常数)</li>
</ul>
</li>
<li><p>求函数 f(x, y, z) &#x3D; x²y + y²z + z²x 关于 x, y, z 的偏导数。</p>
<ul>
<li>∂f&#x2F;∂x &#x3D; 2xy + z²</li>
<li>∂f&#x2F;∂y &#x3D; x² + 2yz</li>
<li>∂f&#x2F;∂z &#x3D; y² + 2zx</li>
</ul>
</li>
</ol>
<p><strong>2.3 梯度向量</strong></p>
<ul>
<li><p><strong>定义：</strong> 对于多元函数 f(x₁, x₂, …, xₙ)，其所有偏导数组成的向量称为<strong>梯度向量</strong> (Gradient Vector)，记作 ∇f 或 grad f。</p>
<p>∇f &#x3D; [∂f&#x2F;∂x₁, ∂f&#x2F;∂x₂, …, ∂f&#x2F;∂xₙ]</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>梯度向量指向函数值增长最快的方向。</li>
<li>梯度向量的模 (长度) 表示函数值增长的速率。</li>
</ul>
</li>
</ul>
<p><strong>代码示例 (Python - SymPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x, y = sp.symbols(<span class="string">&#x27;x y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">f = x**<span class="number">2</span> + <span class="number">3</span>*x*y + y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算偏导数</span></span><br><span class="line">df_dx = sp.diff(f, x)</span><br><span class="line">df_dy = sp.diff(f, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;∂f/∂x:&quot;</span>, df_dx)  <span class="comment"># 输出：2*x + 3*y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;∂f/∂y:&quot;</span>, df_dy)  <span class="comment"># 输出：3*x + 2*y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度向量</span></span><br><span class="line">gradient = [df_dx, df_dy]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度向量:&quot;</span>, gradient) <span class="comment"># 输出：[2*x + 3*y, 3*x + 2*y]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算在某一点处的梯度</span></span><br><span class="line">point = &#123;x: <span class="number">1</span>, y: <span class="number">2</span>&#125;</span><br><span class="line">gradient_at_point = [g.subs(point) <span class="keyword">for</span> g <span class="keyword">in</span> gradient]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;在点 (1, 2) 处的梯度:&quot;</span>, gradient_at_point)  <span class="comment"># 输出：[8, 7]</span></span><br></pre></td></tr></table></figure>
<p><strong>2.4 方向导数</strong></p>
<ul>
<li><strong>定义:</strong> 方向导数表示函数沿着某个特定方向的变化率。</li>
<li><strong>理解:</strong> 偏导数是沿着坐标轴方向的特殊方向导数</li>
</ul>
<p><strong>2.5 全微分</strong></p>
<ul>
<li><strong>定义:</strong> 函数的所有自变量都发生微小变化时，函数值的总变化量。</li>
<li><strong>公式</strong> 对于二元函数z &#x3D; f(x,y)<br>  dz &#x3D; (∂f&#x2F;∂x)dx + (∂f&#x2F;∂y)dy</li>
</ul>
<p>偏导数是多元函数中关于单个自变量的变化率。梯度向量由所有偏导数组成，指向函数值增长最快的方向。理解偏导数和梯度向量，对于后续学习梯度下降法等优化算法至关重要。</p>
<hr>
<h3 id="3-梯度下降法"><a href="#3-梯度下降法" class="headerlink" title="3. 梯度下降法"></a>3. 梯度下降法</h3><p><strong>3.1 什么是梯度下降法？</strong></p>
<ul>
<li><p><strong>背景：</strong> 在深度学习中，我们经常需要找到一个函数的最小值 (例如损失函数)。梯度下降法 (Gradient Descent) 是一种常用的迭代优化算法，用于求解函数的局部最小值。</p>
</li>
<li><p><strong>原理：</strong></p>
<ul>
<li>梯度向量指向函数值增长最快的方向，那么负梯度方向就是函数值下降最快的方向。</li>
<li>梯度下降法沿着负梯度方向逐步迭代，不断逼近函数的局部最小值。</li>
</ul>
</li>
<li><p><strong>形象比喻：</strong></p>
<ul>
<li>想象你站在一座山上，想要下到山谷的最低点。</li>
<li>你环顾四周，找到最陡峭的下坡方向 (负梯度方向)。</li>
<li>你沿着这个方向迈出一小步。</li>
<li>重复这个过程，直到你到达一个平坦的地方 (局部最小值)。</li>
</ul>
</li>
</ul>
<p><strong>3.2 梯度下降法的步骤</strong></p>
<ol>
<li><strong>初始化：</strong> 选择一个初始点 x₀ (可以随机选择)。</li>
<li><strong>迭代：</strong> 重复以下步骤，直到满足停止条件：<ul>
<li>计算函数 f(x) 在当前点 xᵢ 的梯度 ∇f(xᵢ)。</li>
<li>更新 xᵢ：xᵢ₊₁ &#x3D; xᵢ - α∇f(xᵢ)，其中 α 是学习率 (步长)。</li>
</ul>
</li>
<li><strong>停止条件：</strong><ul>
<li>达到最大迭代次数。</li>
<li>梯度向量的模 (长度) 小于某个阈值。</li>
<li>函数值的变化小于某个阈值。</li>
</ul>
</li>
</ol>
<p><strong>3.3 学习率 (步长)</strong></p>
<ul>
<li><strong>学习率 α：</strong> 一个小的正数，控制每次迭代的步长。<ul>
<li>学习率过大：可能导致震荡，无法收敛到最小值。</li>
<li>学习率过小：收敛速度太慢，需要更多迭代次数。</li>
</ul>
</li>
</ul>
<p><strong>3.4 局部最小值、全局最小值和鞍点</strong></p>
<ul>
<li><strong>局部最小值 (Local Minimum)：</strong> 在某个邻域内，函数值最小的点。</li>
<li><strong>全局最小值 (Global Minimum)：</strong> 在整个定义域内，函数值最小的点。</li>
<li><strong>鞍点 (Saddle Point)：</strong> 梯度为零，但既不是局部最小值也不是局部最大值的点 (形状像马鞍)。<ul>
<li>梯度下降法可能会陷入局部最小值或鞍点。</li>
</ul>
</li>
</ul>
<p><strong>代码示例 (Python - NumPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数 (示例：f(x) = x²)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度 (示例：f&#x27;(x) = 2x)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">initial_x, learning_rate, num_iterations</span>):</span><br><span class="line">    x = initial_x</span><br><span class="line">    x_history = [x]  <span class="comment"># 记录 x 的历史值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        grad = gradient(x)</span><br><span class="line">        x = x - learning_rate * grad</span><br><span class="line">        x_history.append(x)</span><br><span class="line">    <span class="keyword">return</span> x, x_history</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">initial_x = <span class="number">4</span>  <span class="comment"># 初始点</span></span><br><span class="line">learning_rate = <span class="number">0.1</span>  <span class="comment"># 学习率</span></span><br><span class="line">num_iterations = <span class="number">20</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行梯度下降</span></span><br><span class="line">min_x, x_history = gradient_descent(initial_x, learning_rate, num_iterations)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最小值点:&quot;</span>, min_x)  <span class="comment"># 输出：接近 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line">y_values = f(x_values)</span><br><span class="line"></span><br><span class="line">plt.plot(x_values, y_values)</span><br><span class="line">plt.scatter(x_history, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> x_history], color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;f(x)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gradient Descent&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p>梯度下降法是一种常用的优化算法，用于求解函数的最小值。它沿着负梯度方向逐步迭代，不断逼近最小值。学习率的选择很重要，过大或过小都可能导致算法无法收敛或收敛速度过慢。梯度下降法可能会陷入局部最小值。</p>
<p><strong>范例 1：一元函数的梯度下降</strong></p>
<p>我们以一个简单的一元函数为例：f(x) &#x3D; x² - 4x + 5</p>
<ol>
<li><p><strong>目标：</strong> 找到函数 f(x) 的最小值。</p>
</li>
<li><p><strong>导数：</strong> f’(x) &#x3D; 2x - 4</p>
</li>
<li><p><strong>梯度下降：</strong></p>
<ul>
<li>初始化：选择一个初始点 x₀，例如 x₀ &#x3D; 5。</li>
<li>学习率：设置学习率 α，例如 α &#x3D; 0.1。</li>
<li>迭代：<ul>
<li>计算梯度：f’(xᵢ) &#x3D; 2xᵢ - 4</li>
<li>更新 x：xᵢ₊₁ &#x3D; xᵢ - αf’(xᵢ) &#x3D; xᵢ - 0.1(2xᵢ - 4)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Python 代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span> - <span class="number">4</span>*x + <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义导数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x - <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">initial_x, learning_rate, num_iterations</span>):</span><br><span class="line">    x = initial_x</span><br><span class="line">    x_history = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        grad = gradient(x)</span><br><span class="line">        x = x - learning_rate * grad</span><br><span class="line">        x_history.append(x)</span><br><span class="line">    <span class="keyword">return</span> x, x_history</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">initial_x = <span class="number">5</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_iterations = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行梯度下降</span></span><br><span class="line">min_x, x_history = gradient_descent(initial_x, learning_rate, num_iterations)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最小值点:&quot;</span>, min_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">100</span>)</span><br><span class="line">y_values = f(x_values)</span><br><span class="line"></span><br><span class="line">plt.plot(x_values, y_values)</span><br><span class="line">plt.scatter(x_history, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> x_history], color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;f(x)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gradient Descent for f(x) = x² - 4x + 5&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行这段代码，你会看到一个抛物线图形，红色的点表示梯度下降的迭代过程，最终红点会逐渐靠近抛物线的最低点 (x &#x3D; 2)。</p>
</li>
</ol>
<p><strong>范例 2：二元函数的梯度下降</strong></p>
<p>我们以一个二元函数为例：f(x, y) &#x3D; x² + 2y²</p>
<ol>
<li><p><strong>目标：</strong> 找到函数 f(x, y) 的最小值。</p>
</li>
<li><p><strong>偏导数：</strong></p>
<ul>
<li>∂f&#x2F;∂x &#x3D; 2x</li>
<li>∂f&#x2F;∂y &#x3D; 4y</li>
</ul>
</li>
<li><p><strong>梯度：</strong> ∇f(x, y) &#x3D; [2x, 4y]</p>
</li>
<li><p><strong>梯度下降：</strong></p>
<ul>
<li>初始化：选择一个初始点 (x₀, y₀)，例如 (x₀, y₀) &#x3D; (2, 3)。</li>
<li>学习率：设置学习率 α，例如 α &#x3D; 0.1。</li>
<li>迭代：<ul>
<li>计算梯度：∇f(xᵢ, yᵢ) &#x3D; [2xᵢ, 4yᵢ]</li>
<li>更新 x 和 y：<ul>
<li>xᵢ₊₁ &#x3D; xᵢ - α * 2xᵢ</li>
<li>yᵢ₊₁ &#x3D; yᵢ - α * 4yᵢ</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Python 代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">2</span>*y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> np.array([<span class="number">2</span>*x, <span class="number">4</span>*y])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">initial_point, learning_rate, num_iterations</span>):</span><br><span class="line">    point = np.array(initial_point)</span><br><span class="line">    point_history = [point]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        grad = gradient(point[<span class="number">0</span>], point[<span class="number">1</span>])</span><br><span class="line">        point = point - learning_rate * grad</span><br><span class="line">        point_history.append(point)</span><br><span class="line">    <span class="keyword">return</span> point, np.array(point_history)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">initial_point = [<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_iterations = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行梯度下降</span></span><br><span class="line">min_point, point_history = gradient_descent(initial_point, learning_rate, num_iterations)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最小值点:&quot;</span>, min_point)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化 (3D 图)</span></span><br><span class="line">x_values = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">y_values = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">X, Y = np.meshgrid(x_values, y_values)</span><br><span class="line">Z = f(X, Y)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(X, Y, Z, cmap=<span class="string">&#x27;viridis&#x27;</span>, alpha=<span class="number">0.8</span>)</span><br><span class="line">ax.scatter(point_history[:, <span class="number">0</span>], point_history[:, <span class="number">1</span>], f(point_history[:, <span class="number">0</span>], point_history[:, <span class="number">1</span>]), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;f(x, y)&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Gradient Descent for f(x, y) = x² + 2y²&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行这段代码，你会看到一个三维的碗状图形，红色的点表示梯度下降的迭代过程，最终红点会逐渐靠近碗底 (最小值点 (0, 0))。</p>
</li>
</ol>
<p><strong>范例 3：学习率的影响</strong></p>
<p>我们修改范例 1 的代码，尝试不同的学习率，观察梯度下降的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (函数 f(x) 和导数 gradient(x) 的定义与范例 1 相同) ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法 (添加学习率参数)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">initial_x, learning_rate, num_iterations</span>):</span><br><span class="line">    x = initial_x</span><br><span class="line">    x_history = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        grad = gradient(x)</span><br><span class="line">        x = x - learning_rate * grad</span><br><span class="line">        x_history.append(x)</span><br><span class="line">    <span class="keyword">return</span> x, x_history</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">initial_x = <span class="number">5</span></span><br><span class="line">num_iterations = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同的学习率</span></span><br><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">100</span>)</span><br><span class="line">y_values = f(x_values)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x_values, y_values)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    min_x, x_history = gradient_descent(initial_x, lr, num_iterations)</span><br><span class="line">    plt.plot(x_history, [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> x_history], marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">f&#x27;LR=<span class="subst">&#123;lr&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;f(x)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Effect of Learning Rate on Gradient Descent&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行这段代码：<br>学习率设置的比较小（0.01）：收敛比较慢<br>学习率比较合适（0.1）：能够正确的收敛<br>学习率有点大（0.5）：收敛较快，但是会有一些震荡<br>学习率过大（1.1）： 发散，不会收敛到正确的答案</p>
<p>通过这些范例，你可以更直观地理解梯度下降法的原理、步骤以及学习率的影响。在实际应用中，你需要根据具体的问题选择合适的学习率和迭代次数。</p>
<hr>
<h3 id="4-泰勒展开"><a href="#4-泰勒展开" class="headerlink" title="4. 泰勒展开"></a>4. 泰勒展开</h3><p><strong>4.1 什么是泰勒展开？</strong></p>
<ul>
<li><p><strong>背景：</strong> 在数学中，我们经常需要用简单的函数 (如多项式函数) 来近似表示复杂的函数。泰勒展开 (Taylor Expansion) 提供了一种将函数表示为无穷级数 (多项式) 的方法。</p>
</li>
<li><p><strong>思想：</strong> 在某一点附近，用函数的多项式来逼近函数的值。</p>
</li>
<li><p><strong>泰勒公式 (Taylor’s Formula)：</strong><br>设函数 f(x) 在包含 x₀ 的某个开区间 (a, b) 内具有 (n+1) 阶导数，则对于任意 x ∈ (a, b)，有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = f(x₀) + f&#x27;(x₀)(x - x₀) + f&#x27;&#x27;(x₀)(x - x₀)²/2! + ... + f⁽ⁿ⁾(x₀)(x - x₀)ⁿ/n! + Rₙ(x)</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>f’(x₀), f’’(x₀), …, f⁽ⁿ⁾(x₀) 分别表示 f(x) 在 x₀ 处的 1 阶、2 阶、…、n 阶导数。</li>
<li>n! 表示 n 的阶乘 (n! &#x3D; 1 × 2 × 3 × … × n)。</li>
<li>Rₙ(x) 是<strong>余项</strong> (Remainder)，表示泰勒展开的误差。<ul>
<li>拉格朗日余项：Rₙ(x) &#x3D; f⁽ⁿ⁺¹⁾(ξ)(x - x₀)ⁿ⁺¹&#x2F;(n+1)!，其中 ξ 是介于 x₀ 和 x 之间的某个值。</li>
<li>皮亚诺余项:  Rₙ(x) &#x3D; o((x - x₀)ⁿ)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>麦克劳林公式 (Maclaurin’s Formula)：</strong> 当 x₀ &#x3D; 0 时，泰勒公式称为麦克劳林公式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = f(0) + f&#x27;(0)x + f&#x27;&#x27;(0)x²/2! + ... + f⁽ⁿ⁾(0)xⁿ/n! + Rₙ(x)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>4.2 一阶泰勒展开和二阶泰勒展开</strong></p>
<ul>
<li><p><strong>一阶泰勒展开 (线性近似)：</strong><br>只保留泰勒公式的前两项：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) ≈ f(x₀) + f&#x27;(x₀)(x - x₀)</span><br></pre></td></tr></table></figure>

<p>几何意义：用函数在点 x₀ 处的切线来近似函数的值。</p>
</li>
<li><p><strong>二阶泰勒展开 (二次近似)：</strong><br>保留泰勒公式的前三项：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) ≈ f(x₀) + f&#x27;(x₀)(x - x₀) + f&#x27;&#x27;(x₀)(x - x₀)²/2!</span><br></pre></td></tr></table></figure>

<p>几何意义：用函数在点 x₀ 处的抛物线来近似函数的值。</p>
</li>
</ul>
<p><strong>4.3 泰勒展开的应用</strong></p>
<ul>
<li><strong>近似计算：</strong> 可以用泰勒展开来计算函数的近似值，特别是当 x 接近 x₀ 时。</li>
<li><strong>优化：</strong> 在优化算法中，可以用泰勒展开来近似目标函数，从而简化优化问题 (例如牛顿法)。</li>
<li><strong>物理学：</strong> 在物理学中，泰勒展开被广泛用于近似各种物理量。</li>
</ul>
<p><strong>代码示例 (Python - SymPy):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数 (示例：e^x)</span></span><br><span class="line">f = sp.exp(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算泰勒展开 (在 x=0 处展开到 5 阶)</span></span><br><span class="line">taylor_expansion = f.series(x, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;泰勒展开:&quot;</span>, taylor_expansion)</span><br><span class="line"><span class="comment"># 输出：1 + x + x**2/2 + x**3/6 + x**4/24 + O(x**5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 e^0.1 的近似值 (使用泰勒展开)</span></span><br><span class="line">x0 = <span class="number">0</span></span><br><span class="line">approx_value = taylor_expansion.subs(x, <span class="number">0.1</span>).n()  <span class="comment"># .n() 将结果转换为数值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;e^0.1 的近似值:&quot;</span>, approx_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 e^0.1 的真实值</span></span><br><span class="line">true_value = sp.exp(<span class="number">0.1</span>).n()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;e^0.1 的真实值:&quot;</span>, true_value)</span><br></pre></td></tr></table></figure>



<p>泰勒展开是一种将函数表示为多项式的方法，可以用于近似计算、优化等。一阶泰勒展开是用切线近似函数，二阶泰勒展开是用抛物线近似函数。</p>
<p><strong>范例 1：  eˣ 的泰勒展开</strong></p>
<ol>
<li><p><strong>函数：</strong> f(x) &#x3D; eˣ</p>
</li>
<li><p><strong>麦克劳林展开 (在 x &#x3D; 0 处展开)：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eˣ = 1 + x + x²/2! + x³/3! + x⁴/4! + ...</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Python 代码 (使用 SymPy)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">f = sp.exp(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算不同阶数的泰勒展开</span></span><br><span class="line">taylor_1 = f.series(x, <span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># 1 阶</span></span><br><span class="line">taylor_3 = f.series(x, <span class="number">0</span>, <span class="number">3</span>)  <span class="comment"># 3 阶</span></span><br><span class="line">taylor_5 = f.series(x, <span class="number">0</span>, <span class="number">5</span>)  <span class="comment"># 5 阶</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1 阶泰勒展开:&quot;</span>, taylor_1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3 阶泰勒展开:&quot;</span>, taylor_3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5 阶泰勒展开:&quot;</span>, taylor_5)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">f_values = [f.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 真实值</span></span><br><span class="line">t1_values = [taylor_1.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 1 阶</span></span><br><span class="line">t3_values = [taylor_3.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 3 阶</span></span><br><span class="line">t5_values = [taylor_5.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 5 阶</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x_values, f_values, label=<span class="string">&#x27;eˣ&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t1_values, label=<span class="string">&#x27;1st Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t3_values, label=<span class="string">&#x27;3rd Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t5_values, label=<span class="string">&#x27;5th Order&#x27;</span>)</span><br><span class="line">plt.ylim(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># 设置 y 轴范围</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Taylor Expansion of eˣ&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行这段代码，你会看到 eˣ 的图像以及不同阶数泰勒展开的图像。在 x &#x3D; 0 附近，高阶泰勒展开与原函数的拟合程度更好。</p>
</li>
</ol>
<p><strong>范例 2：sin(x) 的泰勒展开</strong></p>
<ol start="4">
<li><p><strong>函数：</strong> f(x) &#x3D; sin(x)</p>
</li>
<li><p><strong>麦克劳林展开 (在 x &#x3D; 0 处展开)：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sin(x) = x - x³/3! + x⁵/5! - x⁷/7! + ...</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Python 代码 (使用 SymPy)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">f = sp.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算不同阶数的泰勒展开</span></span><br><span class="line">taylor_1 = f.series(x, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># 1 阶 (注意：sin(x) 的 1 阶和 2 阶展开相同)</span></span><br><span class="line">taylor_3 = f.series(x, <span class="number">0</span>, <span class="number">4</span>)  <span class="comment"># 3 阶</span></span><br><span class="line">taylor_5 = f.series(x, <span class="number">0</span>, <span class="number">6</span>)  <span class="comment"># 5 阶</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1 阶泰勒展开:&quot;</span>, taylor_1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3 阶泰勒展开:&quot;</span>, taylor_3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5 阶泰勒展开:&quot;</span>, taylor_5)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(-<span class="number">2</span>*np.pi, <span class="number">2</span>*np.pi, <span class="number">400</span>)</span><br><span class="line">f_values = [f.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 真实值</span></span><br><span class="line">t1_values = [taylor_1.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 1 阶</span></span><br><span class="line">t3_values = [taylor_3.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 3 阶</span></span><br><span class="line">t5_values = [taylor_5.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 5 阶</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x_values, f_values, label=<span class="string">&#x27;sin(x)&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t1_values, label=<span class="string">&#x27;1st Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t3_values, label=<span class="string">&#x27;3rd Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t5_values, label=<span class="string">&#x27;5th Order&#x27;</span>)</span><br><span class="line">plt.ylim(-<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Taylor Expansion of sin(x)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行这段代码，你会看到 sin(x) 的图像以及不同阶数泰勒展开的图像。在 x &#x3D; 0 附近，高阶泰勒展开与原函数的拟合程度更好。随着远离展开点，误差会逐渐增大。</p>
</li>
</ol>
<p><strong>范例3.  一元二次函数进行泰勒展开</strong></p>
<p>以 f(x) &#x3D; x² + 2x + 1, 在x&#x3D;1处进行泰勒展开</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">f = x**<span class="number">2</span> + <span class="number">2</span>*x +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算不同阶数的泰勒展开</span></span><br><span class="line">x0 = <span class="number">1</span></span><br><span class="line">taylor_0 = f.series(x, x0, <span class="number">1</span>)  <span class="comment"># 0 阶</span></span><br><span class="line">taylor_1 = f.series(x, x0, <span class="number">2</span>)  <span class="comment"># 1 阶</span></span><br><span class="line">taylor_2 = f.series(x, x0, <span class="number">3</span>)  <span class="comment"># 2 阶</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;0 阶泰勒展开:&quot;</span>, taylor_0)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1 阶泰勒展开:&quot;</span>, taylor_1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;2 阶泰勒展开:&quot;</span>, taylor_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(-<span class="number">2</span>, <span class="number">4</span>, <span class="number">400</span>)</span><br><span class="line">f_values = [f.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 真实值</span></span><br><span class="line">t0_values = [taylor_0.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 1 阶</span></span><br><span class="line">t1_values = [taylor_1.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 3 阶</span></span><br><span class="line">t2_values = [taylor_2.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]  <span class="comment"># 5 阶</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x_values, f_values, label=<span class="string">&#x27;f(x)&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t0_values, label=<span class="string">&#x27;0st Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t1_values, label=<span class="string">&#x27;1st Order&#x27;</span>)</span><br><span class="line">plt.plot(x_values, t2_values, label=<span class="string">&#x27;2nd Order&#x27;</span>)</span><br><span class="line">plt.ylim(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># 设置 y 轴范围</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Taylor Expansion of f(x)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到对于2次函数， 其二阶泰勒展开和原函数是完全相同的</p>
<p>通过这些范例，你可以看到泰勒展开是如何用多项式来逼近函数的，以及展开的阶数越高，在展开点附近逼近的效果越好。</p>
<hr>
<p>好的，我们继续学习微积分的下一个知识点：<strong>凸优化 (选学)</strong>。</p>
<h3 id="5-凸优化-Convex-Optimization"><a href="#5-凸优化-Convex-Optimization" class="headerlink" title="5. 凸优化 (Convex Optimization)"></a>5. 凸优化 (Convex Optimization)</h3><p><strong>5.1 为什么学习凸优化？</strong></p>
<ul>
<li>在深度学习中，许多优化问题被设计为凸优化问题，因为凸优化问题具有良好的性质：局部最优解就是全局最优解。这使得优化算法更容易找到全局最优解，而不用担心陷入局部最优解。</li>
</ul>
<p><strong>5.2 凸集 (Convex Set)</strong></p>
<ul>
<li><p><strong>定义：</strong> 在一个集合中，如果任意两点之间的连线上的所有点都属于这个集合，那么这个集合就是凸集。</p>
</li>
<li><p><strong>数学表达：</strong> 对于集合 C 中的任意两点 x₁ 和 x₂，以及任意 θ ∈ [0, 1]，如果 θx₁ + (1 - θ)x₂ ∈ C，那么集合 C 是凸集。</p>
</li>
<li><p><strong>直观理解：</strong> 凸集没有“凹陷”的部分。</p>
</li>
<li><p><strong>常见凸集：</strong></p>
<ul>
<li>直线</li>
<li>线段</li>
<li>平面</li>
<li>球体</li>
<li>多面体</li>
</ul>
</li>
<li><p><strong>非凸集</strong></p>
<ul>
<li>存在“凹陷”的部分</li>
</ul>
</li>
</ul>
<p><strong>5.3 凸函数 (Convex Function)</strong></p>
<ul>
<li><p><strong>定义：</strong> 如果一个函数的定义域是凸集，并且对于定义域中的任意两点 x₁ 和 x₂，以及任意 θ ∈ [0, 1]，满足：</p>
<p>f(θx₁ + (1 - θ)x₂) ≤ θf(x₁) + (1 - θ)f(x₂)</p>
<p>那么这个函数就是凸函数。</p>
</li>
<li><p><strong>几何意义：</strong> 函数图像上任意两点之间的连线 (弦) 位于函数图像的上方或与其重合。</p>
</li>
<li><p><strong>直观理解：</strong> 凸函数像一个“碗”的形状，没有“波峰”。</p>
</li>
<li><p><strong>常见凸函数：</strong></p>
<ul>
<li>线性函数 (f(x) &#x3D; ax + b)</li>
<li>二次函数 (f(x) &#x3D; ax² + bx + c，其中 a ≥ 0)</li>
<li>指数函数 (f(x) &#x3D; eˣ)</li>
<li>负对数函数 (f(x) &#x3D; -log(x))</li>
<li>范数函数 (f(x) &#x3D; ||x||)</li>
</ul>
</li>
<li><p><strong>严格凸函数 (Strictly Convex Function)：</strong><br>如果对于定义域中的任意两点 x₁ 和 x₂ (x₁ ≠ x₂)，以及任意 θ ∈ (0, 1)，满足：</p>
<p>f(θx₁ + (1 - θ)x₂) &lt; θf(x₁) + (1 - θ)f(x₂)</p>
<p>那么这个函数就是严格凸函数。</p>
</li>
</ul>
<p><strong>5.4 凸优化问题</strong></p>
<ul>
<li><p><strong>定义：</strong> 在一个凸集上，最小化一个凸函数的问题，称为凸优化问题。</p>
</li>
<li><p><strong>标准形式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">minimize  f(x)</span><br><span class="line">subject to  x ∈ C</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>f(x) 是凸函数。</li>
<li>C 是凸集。</li>
</ul>
</li>
<li><p><strong>性质：</strong> 凸优化问题的任何局部最优解都是全局最优解。</p>
</li>
</ul>
<p><strong>5.5 凸性判断</strong></p>
<ul>
<li><p><strong>一阶条件：</strong> 对于可微函数 f(x)，如果其定义域是凸集，且对于定义域中的任意两点 x 和 y，满足：</p>
<p>f(y) ≥ f(x) + ∇f(x)ᵀ(y - x)</p>
<p>那么 f(x) 是凸函数。</p>
</li>
<li><p><strong>二阶条件：</strong> 对于二次可微函数 f(x)，如果其定义域是凸集，且其 Hessian 矩阵 (二阶偏导数矩阵) 是半正定的，那么 f(x) 是凸函数。</p>
</li>
</ul>
<p>凸优化问题具有良好的性质：局部最优解就是全局最优解。这使得凸优化问题在深度学习中非常重要。了解凸集、凸函数和凸优化问题的定义，以及凸性的判断方法，有助于你更好地理解深度学习中的优化算法。</p>
<p><strong>范例 1：凸集和非凸集</strong></p>
<ol>
<li><p><strong>图形示例：</strong></p>
<pre class="mermaid">    graph LR
    subgraph Convex Sets
        A(圆形)
        B(正方形)
        C(三角形)
        D(线段)
    end

    subgraph Non-convex Sets
        E(星形)
        F(环形)
        G(凹多边形)
    end

    A --> |任意两点连线仍在集合内| H(凸集)
    B --> |任意两点连线仍在集合内| H
    C --> |任意两点连线仍在集合内| H
    D --> |任意两点连线仍在集合内| H

    E --> |存在两点连线不在集合内| I(非凸集)
    F --> |存在两点连线不在集合内| I
    G --> |存在两点连线不在集合内| I</pre>
</li>
<li><p><strong>代码示例 (判断点是否在集合内)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 凸集示例：圆形</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_in_circle</span>(<span class="params">point, center, radius</span>):</span><br><span class="line">    <span class="keyword">return</span> np.linalg.norm(np.array(point) - np.array(center)) &lt;= radius</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非凸集示例：星形 (简化版)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_in_star</span>(<span class="params">point</span>):</span><br><span class="line">    x, y = point</span><br><span class="line">    <span class="keyword">if</span> -<span class="number">1</span> &lt;= x &lt;= <span class="number">1</span> <span class="keyword">and</span> -<span class="number">0.2</span> &lt;= y &lt;= <span class="number">0.2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> -<span class="number">0.2</span> &lt;= x &lt;= <span class="number">0.2</span> <span class="keyword">and</span> -<span class="number">1</span> &lt;= y &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试点</span></span><br><span class="line">point1 = (<span class="number">0.5</span>, <span class="number">0.5</span>)  <span class="comment"># 圆内</span></span><br><span class="line">point2 = (<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 圆外</span></span><br><span class="line">point3 = (<span class="number">0.5</span>, <span class="number">0.5</span>)  <span class="comment"># 星形内</span></span><br><span class="line">point4 = (<span class="number">1.5</span>, <span class="number">1.5</span>)    <span class="comment"># 星形外</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;点 <span class="subst">&#123;point1&#125;</span> 是否在圆形内: <span class="subst">&#123;is_in_circle(point1, (<span class="number">0</span>, <span class="number">0</span>), <span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;点 <span class="subst">&#123;point2&#125;</span> 是否在圆形内: <span class="subst">&#123;is_in_circle(point2, (<span class="number">0</span>, <span class="number">0</span>), <span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;点 <span class="subst">&#123;point3&#125;</span> 是否在星形内: <span class="subst">&#123;is_in_star(point3)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;点 <span class="subst">&#123;point4&#125;</span> 是否在星形内: <span class="subst">&#123;is_in_star(point4)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化 (圆形)</span></span><br><span class="line">circle = plt.Circle((<span class="number">0</span>, <span class="number">0</span>), <span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.add_patch(circle)</span><br><span class="line">ax.scatter(*point1, color=<span class="string">&#x27;green&#x27;</span>, label=<span class="string">&#x27;In Circle&#x27;</span>)</span><br><span class="line">ax.scatter(*point2, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Out of Circle&#x27;</span>)</span><br><span class="line">ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>, adjustable=<span class="string">&#x27;box&#x27;</span>)</span><br><span class="line">plt.xlim(-<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.ylim(-<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Convex Set: Circle&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 2：凸函数和非凸函数</strong></p>
<ol>
<li><strong>图形示例：</strong></li>
</ol>
<p>   用mermaid代码表示如下:<br>   <pre class="mermaid">    graph LR<br>    subgraph 凸函数<br>        A(y = x^2)<br>        B(y = e^x)<br>        C(y = |x|)<br>        A --&gt;|弦在图像上方|D(凸函数性质)<br>        B --&gt;|弦在图像上方|D<br>        C --&gt;|弦在图像上方|D<br>    end</pre></p>
<pre><code>subgraph 非凸函数
    E(y = x^3)
    F(y = sin(x))
    E --&gt;|存在弦在图像下方|G(非凸函数)
    F --&gt;|存在弦在图像下方|G
end&lt;/pre&gt;
</code></pre>
<ol start="2">
<li><p><strong>代码示例 (判断凸性 - 利用二阶导数)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义符号变量</span></span><br><span class="line">x = sp.Symbol(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 凸函数示例：f(x) = x²</span></span><br><span class="line">f1 = x**<span class="number">2</span></span><br><span class="line">f1_second_derivative = sp.diff(f1, x, <span class="number">2</span>)  <span class="comment"># 求二阶导数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;f1(x) = x² 的二阶导数: <span class="subst">&#123;f1_second_derivative&#125;</span>&quot;</span>)  <span class="comment"># 输出：2 (恒大于 0，为凸函数)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 非凸函数示例：f(x) = x³</span></span><br><span class="line">f2 = x**<span class="number">3</span></span><br><span class="line">f2_second_derivative = sp.diff(f2, x, <span class="number">2</span>)  <span class="comment"># 求二阶导数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;f2(x) = x³ 的二阶导数: <span class="subst">&#123;f2_second_derivative&#125;</span>&quot;</span>)  <span class="comment"># 输出：6*x (可正可负，非凸函数)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">x_values = np.linspace(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">f1_values = [f1.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]</span><br><span class="line">f2_values = [f2.subs(x, val).n() <span class="keyword">for</span> val <span class="keyword">in</span> x_values]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x_values, f1_values, label=<span class="string">&#x27;f(x) = x² (Convex)&#x27;</span>)</span><br><span class="line">plt.plot(x_values, f2_values, label=<span class="string">&#x27;f(x) = x³ (Non-convex)&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Convex and Non-convex Functions&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 3：凸优化问题示例</strong></p>
<ol>
<li><p><strong>问题：</strong> 在半径为 1 的圆内，找到距离原点最近的点。</p>
</li>
<li><p><strong>数学表达：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">minimize  ||x||₂  (欧几里得范数)</span><br><span class="line">subject to  x² + y² ≤ 1  (x = [x, y])</span><br></pre></td></tr></table></figure>

<ul>
<li>目标函数：||x||₂ 是凸函数。</li>
<li>约束条件：x² + y² ≤ 1 定义了一个凸集 (圆形区域)。</li>
</ul>
</li>
<li><p><strong>这是一个凸优化问题。</strong> 其全局最优解显然是原点 (0, 0)。</p>
</li>
</ol>
<p>通过这些范例，你可以看到凸集和非凸集、凸函数和非凸函数的区别，以及凸优化问题的形式。在深度学习中，我们通常希望优化问题是凸的，因为这样可以保证找到全局最优解。</p>
<hr>
<p>好的，我们进入下一个大的知识点：<strong>IV. 概率论与数理统计基础</strong>。</p>
<p>概率论与数理统计是深度学习中非常重要的数学工具。深度学习模型很多时候是在处理不确定性，概率论提供了一套描述和量化不确定性的方法。数理统计则提供了从数据中学习和推断的方法。</p>
<p>我们从第一个小知识点开始：<strong>1. 随机事件与概率</strong></p>
<p><strong>1.1 随机事件</strong></p>
<ul>
<li><p><strong>随机试验 (Random Experiment)：</strong></p>
<ul>
<li>可以在相同条件下重复进行。</li>
<li>每次试验的结果不止一个，且事先知道所有可能的结果。</li>
<li>每次试验前不能确定会出现哪一个结果。</li>
</ul>
</li>
<li><p><strong>样本空间 (Sample Space)：</strong> 随机试验的所有可能结果组成的集合，通常用 Ω (大写 Omega) 表示。</p>
</li>
<li><p><strong>随机事件 (Random Event)：</strong> 样本空间 Ω 的子集。</p>
<ul>
<li><strong>基本事件 (Elementary Event)：</strong> 由一个样本点组成的单点集。</li>
<li><strong>必然事件 (Certain Event)：</strong> 样本空间 Ω 本身 (每次试验中必定发生的事件)。</li>
<li><strong>不可能事件 (Impossible Event)：</strong> 空集 ∅ (每次试验中都不会发生的事件)。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li><strong>抛硬币：</strong><ul>
<li>试验：抛一枚硬币，观察正面 (H) 还是反面 (T) 朝上。</li>
<li>样本空间：Ω = {H, T}</li>
<li>事件：<ul>
<li>出现正面：{H}</li>
<li>出现反面：{T}</li>
<li>必然事件：{H, T}</li>
<li>不可能事件：∅</li>
</ul>
</li>
</ul>
</li>
<li><strong>掷骰子：</strong><ul>
<li>试验：掷一颗骰子，观察出现的点数。</li>
<li>样本空间：Ω = {1, 2, 3, 4, 5, 6}</li>
<li>事件：<ul>
<li>出现偶数点：{2, 4, 6}</li>
<li>出现奇数点：{1, 3, 5}</li>
<li>点数大于 4：{5, 6}</li>
<li>必然事件：{1, 2, 3, 4, 5, 6}</li>
<li>不可能事件：∅</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>1.2 事件的关系与运算</strong></p>
<ul>
<li><strong>包含：</strong> 如果事件 A 发生必然导致事件 B 发生，则称事件 B 包含事件 A，记作 A ⊆ B 或 B ⊇ A。</li>
<li><strong>相等：</strong> 如果 A ⊆ B 且 B ⊆ A，则称事件 A 与事件 B 相等，记作 A = B。</li>
<li><strong>并 (和)：</strong> 事件 A 或事件 B 至少有一个发生，称为事件 A 与事件 B 的并 (或和)，记作 A ∪ B。</li>
<li><strong>交 (积)：</strong> 事件 A 和事件 B 同时发生，称为事件 A 与事件 B 的交 (或积)，记作 A ∩ B 或 AB。</li>
<li><strong>互斥 (互不相容)：</strong> 如果事件 A 和事件 B 不能同时发生 (A ∩ B = ∅)，则称事件 A 与事件 B 互斥。</li>
<li><strong>对立 (互逆)：</strong> 如果事件 A 和事件 B 有且仅有一个发生 (A ∪ B = Ω 且 A ∩ B = ∅)，则称事件 A 与事件 B 互为对立事件，A 的对立事件记作 A̅。</li>
<li><strong>差：</strong> 事件 A 发生但事件 B 不发生，称为事件 A 与事件 B 的差，记作 A - B。</li>
</ul>
<p><strong>1.3 概率</strong></p>
<ul>
<li><p><strong>概率的定义：</strong></p>
<ul>
<li><p><strong>古典定义 (Classical Definition)：</strong> 如果一个随机试验满足：</p>
<ol>
<li>样本空间 Ω 中只有有限个样本点。</li>
<li>每个样本点发生的可能性相等。</li>
</ol>
<p>则事件 A 的概率定义为：P(A) = A 中包含的样本点数 / Ω 中包含的样本点数</p>
<ul>
<li>例如：掷骰子，出现偶数点的概率 P({2, 4, 6}) = 3/6 = 1/2</li>
</ul>
</li>
<li><p><strong>几何定义 (Geometric Definition)：</strong> 如果一个随机试验满足：</p>
<ol>
<li>样本空间 Ω 是一个可以度量的几何区域 (如长度、面积、体积)。</li>
<li>每个样本点落入 Ω 中任何一个可度量的子区域的可能性与该子区域的度量成正比，而与该子区域的位置和形状无关。</li>
</ol>
<p>则事件 A 的概率定义为：P(A) = A 的度量 / Ω 的度量</p>
<ul>
<li>例如：向一个圆形靶子上射击，击中靶心 (一个小圆形区域) 的概率与靶心区域的面积成正比。</li>
</ul>
</li>
<li><p><strong>频率定义 (Frequency Definition)：</strong> 在相同的条件下，重复进行 n 次试验，事件 A 发生的次数 m 称为事件 A 的频数，m/n 称为事件 A 的频率。当 n 足够大时，频率 m/n 会稳定在一个常数附近，这个常数就认为是事件 A 的概率。</p>
</li>
<li><p><strong>公理化定义 (Axiomatic Definition)：</strong> 设 Ω 是一个样本空间，如果对于 Ω 中的每一个事件 A，都赋予一个实数 P(A)，满足以下三个条件：</p>
<ol>
<li><strong>非负性：</strong> 对于任意事件 A，P(A) ≥ 0。</li>
<li><strong>规范性：</strong> P(Ω) = 1。</li>
<li><strong>可列可加性：</strong> 对于两两互斥的事件 A₁, A₂, A₃, ...，P(A₁ ∪ A₂ ∪ A₃ ∪ ...) = P(A₁) + P(A₂) + P(A₃) + ...</li>
</ol>
<p>则称 P(A) 为事件 A 的概率。</p>
</li>
</ul>
</li>
<li><p><strong>概率的性质：</strong></p>
<ul>
<li>0 ≤ P(A) ≤ 1</li>
<li>P(∅) = 0</li>
<li>P(A̅) = 1 - P(A)</li>
<li>如果 A ⊆ B，则 P(B - A) = P(B) - P(A)</li>
<li>P(A ∪ B) = P(A) + P(B) - P(A ∩ B)</li>
</ul>
</li>
</ul>
<p>随机事件是样本空间的子集。概率是用来度量随机事件发生可能性的数值。理解随机事件、样本空间、事件的关系与运算，以及概率的不同定义和性质，是学习概率论的基础。</p>
<p><strong>范例 1：抛硬币</strong></p>
<ol>
<li><p><strong>试验：</strong> 抛一枚均匀的硬币两次，观察正面 (H) 和反面 (T) 出现的情况。</p>
</li>
<li><p><strong>样本空间：</strong> Ω = {HH, HT, TH, TT}</p>
</li>
<li><p><strong>事件：</strong></p>
<ul>
<li>A: 至少出现一次正面 = {HH, HT, TH}</li>
<li>B: 两次都是正面 = {HH}</li>
<li>C: 两次都是反面 = {TT}</li>
<li>D: 一次正面一次反面 = {HT, TH}</li>
</ul>
</li>
<li><p><strong>概率 (古典概型)：</strong></p>
<ul>
<li>P(A) = 3/4</li>
<li>P(B) = 1/4</li>
<li>P(C) = 1/4</li>
<li>P(D) = 2/4 = 1/2</li>
</ul>
</li>
<li><p><strong>Python 代码 (模拟)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟抛硬币</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">toss_coin</span>():</span><br><span class="line">    <span class="keyword">return</span> random.choice([<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟抛硬币两次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">toss_coin_twice</span>():</span><br><span class="line">    <span class="keyword">return</span> [toss_coin(), toss_coin()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟多次试验</span></span><br><span class="line">num_trials = <span class="number">10000</span></span><br><span class="line">outcomes = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_trials):</span><br><span class="line">    outcomes.append(toss_coin_twice())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算事件发生的频率</span></span><br><span class="line">count_A = <span class="number">0</span>  <span class="comment"># 至少一次正面</span></span><br><span class="line">count_B = <span class="number">0</span>  <span class="comment"># 两次都是正面</span></span><br><span class="line">count_C = <span class="number">0</span>  <span class="comment"># 两次都是反面</span></span><br><span class="line">count_D = <span class="number">0</span>  <span class="comment"># 一次正面一次反面</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> outcome <span class="keyword">in</span> outcomes:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;H&#x27;</span> <span class="keyword">in</span> outcome:</span><br><span class="line">        count_A += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> outcome == [<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;H&#x27;</span>]:</span><br><span class="line">        count_B += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> outcome == [<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;T&#x27;</span>]:</span><br><span class="line">        count_C += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> outcome == [<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>] <span class="keyword">or</span> outcome == [<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;H&#x27;</span>]:</span><br><span class="line">        count_D += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出频率 (近似概率)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;至少一次正面的频率: <span class="subst">&#123;count_A / num_trials&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;两次都是正面的频率: <span class="subst">&#123;count_B / num_trials&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;两次都是反面的频率: <span class="subst">&#123;count_C / num_trials&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;一次正面一次反面的频率: <span class="subst">&#123;count_D / num_trials&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>运行这段代码，你会发现频率会接近上面计算的概率。</p>
</li>
</ol>
<p><strong>范例 2：掷骰子</strong></p>
<ol start="6">
<li><p><strong>试验：</strong> 掷一颗均匀的骰子，观察出现的点数。</p>
</li>
<li><p><strong>样本空间：</strong> Ω = {1, 2, 3, 4, 5, 6}</p>
</li>
<li><p><strong>事件：</strong></p>
<ul>
<li>A: 出现偶数点 = {2, 4, 6}</li>
<li>B: 出现奇数点 = {1, 3, 5}</li>
<li>C: 点数大于 4 = {5, 6}</li>
</ul>
</li>
<li><p><strong>概率 (古典概型)：</strong></p>
<ul>
<li>P(A) = 3/6 = 1/2</li>
<li>P(B) = 3/6 = 1/2</li>
<li>P(C) = 2/6 = 1/3</li>
</ul>
</li>
<li><p><strong>Python 代码 (模拟)：</strong></p>
</li>
</ol>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟掷骰子</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">roll_dice</span>():</span><br><span class="line">    <span class="keyword">return</span> random.randint(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟多次试验</span></span><br><span class="line">num_trials = <span class="number">10000</span></span><br><span class="line">outcomes = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_trials):</span><br><span class="line">    outcomes.append(roll_dice())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算事件发生的频率</span></span><br><span class="line">count_A = <span class="number">0</span>  <span class="comment"># 偶数点</span></span><br><span class="line">count_B = <span class="number">0</span>  <span class="comment"># 奇数点</span></span><br><span class="line">count_C = <span class="number">0</span>  <span class="comment"># 点数大于 4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> outcome <span class="keyword">in</span> outcomes:</span><br><span class="line">    <span class="keyword">if</span> outcome <span class="keyword">in</span> [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]:</span><br><span class="line">        count_A += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> outcome <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]:</span><br><span class="line">        count_B += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> outcome <span class="keyword">in</span> [<span class="number">5</span>, <span class="number">6</span>]:</span><br><span class="line">        count_C += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出频率 (近似概率)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;偶数点的频率: <span class="subst">&#123;count_A / num_trials&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;奇数点的频率: <span class="subst">&#123;count_B / num_trials&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;点数大于 4 的频率: <span class="subst">&#123;count_C / num_trials&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
<p><strong>范例 3：事件的关系与运算</strong><br>假设有一个样本空间 S = {1, 2, 3, 4, 5, 6}，对应于掷一个六面骰子的可能结果。我们定义以下事件：</p>
<ul>
<li>事件 A：掷出偶数 {2, 4, 6}</li>
<li>事件 B：掷出大于 3 的数 {4, 5, 6}</li>
</ul>
<p>计算以下事件的概率<br>11. AUB<br>12. A ∩ B<br>13. A&#39; (A的补集)</p>
<p><strong>解答</strong></p>
<ol start="14">
<li><p><strong>A ∪ B（A 或 B 发生）</strong>:<br>*   A ∪ B = {2, 4, 6} ∪ {4, 5, 6} = {2, 4, 5, 6}<br>*   P(A ∪ B) = 4/6 =2/3</p>
</li>
<li><p><strong>A ∩ B（A 和 B 同时发生）</strong>:<br>*   A ∩ B = {2, 4, 6} ∩ {4, 5, 6} = {4, 6}<br>*   P(A ∩ B) = 2/6=1/3</p>
</li>
<li><p><strong>A&#39;（A 的补集，即 A 不发生）</strong>:<br>*   A&#39; = {1, 3, 5}<br>*   P(A&#39;) = 3/6=1/2</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 定义样本空间</span></span><br><span class="line">S = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义事件 A 和 B</span></span><br><span class="line">A = &#123;<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>&#125;</span><br><span class="line">B = &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 A ∪ B</span></span><br><span class="line">A_union_B = A.union(B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 A ∩ B</span></span><br><span class="line">A_intersect_B = A.intersection(B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 A 的补集</span></span><br><span class="line">A_complement = S - A</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A ∪ B:&quot;</span>, A_union_B)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A ∩ B:&quot;</span>, A_intersect_B)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A&#x27;:&quot;</span>, A_complement)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟投骰子</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">roll_dice</span>():</span><br><span class="line">    <span class="keyword">return</span> np.random.choice(<span class="built_in">list</span>(S))</span><br><span class="line"></span><br><span class="line">n=<span class="number">10000</span></span><br><span class="line"><span class="comment">#在n次实验中， 统计AUB, A∩B, A&#x27; 发生的次数</span></span><br><span class="line">count_aub = <span class="number">0</span></span><br><span class="line">count_aib = <span class="number">0</span></span><br><span class="line">count_ac = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">  r = roll_dice()</span><br><span class="line">  <span class="keyword">if</span> r <span class="keyword">in</span> A_union_B:</span><br><span class="line">    count_aub +=<span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> r <span class="keyword">in</span> A_intersect_B:</span><br><span class="line">    count_aib += <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> r <span class="keyword">in</span> A_complement:</span><br><span class="line">    count_ac += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;P(AUB)=&quot;</span>, count_aub/n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;P(A∩B)=&quot;</span>, count_aib/n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;P(A&#x27;)=&quot;</span>, count_ac/n)</span><br></pre></td></tr></table></figure>

<p>通过这些范例，你可以更直观地理解随机事件、样本空间、概率的概念，以及如何用 Python 进行简单的模拟和计算。</p>
<hr>
<h3 id="2-条件概率"><a href="#2-条件概率" class="headerlink" title="2. 条件概率"></a>2. 条件概率</h3><p><strong>2.1 什么是条件概率？</strong></p>
<ul>
<li><p><strong>背景：</strong>  在许多情况下，我们需要知道在某个事件已经发生的条件下，另一个事件发生的概率。</p>
</li>
<li><p><strong>定义：</strong>  设 A 和 B 是两个事件，且 P(A) &gt; 0，在事件 A 已经发生的条件下，事件 B 发生的概率称为<strong>条件概率</strong> (Conditional Probability)，记作 P(B|A)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(B|A) = P(A ∩ B) / P(A)</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>P(A ∩ B) 表示事件 A 和事件 B 同时发生的概率。</li>
<li>P(A) 表示事件 A 发生的概率。</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>条件概率 P(B|A) 并不是 B 的概率，而是在 A 发生的前提下，B 的概率。</li>
<li>条件概率 P(B|A) 改变了样本空间，将原本的样本空间 Ω 缩小为 A。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<p>假设一个盒子里有 5 个球，其中 3 个是红球，2 个是白球。</p>
<ul>
<li>事件 A：第一次摸到红球。</li>
<li>事件 B：第二次摸到红球。</li>
</ul>
<p>如果不放回：</p>
<ul>
<li>P(A) = 3/5</li>
<li>P(A ∩ B) = (3/5) * (2/4) = 3/10  (第一次摸到红球后，剩下 4 个球，其中 2 个是红球)</li>
<li>P(B|A) = P(A ∩ B) / P(A) = (3/10) / (3/5) = 1/2</li>
</ul>
<p>如果放回：</p>
<ul>
<li>P(A) = 3/5</li>
<li>P(A∩B) = (3/5)*(3/5) = 9/25</li>
<li>P(B|A) = P(A∩B)/P(A) = (9/25)/(3/5) = 3/5</li>
</ul>
<p><strong>2.2 条件概率的性质</strong></p>
<ul>
<li>非负性：P(B|A) ≥ 0</li>
<li>规范性：P(Ω|A) = 1</li>
<li>可列可加性：如果 B₁, B₂, B₃, ... 是两两互斥的事件，则 P(B₁ ∪ B₂ ∪ B₃ ∪ ... | A) = P(B₁|A) + P(B₂|A) + P(B₃|A) + ...</li>
<li>P(B̅|A) = 1 - P(B|A)</li>
<li>乘法公式：P(A ∩ B) = P(A)P(B|A) = P(B)P(A|B)</li>
</ul>
<p><strong>2.3 全概率公式</strong></p>
<ul>
<li><p><strong>背景：</strong>  有时我们需要计算一个事件 B 的概率，但是直接计算 P(B) 比较困难，而将 B 分解成若干个互斥事件的并集后，更容易计算。</p>
</li>
<li><p><strong>全概率公式 (Law of Total Probability)：</strong><br>设事件 A₁, A₂, ..., Aₙ 是样本空间 Ω 的一个划分 (即 A₁, A₂, ..., Aₙ 两两互斥，且 A₁ ∪ A₂ ∪ ... ∪ Aₙ = Ω)，且 P(Aᵢ) &gt; 0，则对于任意事件 B，有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(B) = P(A₁)P(B|A₁) + P(A₂)P(B|A₂) + ... + P(Aₙ)P(B|Aₙ)</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(B) = ∑ P(Aᵢ)P(B|Aᵢ)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>全概率公式将事件 B 的概率分解为在不同“原因” (Aᵢ) 下 B 发生的概率的加权平均。</li>
<li>权重是每个“原因”发生的概率 P(Aᵢ)。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<p>假设有两家工厂生产同一种产品，甲厂的产量占 60%，乙厂的产量占 40%。甲厂产品的合格率为 95%，乙厂产品的合格率为 90%。现在从这两种产品中随机抽取一件，求它是合格品的概率。</p>
<ul>
<li>事件 A₁：产品是甲厂生产的。</li>
<li>事件 A₂：产品是乙厂生产的。</li>
<li>事件 B：产品是合格品。</li>
</ul>
<p>已知：</p>
<ul>
<li>P(A₁) = 0.6</li>
<li>P(A₂) = 0.4</li>
<li>P(B|A₁) = 0.95</li>
<li>P(B|A₂) = 0.90</li>
</ul>
<p>根据全概率公式：<br>P(B) = P(A₁)P(B|A₁) + P(A₂)P(B|A₂) = 0.6 * 0.95 + 0.4 * 0.90 = 0.93</p>
<p><strong>2.4 贝叶斯公式</strong></p>
<ul>
<li><p><strong>背景：</strong>  贝叶斯公式 (Bayes&#39; Theorem) 描述了在已知一些条件下，事件发生的概率是如何改变的。它将 P(B|A) 和 P(A|B) 联系起来。</p>
</li>
<li><p><strong>贝叶斯公式：</strong><br>设事件 A₁, A₂, ..., Aₙ 是样本空间 Ω 的一个划分，且 P(Aᵢ) &gt; 0，P(B) &gt; 0，则：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(Aᵢ|B) = [P(Aᵢ)P(B|Aᵢ)] / P(B) =  [P(Aᵢ)P(B|Aᵢ)] / [∑ P(Aⱼ)P(B|Aⱼ)]</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>P(Aᵢ|B) 是后验概率 (Posterior Probability)，表示在观察到事件 B 发生后，事件 Aᵢ 发生的概率。</li>
<li>P(Aᵢ) 是先验概率 (Prior Probability)，表示在没有观察到事件 B 时，事件 Aᵢ 发生的概率。</li>
<li>P(B|Aᵢ) 是似然函数 (Likelihood Function)，表示在事件 Aᵢ 发生的条件下，事件 B 发生的概率。</li>
<li>P(B) 是证据 (Evidence)，可以用全概率公式计算。</li>
<li>贝叶斯公式可以看作是根据新的信息 (事件 B) 来更新我们对事件 Aᵢ 的概率的认识。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong><br>沿用全概率公式中的例子， 如果抽到一件产品是合格的，求它是甲厂生产的概率。</p>
<ul>
<li>P(A₁|B) = [P(A₁)P(B|A₁)]/P(B) = [0.6*0.95]/0.93 = 0.613</li>
</ul>
<p>条件概率是在某个事件已经发生的条件下，另一个事件发生的概率。全概率公式将一个事件的概率分解为在不同条件下发生的概率的加权平均。贝叶斯公式描述了在已知一些条件下，事件发生的概率是如何改变的。</p>
<p><strong>范例 1：盒子摸球 (条件概率)</strong></p>
<p>假设一个盒子里有 5 个红球和 3 个白球。</p>
<ol>
<li><p><strong>不放回摸球：</strong></p>
<ul>
<li>事件 A：第一次摸到红球。 P(A) = 5/8</li>
<li>事件 B：第二次摸到红球。</li>
<li>求 P(B|A)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 P(B|A)</span></span><br><span class="line">p_A = <span class="number">5</span>/<span class="number">8</span></span><br><span class="line">p_A_and_B = (<span class="number">5</span>/<span class="number">8</span>) * (<span class="number">4</span>/<span class="number">7</span>)  <span class="comment"># 第一次红球，第二次红球</span></span><br><span class="line">p_B_given_A = p_A_and_B / p_A</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;不放回摸球，P(B|A) = <span class="subst">&#123;p_B_given_A&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.5714 (4/7)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>放回摸球：</strong></p>
<ul>
<li>事件 A：第一次摸到红球。 P(A) = 5/8</li>
<li>事件 B：第二次摸到红球。</li>
<li>求 P(B|A)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 P(B|A)</span></span><br><span class="line">p_A = <span class="number">5</span>/<span class="number">8</span></span><br><span class="line">p_A_and_B = (<span class="number">5</span>/<span class="number">8</span>) * (<span class="number">5</span>/<span class="number">8</span>)  <span class="comment"># 第一次红球，第二次红球</span></span><br><span class="line">p_B_given_A = p_A_and_B / p_A</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;放回摸球，P(B|A) = <span class="subst">&#123;p_B_given_A&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.625 (5/8)</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 2：两家工厂生产的产品 (全概率公式和贝叶斯公式)</strong></p>
<p>假设有两家工厂 (A 和 B) 生产同一种零件。A 厂生产 60% 的零件，B 厂生产 40% 的零件。A 厂的次品率为 2%，B 厂的次品率为 5%。</p>
<ol>
<li><p><strong>求任取一个零件是次品的概率 (全概率公式)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 已知条件</span></span><br><span class="line">p_A = <span class="number">0.6</span>  <span class="comment"># A 厂生产的概率</span></span><br><span class="line">p_B = <span class="number">0.4</span>  <span class="comment"># B 厂生产的概率</span></span><br><span class="line">p_defective_given_A = <span class="number">0.02</span>  <span class="comment"># A 厂次品率</span></span><br><span class="line">p_defective_given_B = <span class="number">0.05</span>  <span class="comment"># B 厂次品率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全概率公式计算 P(defective)</span></span><br><span class="line">p_defective = p_A * p_defective_given_A + p_B * p_defective_given_B</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;任取一个零件是次品的概率: <span class="subst">&#123;p_defective&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.032</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>如果取到一个零件是次品，求它是 A 厂生产的概率 (贝叶斯公式)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 贝叶斯公式计算 P(A|defective)</span></span><br><span class="line">p_A_given_defective = (p_A * p_defective_given_A) / p_defective</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;取到次品，是 A 厂生产的概率: <span class="subst">&#123;p_A_given_defective&#125;</span>&quot;</span>) <span class="comment"># 输出: 0.375</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 3：疾病检测 (贝叶斯公式)</strong></p>
<p>假设某种疾病的发病率为 0.1%。现在有一种检测方法，对于患病者，检测结果为阳性的概率为 99%；对于未患病者，检测结果为阳性的概率为 1% (假阳性)。</p>
<p>如果一个人的检测结果为阳性，求他实际患病的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 已知条件</span></span><br><span class="line">p_disease = <span class="number">0.001</span>  <span class="comment"># 发病率</span></span><br><span class="line">p_positive_given_disease = <span class="number">0.99</span>  <span class="comment"># 患病者检测阳性率</span></span><br><span class="line">p_positive_given_no_disease = <span class="number">0.01</span>  <span class="comment"># 未患病者检测阳性率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全概率公式计算 P(positive)</span></span><br><span class="line">p_no_disease = <span class="number">1</span> - p_disease</span><br><span class="line">p_positive = p_disease * p_positive_given_disease + p_no_disease * p_positive_given_no_disease</span><br><span class="line"></span><br><span class="line"><span class="comment"># 贝叶斯公式计算 P(disease|positive)</span></span><br><span class="line">p_disease_given_positive = (p_disease * p_positive_given_disease) / p_positive</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;检测结果阳性，实际患病的概率: <span class="subst">&#123;p_disease_given_positive&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.09016</span></span><br></pre></td></tr></table></figure>
<p>结果分析：<br>尽管检测的准确性相当高（对于患病者有 99% 的阳性率，对于未患病者有 99% 的阴性率），但由于疾病的低发病率，即使检测结果为阳性，实际患病的概率也相对较低（大约 9.02%）。这是因为在大量检测中，假阳性的人数可能会超过真阳性的人数。</p>
<p>通过这些范例，你可以看到条件概率、全概率公式和贝叶斯公式在不同场景下的应用，以及如何用 Python 代码进行计算。这些概念在深度学习中非常重要，例如在朴素贝叶斯分类器、概率图模型等算法中都有应用。</p>
<hr>
<p>好的，我们继续学习概率论与数理统计的下一个知识点：<strong>随机变量</strong>。</p>
<h3 id="3-随机变量"><a href="#3-随机变量" class="headerlink" title="3. 随机变量"></a>3. 随机变量</h3><p><strong>3.1 什么是随机变量？</strong></p>
<ul>
<li><p><strong>定义：</strong> 随机变量 (Random Variable) 是一个变量，其取值依赖于随机试验的结果。</p>
<ul>
<li>更严格地说，随机变量是从样本空间 Ω 到实数集 R 的一个函数：X = X(ω), ω ∈ Ω。</li>
<li>用大写字母表示随机变量，如 X, Y, Z。</li>
<li>用小写字母表示随机变量的取值，如 x, y, z。</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>随机变量将随机试验的结果 (样本点) 映射为数值。</li>
<li>随机变量的取值具有不确定性，但取每个值的概率是可以确定的。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ul>
<li><strong>抛硬币：</strong><ul>
<li>试验：抛一枚硬币两次。</li>
<li>样本空间：Ω = {HH, HT, TH, TT}</li>
<li>定义随机变量 X 为两次抛硬币中出现正面的次数。<ul>
<li>X(HH) = 2</li>
<li>X(HT) = 1</li>
<li>X(TH) = 1</li>
<li>X(TT) = 0</li>
<li>X 的可能取值为 {0, 1, 2}</li>
</ul>
</li>
</ul>
</li>
<li><strong>掷骰子：</strong><ul>
<li>试验：掷一颗骰子。</li>
<li>样本空间：Ω = {1, 2, 3, 4, 5, 6}</li>
<li>定义随机变量 Y 为掷骰子出现的点数。<ul>
<li>Y(1) = 1, Y(2) = 2, ..., Y(6) = 6</li>
<li>Y 的可能取值为 {1, 2, 3, 4, 5, 6}</li>
</ul>
</li>
</ul>
</li>
<li><strong>灯泡寿命：</strong><ul>
<li>试验：测试一个灯泡的寿命。</li>
<li>样本空间：Ω = {t | t ≥ 0} (t 表示时间)</li>
<li>定义随机变量 T 为灯泡的寿命。<ul>
<li>T 的可能取值为所有非负实数。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>3.2 随机变量的分类</strong></p>
<ul>
<li><strong>离散型随机变量 (Discrete Random Variable)：</strong> 所有可能取值是有限个或可列无限个的随机变量。<ul>
<li>例如：抛硬币两次出现正面的次数、掷骰子出现的点数、一批产品中的次品数。</li>
</ul>
</li>
<li><strong>连续型随机变量 (Continuous Random Variable)：</strong> 所有可能取值充满一个区间 (或多个区间) 的随机变量。<ul>
<li>例如：灯泡的寿命、人的身高、温度。</li>
</ul>
</li>
</ul>
<p><strong>3.3 离散型随机变量的概率分布</strong></p>
<ul>
<li><p><strong>概率分布列 (Probability Distribution)：</strong> 描述离散型随机变量 X 的所有可能取值及其对应的概率。</p>
<table>
<thead>
<tr>
<th>X</th>
<th>x₁</th>
<th>x₂</th>
<th>...</th>
<th>xₙ</th>
<th>...</th>
</tr>
</thead>
<tbody><tr>
<td>P(X=xᵢ)</td>
<td>p₁</td>
<td>p₂</td>
<td>...</td>
<td>pₙ</td>
<td>...</td>
</tr>
</tbody></table>
<p>其中：</p>
<ul>
<li>pᵢ = P(X = xᵢ) ≥ 0</li>
<li>∑pᵢ = 1</li>
</ul>
</li>
<li><p><strong>常见离散型分布：</strong></p>
<ul>
<li><strong>伯努利分布 (Bernoulli Distribution) (0-1 分布)：</strong><ul>
<li>试验：只有两个可能结果 (成功或失败) 的试验。</li>
<li>随机变量 X：如果成功，X = 1；如果失败，X = 0。</li>
<li>概率分布：P(X = 1) = p, P(X = 0) = 1 - p (0 &lt; p &lt; 1)</li>
</ul>
</li>
<li><strong>二项分布 (Binomial Distribution)：</strong><ul>
<li>试验：在相同条件下独立重复进行 n 次伯努利试验。</li>
<li>随机变量 X：n 次试验中成功的次数。</li>
<li>概率分布：P(X = k) = C(n, k) * pᵏ * (1 - p)ⁿ⁻ᵏ (k = 0, 1, 2, ..., n)，其中 C(n, k) = n! / (k!(n-k)!) 是组合数。</li>
</ul>
</li>
<li><strong>泊松分布 (Poisson Distribution)：</strong><ul>
<li>描述在一定时间或空间内，稀有事件发生的次数。</li>
<li>概率分布：P(X = k) = (λᵏ * e⁻ˡᵃᵐᵇᵈᵃ) / k! (k = 0, 1, 2, ...)，其中 λ &gt; 0 是一个常数。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>3.4 连续型随机变量的概率分布</strong></p>
<ul>
<li><p><strong>概率密度函数 (Probability Density Function, PDF)：</strong></p>
<ul>
<li>对于连续型随机变量 X，用一个函数 f(x) 来描述其概率分布。</li>
<li>f(x) 满足：<ul>
<li>f(x) ≥ 0</li>
<li>∫f(x)dx = 1 (积分区间为整个实数轴)</li>
</ul>
</li>
<li>P(a ≤ X ≤ b) = ∫[a,b] f(x)dx (表示 X 取值在 a 和 b 之间的概率)</li>
<li>注意：对于连续型随机变量，P(X = c) = 0 (c 是一个常数)。</li>
</ul>
</li>
<li><p><strong>常见连续型分布：</strong></p>
<ul>
<li><strong>均匀分布 (Uniform Distribution)：</strong><ul>
<li>在区间 [a, b] 上，随机变量取任意值的概率相等。</li>
<li>概率密度函数：f(x) = 1 / (b - a) (a ≤ x ≤ b), f(x) = 0 (其他)</li>
</ul>
</li>
<li><strong>指数分布 (Exponential Distribution)：</strong><ul>
<li>描述独立随机事件发生的时间间隔。</li>
<li>概率密度函数：f(x) = λe⁻ˡᵃᵐᵇᵈᵃˣ (x ≥ 0), f(x) = 0 (x &lt; 0)，其中 λ &gt; 0 是一个常数。</li>
</ul>
</li>
<li><strong>正态分布 (Normal Distribution) (高斯分布 Gaussian Distribution)：</strong><ul>
<li>一种非常常见的连续型分布，许多自然现象和社会现象都近似服从正态分布。</li>
<li>概率密度函数：f(x) = (1 / (√(2π)σ)) * e^(-(x-μ)² / (2σ²))，其中 μ 是均值，σ 是标准差。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>随机变量是将随机试验的结果映射为数值的变量。离散型随机变量的概率分布用概率分布列描述，连续型随机变量的概率分布用概率密度函数描述。</p>
<p><strong>范例 1：离散型随机变量 - 抛硬币</strong></p>
<ol>
<li><p><strong>试验：</strong> 抛一枚均匀的硬币三次。</p>
</li>
<li><p><strong>随机变量 X：</strong> 出现正面的次数。</p>
</li>
<li><p><strong>概率分布列：</strong></p>
<table>
<thead>
<tr>
<th>X</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td>P(X=x)</td>
<td>1/8</td>
<td>3/8</td>
<td>3/8</td>
<td>1/8</td>
</tr>
</tbody></table>
</li>
<li><p><strong>Python 代码 (模拟和可视化)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟抛硬币三次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">toss_coin_thrice</span>():</span><br><span class="line">    outcomes = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        outcomes.append(random.choice([<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> outcomes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算出现正面的次数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_heads</span>(<span class="params">outcomes</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> outcome <span class="keyword">in</span> outcomes:</span><br><span class="line">        <span class="keyword">if</span> outcome == <span class="string">&#x27;H&#x27;</span>:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟多次试验</span></span><br><span class="line">num_trials = <span class="number">10000</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_trials):</span><br><span class="line">    outcomes = toss_coin_thrice()</span><br><span class="line">    results.append(count_heads(outcomes))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计每个值的频率</span></span><br><span class="line">values, counts = np.unique(results, return_counts=<span class="literal">True</span>)</span><br><span class="line">frequencies = counts / num_trials</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.bar(values, frequencies)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Heads (X)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Frequency (Approximate Probability)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Probability Distribution of X (Number of Heads in 3 Tosses)&#x27;</span>)</span><br><span class="line">plt.xticks(values)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行这段代码，你可以看到一个条形图，显示了抛硬币三次出现正面次数的概率分布（模拟结果）。</p>
</li>
</ol>
<p><strong>范例 2：连续型随机变量 - 指数分布</strong></p>
<ol>
<li><p><strong>随机变量 X：</strong> 电子元件的寿命 (单位：小时)。</p>
</li>
<li><p><strong>假设 X 服从指数分布，参数 λ = 0.001。</strong></p>
</li>
<li><p><strong>概率密度函数：</strong> f(x) = 0.001 * e^(-0.001x) (x ≥ 0)</p>
</li>
<li><p><strong>Python 代码 (可视化)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> expon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">lambda_val = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 x 值</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">5000</span>, <span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算概率密度函数</span></span><br><span class="line">pdf = expon.pdf(x, scale=<span class="number">1</span>/lambda_val)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.plot(x, pdf)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x (Hours)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability Density f(x)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Exponential Distribution (λ = 0.001)&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 P(1000 ≤ X ≤ 2000)</span></span><br><span class="line">prob = expon.cdf(<span class="number">2000</span>, scale=<span class="number">1</span>/lambda_val) - expon.cdf(<span class="number">1000</span>, scale=<span class="number">1</span>/lambda_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;P(1000 ≤ X ≤ 2000) = <span class="subst">&#123;prob&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这段代码会绘制指数分布的概率密度函数图像，并计算 X 在 1000 到 2000 之间的概率。</p>
</li>
</ol>
<p><strong>范例 3：正态分布</strong></p>
<ol>
<li><p><strong>随机变量 X：</strong> 某地区成年男性的身高 (单位：cm)。</p>
</li>
<li><p><strong>假设 X 服从正态分布，均值 μ = 170，标准差 σ = 5。</strong></p>
</li>
<li><p><strong>概率密度函数：</strong> f(x) = (1 / (√(2π) * 5)) * e^(-(x-170)² / (2 * 5²))</p>
</li>
<li><p><strong>Python 代码 (可视化)：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">mu = <span class="number">170</span></span><br><span class="line">sigma = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 x 值</span></span><br><span class="line">x = np.linspace(<span class="number">150</span>, <span class="number">190</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算概率密度函数</span></span><br><span class="line">pdf = norm.pdf(x, loc=mu, scale=sigma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.plot(x, pdf)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x (Height in cm)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability Density f(x)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Normal Distribution (μ = 170, σ = 5)&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 P(165 ≤ X ≤ 175)</span></span><br><span class="line">prob = norm.cdf(<span class="number">175</span>, loc=mu, scale=sigma) - norm.cdf(<span class="number">165</span>, loc=mu, scale=sigma)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;P(165 ≤ X ≤ 175) = <span class="subst">&#123;prob&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码会绘制出一个正态分布的概率密度函数，即我们常说的“钟形曲线”，并计算出身高在165cm到175cm之间的概率</p>
</li>
</ol>
<p>通过这些范例，你可以看到离散型随机变量和连续型随机变量的概率分布是如何表示的，以及如何用 Python 进行模拟和可视化。</p>
<hr>
<h3 id="4-随机变量的数字特征"><a href="#4-随机变量的数字特征" class="headerlink" title="4. 随机变量的数字特征"></a>4. 随机变量的数字特征</h3><p><strong>4.1 期望 (均值)</strong></p>
<ul>
<li><p><strong>定义：</strong> 期望 (Expectation) 或均值 (Mean) 是随机变量取值的平均水平的度量。</p>
</li>
<li><p><strong>离散型随机变量：</strong><br>如果离散型随机变量 X 的概率分布列为 P(X = xᵢ) = pᵢ，则 X 的期望定义为：</p>
<p>E(X) = ∑ xᵢpᵢ</p>
</li>
<li><p><strong>连续型随机变量：</strong><br>如果连续型随机变量 X 的概率密度函数为 f(x)，则 X 的期望定义为：</p>
<p>E(X) = ∫ xf(x) dx (积分区间为整个实数轴)</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>期望是随机变量所有可能取值的加权平均，权重是每个取值出现的概率。</li>
<li>期望反映了随机变量的“中心”位置。</li>
</ul>
</li>
<li><p><strong>性质：</strong></p>
<ul>
<li>E(c) = c (c 是常数)</li>
<li>E(cX) = cE(X) (c 是常数)</li>
<li>E(X + Y) = E(X) + E(Y) (X 和 Y 是任意两个随机变量)</li>
<li>如果 X 和 Y 相互独立，则 E(XY) = E(X)E(Y)</li>
</ul>
</li>
</ul>
<p><strong>4.2 方差</strong></p>
<ul>
<li><p><strong>定义：</strong> 方差 (Variance) 是随机变量取值与其期望的偏离程度的度量。</p>
<p>Var(X) = E[(X - E(X))²]</p>
</li>
<li><p><strong>离散型随机变量：</strong><br>Var(X) = ∑ (xᵢ - E(X))²pᵢ</p>
</li>
<li><p><strong>连续型随机变量：</strong><br>Var(X) = ∫ (x - E(X))²f(x) dx</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>方差越大，随机变量的取值越分散。</li>
<li>方差越小，随机变量的取值越集中在期望附近。</li>
</ul>
</li>
<li><p><strong>性质：</strong></p>
<ul>
<li>Var(c) = 0 (c 是常数)</li>
<li>Var(cX) = c²Var(X) (c 是常数)</li>
<li>如果 X 和 Y 相互独立，则 Var(X + Y) = Var(X) + Var(Y)</li>
<li>Var(X) = E(X²) - (E(X))²</li>
</ul>
</li>
</ul>
<p><strong>4.3 标准差</strong></p>
<ul>
<li><p><strong>定义：</strong> 标准差 (Standard Deviation) 是方差的算术平方根。</p>
<p>SD(X) = √Var(X)</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>标准差与随机变量具有相同的量纲，更直观地反映了随机变量取值的离散程度。</li>
</ul>
</li>
</ul>
<p><strong>4.4 协方差</strong></p>
<ul>
<li><p><strong>定义：</strong> 协方差 (Covariance) 是衡量两个随机变量 X 和 Y 的线性相关程度的量。</p>
<p>Cov(X, Y) = E[(X - E(X))(Y - E(Y))]</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>Cov(X, Y) &gt; 0：X 和 Y 正相关 (当 X 增大时，Y 也倾向于增大)。</li>
<li>Cov(X, Y) &lt; 0：X 和 Y 负相关 (当 X 增大时，Y 倾向于减小)。</li>
<li>Cov(X, Y) = 0：X 和 Y 不线性相关 (但不一定独立)。</li>
</ul>
</li>
<li><p><strong>性质：</strong></p>
<ul>
<li>Cov(X, X) = Var(X)</li>
<li>Cov(X, Y) = Cov(Y, X)</li>
<li>Cov(aX, bY) = abCov(X, Y) (a, b 是常数)</li>
<li>Cov(X + a, Y + b) = Cov(X, Y) (a, b 是常数)</li>
<li>Cov(X, Y) = E(XY) - E(X)E(Y)</li>
</ul>
</li>
</ul>
<p><strong>4.5 相关系数</strong></p>
<ul>
<li><p><strong>定义：</strong> 相关系数 (Correlation Coefficient) 是衡量两个随机变量 X 和 Y 的线性相关程度的标准化量。</p>
<p>ρ(X, Y) = Cov(X, Y) / (√Var(X) * √Var(Y))</p>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>-1 ≤ ρ(X, Y) ≤ 1</li>
<li>ρ(X, Y) = 1：X 和 Y 完全正相关。</li>
<li>ρ(X, Y) = -1：X 和 Y 完全负相关。</li>
<li>ρ(X, Y) = 0：X 和 Y 不线性相关。</li>
<li>|ρ(X, Y)| 越接近 1，X 和 Y 的线性相关性越强。</li>
</ul>
</li>
</ul>
<p>期望、方差、标准差、协方差和相关系数是描述随机变量数字特征的重要概念。期望反映了随机变量的平均水平，方差和标准差反映了随机变量取值的离散程度，协方差和相关系数反映了两个随机变量之间的线性相关程度。</p>
<p><strong>范例 1：离散型随机变量 - 抛硬币</strong></p>
<ol>
<li><p><strong>试验：</strong> 抛一枚均匀的硬币三次。</p>
</li>
<li><p><strong>随机变量 X：</strong> 出现正面的次数。</p>
</li>
<li><p><strong>概率分布列：</strong></p>
<table>
<thead>
<tr>
<th>X</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td>P(X=x)</td>
<td>1/8</td>
<td>3/8</td>
<td>3/8</td>
<td>1/8</td>
</tr>
</tbody></table>
</li>
<li><p><strong>计算期望、方差和标准差：</strong></p>
<ul>
<li>E(X) = 0 * (1/8) + 1 * (3/8) + 2 * (3/8) + 3 * (1/8) = 1.5</li>
<li>Var(X) = (0 - 1.5)² * (1/8) + (1 - 1.5)² * (3/8) + (2 - 1.5)² * (3/8) + (3 - 1.5)² * (1/8) = 0.75</li>
<li>SD(X) = √Var(X) = √0.75 ≈ 0.866</li>
</ul>
</li>
<li><p><strong>Python 代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 概率分布列</span></span><br><span class="line">x_values = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">probabilities = np.array([<span class="number">1</span>/<span class="number">8</span>, <span class="number">3</span>/<span class="number">8</span>, <span class="number">3</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算期望</span></span><br><span class="line">expected_value = np.<span class="built_in">sum</span>(x_values * probabilities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;E(X) = <span class="subst">&#123;expected_value&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算方差</span></span><br><span class="line">variance = np.<span class="built_in">sum</span>((x_values - expected_value)**<span class="number">2</span> * probabilities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Var(X) = <span class="subst">&#123;variance&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算标准差</span></span><br><span class="line">standard_deviation = np.sqrt(variance)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SD(X) = <span class="subst">&#123;standard_deviation&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 2：连续型随机变量 - 指数分布</strong></p>
<ol start="6">
<li><p><strong>随机变量 X：</strong> 电子元件的寿命 (单位：小时)。</p>
</li>
<li><p><strong>假设 X 服从指数分布，参数 λ = 0.001。</strong></p>
</li>
<li><p><strong>概率密度函数：</strong> f(x) = 0.001 * e^(-0.001x) (x ≥ 0)</p>
</li>
<li><p><strong>计算期望、方差和标准差：</strong></p>
<ul>
<li>E(X) = 1/λ = 1/0.001 = 1000</li>
<li>Var(X) = 1/λ² = 1/0.001² = 1,000,000</li>
<li>SD(X) = √Var(X) = 1000</li>
</ul>
</li>
<li><p><strong>Python 代码：</strong></p>
</li>
</ol>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> expon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">lambda_val = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算期望、方差和标准差</span></span><br><span class="line">expected_value = expon.mean(scale=<span class="number">1</span>/lambda_val)</span><br><span class="line">variance = expon.var(scale=<span class="number">1</span>/lambda_val)</span><br><span class="line">standard_deviation = expon.std(scale=<span class="number">1</span>/lambda_val)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;E(X) = <span class="subst">&#123;expected_value&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Var(X) = <span class="subst">&#123;variance&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SD(X) = <span class="subst">&#123;standard_deviation&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
<p><strong>范例 3：正态分布</strong></p>
<ol start="11">
<li><p><strong>随机变量 X：</strong> 某地区成年男性的身高 (单位：cm)。</p>
</li>
<li><p><strong>假设 X 服从正态分布，均值 μ = 170，标准差 σ = 5。</strong></p>
</li>
<li><p><strong>Python 代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">mu = <span class="number">170</span></span><br><span class="line">sigma = <span class="number">5</span></span><br><span class="line"><span class="comment"># 计算期望、方差和标准差</span></span><br><span class="line">expected_value = norm.mean(loc=mu, scale=sigma)</span><br><span class="line">variance = norm.var(loc=mu, scale=sigma)</span><br><span class="line">standard_deviation = norm.std(loc=mu, scale=sigma)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;E(X) = <span class="subst">&#123;expected_value&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Var(X) = <span class="subst">&#123;variance&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;SD(X) = <span class="subst">&#123;standard_deviation&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 4：协方差和相关系数</strong></p>
<p>假设有两个随机变量 X 和 Y，它们的联合概率分布如下：</p>
<table>
<thead>
<tr>
<th>X\Y</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.1</td>
<td>0.2</td>
<td>0.1</td>
</tr>
<tr>
<td>2</td>
<td>0.1</td>
<td>0.1</td>
<td>0.4</td>
</tr>
</tbody></table>
<ol start="14">
<li><strong>计算 E(X), E(Y), Var(X), Var(Y), Cov(X, Y), ρ(X, Y)。</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 联合概率分布</span></span><br><span class="line">joint_prob = np.array([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.1</span>],</span><br><span class="line">                       [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># X 和 Y 的取值</span></span><br><span class="line">x_values = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y_values = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算边际概率分布</span></span><br><span class="line">prob_X = np.<span class="built_in">sum</span>(joint_prob, axis=<span class="number">1</span>)</span><br><span class="line">prob_Y = np.<span class="built_in">sum</span>(joint_prob, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算期望</span></span><br><span class="line">E_X = np.<span class="built_in">sum</span>(x_values * prob_X)</span><br><span class="line">E_Y = np.<span class="built_in">sum</span>(y_values * prob_Y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;E(X) = <span class="subst">&#123;E_X&#125;</span>&quot;</span>)  <span class="comment"># 输出：1.6</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;E(Y) = <span class="subst">&#123;E_Y&#125;</span>&quot;</span>)  <span class="comment"># 输出：2.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算方差</span></span><br><span class="line">Var_X = np.<span class="built_in">sum</span>((x_values - E_X)**<span class="number">2</span> * prob_X)</span><br><span class="line">Var_Y = np.<span class="built_in">sum</span>((y_values - E_Y)**<span class="number">2</span> * prob_Y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Var(X) = <span class="subst">&#123;Var_X&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.24</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Var(Y) = <span class="subst">&#123;Var_Y&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.56</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算协方差</span></span><br><span class="line">Cov_XY = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_values)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_values)):</span><br><span class="line">        Cov_XY += (x_values[i] - E_X) * (y_values[j] - E_Y) * joint_prob[i, j]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cov(X, Y) = <span class="subst">&#123;Cov_XY&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数</span></span><br><span class="line">Corr_XY = Cov_XY / (np.sqrt(Var_X) * np.sqrt(Var_Y))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ρ(X, Y) = <span class="subst">&#123;Corr_XY&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.3273</span></span><br></pre></td></tr></table></figure>

<p>通过这些范例，你可以看到如何计算随机变量的期望、方差、标准差、协方差和相关系数，以及如何用 Python 代码来实现。这些数字特征是描述随机变量的重要工具。</p>
<hr>
<h3 id="5-最大似然估计-MLE"><a href="#5-最大似然估计-MLE" class="headerlink" title="5. 最大似然估计 (MLE)"></a>5. 最大似然估计 (MLE)</h3><p><strong>5.1 什么是最大似然估计？</strong></p>
<ul>
<li><p><strong>背景：</strong> 在统计学中，我们经常需要根据一组观测数据来估计一个概率模型的参数。最大似然估计 (Maximum Likelihood Estimation, MLE) 是一种常用的参数估计方法。</p>
</li>
<li><p><strong>思想：</strong> 找到一组参数，使得在这组参数下，观测数据出现的概率最大。</p>
</li>
<li><p><strong>“似然” (Likelihood) 的含义：</strong></p>
<ul>
<li>似然函数 (Likelihood Function) 描述了在给定参数下，观测数据出现的可能性大小。</li>
<li>似然函数与概率函数不同：<ul>
<li>概率函数：已知参数，求观测数据出现的概率。</li>
<li>似然函数：已知观测数据，求参数的可能性大小。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>5.2 最大似然估计的步骤</strong></p>
<ol>
<li><p><strong>写出似然函数：</strong></p>
<ul>
<li><p>假设我们有一组独立同分布的观测数据 x₁, x₂, ..., xₙ，它们来自一个概率模型，该模型的参数为 θ (可以是一个参数，也可以是一个参数向量)。</p>
</li>
<li><p>似然函数 L(θ) 定义为：</p>
<p>L(θ) = P(x₁, x₂, ..., xₙ | θ)</p>
<p>由于观测数据是独立同分布的，因此：</p>
<p>L(θ) = ∏ P(xᵢ | θ)  (∏ 表示连乘)</p>
<ul>
<li>对于离散型随机变量：P(xᵢ | θ) 是概率分布列。</li>
<li>对于连续型随机变量：P(xᵢ | θ) 是概率密度函数。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>取对数：</strong></p>
<ul>
<li><p>通常对似然函数取对数，得到对数似然函数 (Log-likelihood Function)：</p>
<p>l(θ) = log L(θ) = ∑ log P(xᵢ | θ)</p>
</li>
<li><p>取对数的好处：</p>
<ul>
<li>将连乘变为连加，简化计算。</li>
<li>对数函数是单调递增函数，不改变最大值点。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>求导 (或偏导)：</strong></p>
<ul>
<li><p>对于连续型参数，通常对对数似然函数求导 (或偏导)，并令导数 (或偏导数) 等于 0。</p>
<p>∂l(θ)/∂θ = 0</p>
</li>
</ul>
</li>
<li><p><strong>解方程 (或方程组)：</strong></p>
<ul>
<li>解上述方程 (或方程组)，得到参数 θ 的估计值，记作 θ̂。</li>
<li>θ̂ 就是参数 θ 的最大似然估计。</li>
</ul>
</li>
</ol>
<p><strong>5.3 举例</strong></p>
<p><strong>示例 1：伯努利分布</strong></p>
<p>假设我们有一组独立同分布的伯努利试验结果 x₁, x₂, ..., xₙ，其中 xᵢ = 1 表示成功，xᵢ = 0 表示失败。我们要估计成功的概率 p。</p>
<ol>
<li><p><strong>似然函数：</strong><br>L(p) = ∏ pˣⁱ(1 - p)¹⁻ˣⁱ</p>
</li>
<li><p><strong>对数似然函数：</strong><br>l(p) = ∑ [xᵢ log(p) + (1 - xᵢ) log(1 - p)]</p>
</li>
<li><p><strong>求导：</strong><br>dl(p)/dp = ∑ [xᵢ/p - (1 - xᵢ)/(1 - p)]</p>
</li>
<li><p><strong>解方程：</strong><br>令 dl(p)/dp = 0，解得：<br>p̂ = (∑ xᵢ) / n  (即样本中成功的比例)</p>
</li>
</ol>
<p><strong>示例 2：正态分布</strong></p>
<p>假设我们有一组独立同分布的观测数据 x₁, x₂, ..., xₙ，它们来自一个正态分布 N(μ, σ²)。我们要估计均值 μ 和方差 σ²。</p>
<ol>
<li><p><strong>似然函数</strong><br>L(μ, σ²) = ∏ (1 / (√(2π)σ)) * e^(-(xᵢ-μ)² / (2σ²))</p>
</li>
<li><p><strong>对数似然函数</strong><br>l(μ, σ²) = -n/2 * log(2π) - n/2 * log(σ²) - 1/(2σ²) * ∑(xᵢ - μ)²</p>
</li>
<li><p><strong>求导：</strong><br> ∂l/∂μ = 1/σ² * ∑(xᵢ - μ)<br> ∂l/∂σ² = -n/(2σ²) + 1/(2σ⁴) * ∑(xᵢ - μ)²</p>
</li>
<li><p><strong>解方程：</strong><br>令偏导数为 0，解得：<br>μ̂ = (∑ xᵢ) / n  (样本均值)<br>σ²̂ = (∑ (xᵢ - μ̂)²) / n  (样本方差)</p>
</li>
</ol>
<p>最大似然估计是一种常用的参数估计方法。它的基本思想是找到一组参数，使得在这组参数下，观测数据出现的概率最大。最大似然估计的步骤包括：写出似然函数、取对数、求导 (或偏导) 和解方程 (或方程组)。</p>
<p><strong>范例 1：伯努利分布 (硬币投掷)</strong></p>
<p>假设我们投掷一枚硬币 n 次，出现了 k 次正面。我们想用最大似然估计来估计硬币正面向上的概率 p。</p>
<ol>
<li><p><strong>模型：</strong> 伯努利分布 (Bernoulli Distribution)</p>
<ul>
<li>P(X = 1) = p (正面)</li>
<li>P(X = 0) = 1 - p (反面)</li>
</ul>
</li>
<li><p><strong>数据：</strong> n 次投掷，k 次正面，n-k 次反面。</p>
</li>
<li><p><strong>似然函数：</strong><br>L(p) = pᵏ(1 - p)ⁿ⁻ᵏ</p>
</li>
<li><p><strong>对数似然函数：</strong><br>l(p) = k log(p) + (n - k) log(1 - p)</p>
</li>
<li><p><strong>求导：</strong><br>dl(p)/dp = k/p - (n - k)/(1 - p)</p>
</li>
<li><p><strong>解方程：</strong><br>令 dl(p)/dp = 0，解得：<br>p̂ = k/n (样本中正面的比例)</p>
</li>
<li><p><strong>Python 代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟投硬币</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">toss_coin</span>(<span class="params">n, p</span>):</span><br><span class="line">    <span class="keyword">return</span> np.random.binomial(<span class="number">1</span>, p, n)  <span class="comment"># 1 表示正面，0 表示反面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实概率 (用于模拟)</span></span><br><span class="line">true_p = <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 投掷次数</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">data = toss_coin(n, true_p)</span><br><span class="line">k = np.<span class="built_in">sum</span>(data)  <span class="comment"># 正面次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大似然估计</span></span><br><span class="line">estimated_p = k / n</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;真实概率: <span class="subst">&#123;true_p&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;投掷 <span class="subst">&#123;n&#125;</span> 次，正面 <span class="subst">&#123;k&#125;</span> 次&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最大似然估计: <span class="subst">&#123;estimated_p&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>范例 2：正态分布 (身高)</strong></p>
<p>假设我们有一组某地区成年男性的身高数据 (单位：cm)，我们假设身高服从正态分布 N(μ, σ²)。我们想用最大似然估计来估计均值 μ 和方差 σ²。</p>
<ol start="8">
<li><p><strong>模型：</strong> 正态分布 (Normal Distribution)</p>
<ul>
<li>概率密度函数：f(x) = (1 / (√(2π)σ)) * e^(-(x-μ)² / (2σ²))</li>
</ul>
</li>
<li><p><strong>数据：</strong> x₁, x₂, ..., xₙ</p>
</li>
<li><p><strong>似然函数：</strong><br>L(μ, σ²) = ∏ (1 / (√(2π)σ)) * e^(-(xᵢ-μ)² / (2σ²))</p>
</li>
<li><p><strong>对数似然函数：</strong><br>l(μ, σ²) = -n/2 * log(2π) - n/2 * log(σ²) - 1/(2σ²) * ∑(xᵢ - μ)²</p>
</li>
<li><p><strong>求偏导：</strong><br>∂l/∂μ = 1/σ² * ∑(xᵢ - μ)<br>∂l/∂σ² = -n/(2σ²) + 1/(2σ⁴) * ∑(xᵢ - μ)²</p>
</li>
<li><p><strong>解方程组：</strong><br>令偏导数为 0，解得：<br>μ̂ = (∑ xᵢ) / n (样本均值)<br>σ²̂ = (∑ (xᵢ - μ̂)²) / n (样本方差)</p>
</li>
<li><p><strong>Python 代码：</strong></p>
</li>
</ol>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟生成数据 (正态分布)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">n, mu, sigma</span>):</span><br><span class="line">    <span class="keyword">return</span> np.random.normal(mu, sigma, n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实参数</span></span><br><span class="line">true_mu = <span class="number">170</span></span><br><span class="line">true_sigma = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本数量</span></span><br><span class="line">n = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">data = generate_data(n, true_mu, true_sigma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大似然估计</span></span><br><span class="line">estimated_mu = np.mean(data)</span><br><span class="line">estimated_sigma2 = np.var(data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;真实均值: <span class="subst">&#123;true_mu&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;真实标准差: <span class="subst">&#123;true_sigma&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;样本数量: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;估计均值: <span class="subst">&#123;estimated_mu&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;估计方差: <span class="subst">&#123;estimated_sigma2&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;估计标准差: <span class="subst">&#123;np.sqrt(estimated_sigma2)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
<p>通过这些范例，你可以看到如何针对不同的概率模型，使用最大似然估计来估计参数。最大似然估计是一种非常常用且有效的参数估计方法。</p>
<hr>
<h3 id="6-信息熵与交叉熵"><a href="#6-信息熵与交叉熵" class="headerlink" title="6. 信息熵与交叉熵"></a>6. 信息熵与交叉熵</h3><p><strong>6.1 信息熵 (Entropy)</strong></p>
<ul>
<li><p><strong>定义：</strong> 信息熵 (Entropy) 是用来衡量随机变量不确定性的度量。</p>
<ul>
<li>熵越高，随机变量的不确定性越大。</li>
<li>熵越低，随机变量的不确定性越小。</li>
</ul>
</li>
<li><p><strong>公式：</strong></p>
<ul>
<li><p>对于离散型随机变量 X，其概率分布列为 P(X = xᵢ) = pᵢ，则 X 的信息熵定义为：</p>
<p>H(X) = -∑ pᵢ log₂(pᵢ)  (通常以 2 为底，单位为比特 bits)<br>  = -∑ pᵢ log(pᵢ)   (也可以用e为底，单位为奈特 nats)<br>(规定：如果 pᵢ = 0，则 pᵢ log₂(pᵢ) = 0)</p>
</li>
<li><p>对于连续型随机变量 X，其概率密度函数为 f(x)，则 X 的信息熵定义为：</p>
<p>H(X) = -∫ f(x) log₂(f(x)) dx</p>
</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>信息熵可以看作是“平均信息量”。</li>
<li>如果一个事件发生的概率很小，那么它发生时带来的信息量就很大。</li>
<li>如果一个事件发生的概率很大，那么它发生时带来的信息量就很小。</li>
<li>信息熵就是所有可能事件的信息量的期望。</li>
</ul>
</li>
<li><p><strong>性质：</strong></p>
<ul>
<li>H(X) ≥ 0</li>
<li>当 X 服从均匀分布时，H(X) 最大。</li>
</ul>
</li>
</ul>
<p><strong>举例：</strong></p>
<ol>
<li><p><strong>抛硬币 (均匀)：</strong></p>
<ul>
<li>X = {正面, 反面}</li>
<li>P(正面) = 1/2, P(反面) = 1/2</li>
<li>H(X) = - (1/2)log₂(1/2) - (1/2)log₂(1/2) = 1 比特</li>
</ul>
</li>
<li><p><strong>抛硬币 (不均匀)：</strong></p>
<ul>
<li>X = {正面, 反面}</li>
<li>P(正面) = 0.8, P(反面) = 0.2</li>
<li>H(X) = - 0.8log₂(0.8) - 0.2log₂(0.2) ≈ 0.72 比特</li>
</ul>
</li>
</ol>
<p><strong>6.2 交叉熵 (Cross Entropy)</strong></p>
<ul>
<li><p><strong>定义：</strong> 交叉熵 (Cross Entropy) 是用来衡量两个概率分布之间的差异的度量。</p>
<ul>
<li>假设我们有一个真实的概率分布 p(x) 和一个模型预测的概率分布 q(x)。</li>
<li>交叉熵衡量的是用 q(x) 来编码 p(x) 的平均编码长度。</li>
</ul>
</li>
<li><p><strong>公式：</strong></p>
<ul>
<li><p>对于离散型随机变量：</p>
<p>H(p, q) = -∑ p(x) log₂(q(x))</p>
</li>
<li><p>对于连续型随机变量：</p>
<p>H(p, q) = -∫ p(x) log₂(q(x)) dx</p>
</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>交叉熵越小，表示两个概率分布越接近。</li>
<li>交叉熵越大，表示两个概率分布差异越大。</li>
<li>交叉熵 H(p, q) ≥ H(p) (吉布斯不等式)，当且仅当 p = q 时等号成立。</li>
</ul>
</li>
</ul>
<p><strong>6.3 相对熵 (Kullback-Leibler Divergence, KL Divergence)</strong></p>
<ul>
<li><p><strong>定义：</strong> 相对熵 (Relative Entropy) 或 KL 散度 (Kullback-Leibler Divergence) 也是用来衡量两个概率分布之间的差异的度量。</p>
</li>
<li><p><strong>公式：</strong></p>
<ul>
<li><p>对于离散型随机变量：</p>
<p>D KL(p || q) = ∑ p(x) log₂(p(x) / q(x)) = H(p,q) - H(p)</p>
</li>
<li><p>对于连续型随机变量：</p>
<p>D KL(p || q) = ∫ p(x) log₂(p(x) / q(x)) dx = H(p,q) - H(p)</p>
</li>
</ul>
</li>
<li><p><strong>理解：</strong></p>
<ul>
<li>KL 散度衡量的是用 q(x) 来近似 p(x) 时的信息损失。</li>
<li>KL 散度是非对称的：D KL(p || q) ≠ D KL(q || p)</li>
<li>KL 散度 ≥ 0，当且仅当 p = q 时等号成立。</li>
</ul>
</li>
</ul>
<p><strong>6.4 在深度学习中的应用</strong></p>
<ul>
<li><p><strong>交叉熵作为损失函数：</strong> 在分类问题中，我们通常使用交叉熵作为损失函数。</p>
<ul>
<li><p>例如，在二分类问题中，真实标签 y ∈ {0, 1}，模型预测的概率为 ŷ，则交叉熵损失为：</p>
<p>Loss = -[y log(ŷ) + (1 - y) log(1 - ŷ)]</p>
</li>
<li><p>在多分类问题中, 假设有 K 个类别, 真实标签 y 是一个 one-hot 向量 (只有一个元素为 1，其余为 0)，模型预测的概率向量为 ŷ (每个元素表示属于该类别的概率, 所有元素之和为1)<br>Loss = - ∑yᵢlog(ŷᵢ)</p>
</li>
</ul>
</li>
<li><p><strong>KL 散度用于衡量分布差异：</strong> 在生成模型 (如变分自编码器 VAE、生成对抗网络 GAN) 中，KL 散度可以用来衡量生成的分布与真实分布之间的差异。</p>
</li>
</ul>
<p>信息熵衡量随机变量的不确定性。交叉熵和 KL 散度衡量两个概率分布之间的差异。在深度学习中，交叉熵常被用作分类问题的损失函数，KL 散度常被用于衡量分布差异。</p>
<p><strong>范例 1：计算信息熵</strong></p>
<p>假设有以下几个离散型随机变量的概率分布：</p>
<ol>
<li><strong>均匀分布：</strong> X = {A, B, C, D}, P(A) = P(B) = P(C) = P(D) = 1/4</li>
<li><strong>非均匀分布 1：</strong> Y = {A, B, C, D}, P(A) = 1/2, P(B) = 1/4, P(C) = 1/8, P(D) = 1/8</li>
<li><strong>非均匀分布 2：</strong> Z = {A, B, C, D}, P(A) = 0.9, P(B) = 0.05, P(C) = 0.025, P(D) = 0.025</li>
</ol>
<p>计算这三个随机变量的信息熵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义熵计算函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">probabilities</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;计算信息熵.&quot;&quot;&quot;</span></span><br><span class="line">  entropy = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> probabilities:</span><br><span class="line">    <span class="keyword">if</span> p &gt; <span class="number">0</span>:  <span class="comment"># 避免 log(0) 错误</span></span><br><span class="line">      entropy -= p * np.log2(p)</span><br><span class="line">  <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均匀分布</span></span><br><span class="line">prob_X = [<span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">4</span>]</span><br><span class="line">entropy_X = entropy(prob_X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;X (均匀分布) 的信息熵: <span class="subst">&#123;entropy_X&#125;</span>&quot;</span>)  <span class="comment"># 输出：2.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 非均匀分布 1</span></span><br><span class="line">prob_Y = [<span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>]</span><br><span class="line">entropy_Y = entropy(prob_Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Y (非均匀分布 1) 的信息熵: <span class="subst">&#123;entropy_Y&#125;</span>&quot;</span>)  <span class="comment"># 输出：1.75</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 非均匀分布 2</span></span><br><span class="line">prob_Z = [<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.025</span>, <span class="number">0.025</span>]</span><br><span class="line">entropy_Z = entropy(prob_Z)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Z (非均匀分布 2) 的信息熵: <span class="subst">&#123;entropy_Z&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.622</span></span><br></pre></td></tr></table></figure>

<p>可以看到，均匀分布的熵最大，非均匀分布的熵较小，且分布越不均匀，熵越小。</p>
<p><strong>范例 2：计算交叉熵</strong></p>
<p>假设有两个概率分布：</p>
<ul>
<li><strong>真实分布 p：</strong> p(A) = 0.5, p(B) = 0.3, p(C) = 0.2</li>
<li><strong>预测分布 q1：</strong> q1(A) = 0.6, q1(B) = 0.2, q1(C) = 0.2</li>
<li><strong>预测分布 q2：</strong> q2(A) = 0.8, q2(B) = 0.1, q2(C) = 0.1</li>
</ul>
<p>计算 p 与 q1、p 与 q2 的交叉熵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵计算函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">p, q</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算交叉熵.&quot;&quot;&quot;</span></span><br><span class="line">    cross_entropy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(p)):</span><br><span class="line">        <span class="keyword">if</span> p[i] &gt; <span class="number">0</span>:</span><br><span class="line">          cross_entropy -= p[i] * np.log2(q[i])</span><br><span class="line">    <span class="keyword">return</span> cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实分布</span></span><br><span class="line">p = [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测分布 q1</span></span><br><span class="line">q1 = [<span class="number">0.6</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测分布 q2</span></span><br><span class="line">q2 = [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交叉熵</span></span><br><span class="line">H_p_q1 = cross_entropy(p, q1)</span><br><span class="line">H_p_q2 = cross_entropy(p, q2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;H(p, q1) = <span class="subst">&#123;H_p_q1&#125;</span>&quot;</span>)  <span class="comment"># 输出：1.826</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;H(p, q2) = <span class="subst">&#123;H_p_q2&#125;</span>&quot;</span>)  <span class="comment"># 输出：2.039</span></span><br></pre></td></tr></table></figure>
<p>计算结果表明，q1 比 q2 更接近真实分布 p。</p>
<p><strong>范例 3：交叉熵损失 (二分类)</strong></p>
<p>假设有一个二分类问题，真实标签为 y = 1，模型预测的概率为 ŷ。我们分别计算当 ŷ = 0.9, ŷ = 0.6, ŷ = 0.1 时，交叉熵损失是多少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失函数 (二分类)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;计算二分类交叉熵损失.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> -(y_true * np.log(y_pred) + (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同的预测概率</span></span><br><span class="line">y_pred_1 = <span class="number">0.9</span></span><br><span class="line">y_pred_2 = <span class="number">0.6</span></span><br><span class="line">y_pred_3 = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交叉熵损失</span></span><br><span class="line">loss_1 = cross_entropy_loss(y_true, y_pred_1)</span><br><span class="line">loss_2 = cross_entropy_loss(y_true, y_pred_2)</span><br><span class="line">loss_3 = cross_entropy_loss(y_true, y_pred_3)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;当 ŷ = 0.9 时，交叉熵损失: <span class="subst">&#123;loss_1&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.105</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;当 ŷ = 0.6 时，交叉熵损失: <span class="subst">&#123;loss_2&#125;</span>&quot;</span>)  <span class="comment"># 输出：0.510</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;当 ŷ = 0.1 时，交叉熵损失: <span class="subst">&#123;loss_3&#125;</span>&quot;</span>)  <span class="comment"># 输出：2.302</span></span><br></pre></td></tr></table></figure>
<p>结果展示出，预测概率越接近真实标签，交叉熵损失越小。</p>
<p>通过这些范例，你可以看到信息熵、交叉熵的计算方法，以及它们在不同场景下的应用。这些概念在深度学习中非常重要，尤其是在分类问题和生成模型中。</p>
<hr>
</div><div class="article-licensing box"><div class="licensing-title"><p>深度学习数学基础入门</p><p><a href="http://acorner.ac.cn/2025/02/20/深度学习数学基础入门/">http://acorner.ac.cn/2025/02/20/深度学习数学基础入门/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ViniJack.SJX</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-02-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-02-20</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/LLM/">LLM</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%8E%9F%E7%90%86/">原理</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%A6%82%E7%8E%87/">概率</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%85%A5%E9%97%A8/">入门</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/02/18/llm_gradient_descent/"><span class="level-item">大语言模型中的梯度值：深入理解与应用</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="A-Corner"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">A-Corner</p><p class="is-size-6 is-block">信息的一角</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>China（Guangzhou）</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">13</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">16</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Model/"><span class="level-start"><span class="level-item">Model</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/RAG/"><span class="level-start"><span class="level-item">RAG</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/"><span class="level-start"><span class="level-item">分析报告</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">原理</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">向量数据库</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%B7%A5%E5%85%B7/"><span class="level-start"><span class="level-item">工具</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BE%AE%E8%B0%83/"><span class="level-start"><span class="level-item">微调</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A1%86%E6%9E%B6/"><span class="level-start"><span class="level-item">框架</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="level-start"><span class="level-item">食用文档</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-20T06:44:10.490Z">2025-02-20</time></p><p class="title"><a href="/2025/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/">深度学习数学基础入门</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-18T15:22:21.629Z">2025-02-18</time></p><p class="title"><a href="/2025/02/18/llm_gradient_descent/">大语言模型中的梯度值：深入理解与应用</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-18T09:06:39.943Z">2025-02-18</time></p><p class="title"><a href="/2025/02/18/Embed%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/">Embedding 模型入门级研究报告</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/Model/">Model</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-14T16:03:29.233Z">2025-02-15</time></p><p class="title"><a href="/2025/02/15/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E3%80%81%E8%87%AA%E5%8A%A8%E5%8C%96%E7%88%AC%E8%99%AB%E3%80%81AI%E7%88%AC%E8%99%AB%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">爬虫框架、自动化爬虫、AI爬虫分析报告</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-14T14:55:50.802Z">2025-02-14</time></p><p class="title"><a href="/2025/02/14/AI%E8%87%AA%E5%8A%A8%E5%8C%96%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AF%B9%E6%AF%94%E6%8A%A5%E5%91%8A/">AI自动化爬虫项目对比报告</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model/"><span class="tag">Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RAG/"><span class="tag">RAG</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%A5%E9%97%A8/"><span class="tag">入门</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8E%9F%E7%90%86/"><span class="tag">原理</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">向量数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B7%A5%E5%85%B7/"><span class="tag">工具</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%B0%83/"><span class="tag">微调</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A1%86%E6%9E%B6/"><span class="tag">框架</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87/"><span class="tag">概率</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"><span class="tag">自动化</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="tag">食用文档</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a><p class="is-size-7"><span>&copy; 2025 ViniJack.SJX</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2025 A-Corner</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>