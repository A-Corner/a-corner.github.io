<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>大语言模型中的梯度值：深入理解与应用 - A-Acorner 信息的一角</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="A-Acorner 信息的一角"><meta name="msapplication-TileImage" content="/img/一角favicon.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="A-Acorner 信息的一角"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="大语言模型中的梯度值：深入理解与应用"><meta property="og:type" content="blog"><meta property="og:title" content="大语言模型中的梯度值：深入理解与应用"><meta property="og:url" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/"><meta property="og:site_name" content="A-Acorner 信息的一角"><meta property="og:description" content="大语言模型中的梯度值：深入理解与应用"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219095653.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219101416.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219102019.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219113421.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219113742.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104514.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104823.png"><meta property="og:image" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104930.png"><meta property="article:published_time" content="2025-02-18T15:22:21.629Z"><meta property="article:modified_time" content="2025-02-19T13:55:42.680Z"><meta property="article:author" content="ViniJack.SJX"><meta property="article:tag" content="原理"><meta property="article:tag" content="LLM"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219095653.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://acorner.ac.cn/2025/02/18/llm_gradient_descent/"},"headline":"大语言模型中的梯度值：深入理解与应用","image":["http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219095653.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219101416.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219102019.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219113421.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219113742.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104514.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104823.png","http://acorner.ac.cn/2025/02/18/llm_gradient_descent/20250219104930.png"],"datePublished":"2025-02-18T15:22:21.629Z","dateModified":"2025-02-19T13:55:42.680Z","author":{"@type":"Person","name":"ViniJack.SJX"},"publisher":{"@type":"Organization","name":"A-Acorner 信息的一角","logo":{"@type":"ImageObject","url":"http://acorner.ac.cn/img/一角logo.png"}},"description":"大语言模型中的梯度值：深入理解与应用"}</script><link rel="canonical" href="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/"><link rel="icon" href="/img/%E4%B8%80%E8%A7%92favicon.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link data-pjax rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-02-18T15:22:21.629Z" title="18/2/2025 下午11:22:21">2025-02-18</time>发表</span><span class="level-item"><time dateTime="2025-02-19T13:55:42.680Z" title="19/2/2025 下午9:55:42">2025-02-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/LLM/">LLM</a><span> / </span><a class="link-muted" href="/categories/%E5%8E%9F%E7%90%86/">原理</a></span><span class="level-item">38 分钟读完 (大约5722个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">大语言模型中的梯度值：深入理解与应用</h1><div class="content"><h3 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h3><p>​        梯度是微积分中的一个基本概念，在机器学习和深度学习中扮演着至关重要的角色。特别是在大语言模型（LLM）的训练过程中，梯度指导着模型参数的优化方向。<span id="more"></span>本报告首先由浅入深地介绍梯度的概念，包括其数学定义、几何意义以及在优化算法中的应用。然后，报告将重点探讨梯度在大语言模型中的作用，并深入研究梯度消失和梯度爆炸这两个常见问题。针对这两个问题，报告将分析其产生原因、对模型训练的影响，并详细介绍一系列有效的解决方法，如梯度裁剪、权重正则化、不同激活函数的选择、Batch Normalization、残差连接等。此外，报告还将通过案例分析，展示不同大语言模型（如BERT、GPT）如何处理这些问题。最后，报告将对比分析梯度在不同应用场景（文本生成、机器翻译、代码生成）下的表现，展望未来的发展趋势与挑战，并总结网络舆情与用户关注点。</p>
<h3 id="2-引言：什么是梯度？"><a href="#2-引言：什么是梯度？" class="headerlink" title="2. 引言：什么是梯度？"></a>2. 引言：什么是梯度？</h3><h4 id="2-1-从函数的斜率说起"><a href="#2-1-从函数的斜率说起" class="headerlink" title="2.1 从函数的斜率说起"></a>2.1 从函数的斜率说起</h4><p>在最简单的形式中，一元函数 <em>f</em>(<em>x</em>) 在某一点的导数，就是该函数曲线在该点处切线的斜率。斜率越大，函数在该点上升或下降得越快。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219095653.png" alt="函数曲线及切线"></p>
<p><em>图2-1 函数曲线及切线：展示了一元函数 y &#x3D; x² 曲线及其在 x &#x3D; 1 处的切线。切线的斜率（即导数）为2。</em> (使用Matplotlib绘制)</p>
<h4 id="2-2-偏导数与梯度"><a href="#2-2-偏导数与梯度" class="headerlink" title="2.2 偏导数与梯度"></a>2.2 偏导数与梯度</h4><p>​        对于多变量函数，例如 <em>f</em>(<em>x</em>, <em>y</em>)，我们需要引入偏导数的概念。偏导数是函数关于其中一个变量的变化率，同时保持其他变量不变。例如，∂<em>f</em>&#x2F;∂<em>x</em> 表示函数 <em>f</em> 关于变量 <em>x</em> 的偏导数。梯度是一个向量，其各个分量分别对应于函数关于各个变量的偏导数。对于二元函数，梯度表示为：∇<em>f</em> &#x3D; [∂<em>f</em>&#x2F;∂<em>x</em>, ∂<em>f</em>&#x2F;∂<em>y</em>]</p>
<h4 id="2-3-梯度的方向与大小"><a href="#2-3-梯度的方向与大小" class="headerlink" title="2.3 梯度的方向与大小"></a>2.3 梯度的方向与大小</h4><p>梯度的方向指向函数值增长最快的方向，梯度的大小表示函数值增长的速率。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219101416.png" alt="梯度方向示意图"></p>
<p><em>图2-3 梯度方向示意图：等高线图，展示了二维函数 f(x, y) 的梯度。红色箭头表示梯度向量，指向函数值增加最快的方向。等高线越密集，表示梯度越大，函数值变化越快。</em> (使用Matplotlib绘制)</p>
<h4 id="2-4-梯度下降法"><a href="#2-4-梯度下降法" class="headerlink" title="2.4 梯度下降法"></a>2.4 梯度下降法</h4><p>梯度下降法是一种常用的优化算法，其核心思想是沿着梯度的反方向迭代更新变量的值，从而逐步逼近函数的最小值。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219102019.png" alt="梯度下降法示意图"></p>
<p><em>图2-4 梯度下降法示意图：展示了梯度下降法如何沿着梯度的反方向逐步找到函数的最小值。蓝色曲线表示函数的等高线，红色箭头表示每一步的梯度方向，绿色点表示迭代的路径。</em> (使用Matplotlib绘制)</p>
<p>更新公式为：</p>
<p><em>x</em><sub><em>t</em>+1</sub> &#x3D; <em>x</em><sub><em>t</em></sub> - η∇<em>f</em>(<em>x</em><sub><em>t</em></sub>)</p>
<p>其中，<em>x</em><sub><em>t</em></sub> 是当前变量值，η 是学习率（一个正数，控制每次更新的步长）。</p>
<h3 id="3-大语言模型中的梯度"><a href="#3-大语言模型中的梯度" class="headerlink" title="3. 大语言模型中的梯度"></a>3. 大语言模型中的梯度</h3><h4 id="3-1-神经网络与反向传播"><a href="#3-1-神经网络与反向传播" class="headerlink" title="3.1 神经网络与反向传播"></a>3.1 神经网络与反向传播</h4><p>大语言模型通常基于深度神经网络（DNN），特别是Transformer架构。神经网络由多个层组成，每层包含多个神经元。每个神经元接收来自前一层神经元的输入，进行加权求和，并通过激活函数产生输出。</p>
<p>反向传播算法是训练神经网络的关键。它通过链式法则计算损失函数关于每个参数（权重和偏置）的梯度，然后利用梯度下降法（或其变体，如Adam）更新参数。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219113421.png" alt="神经网络结构图"></p>
<p><em>图3-1 神经网络结构图：展示了一个具有两个隐藏层的全连接神经网络。每个圆圈代表一个神经元，箭头代表连接（权重）。输入层接收输入数据，隐藏层进行特征提取，输出层产生预测结果。</em> </p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219113742.png" alt="反向传播示意图"></p>
<p><em>图3-2 反向传播示意图：展示了反向传播算法如何计算梯度。误差信号从输出层反向传播到输入层，根据链式法则计算每个权重和偏置的梯度。</em><br>图片来源：url:<a target="_blank" rel="noopener" href="https://serokell.io/blog/understanding-backpropagation">https://serokell.io/blog/understanding-backpropagation</a> (Backpropagation in Neural Networks)</p>
<h4 id="3-2-损失函数与梯度计算"><a href="#3-2-损失函数与梯度计算" class="headerlink" title="3.2 损失函数与梯度计算"></a>3.2 损失函数与梯度计算</h4><p>大语言模型通常使用交叉熵损失函数来衡量模型预测与真实标签之间的差异。对于多分类问题，设模型的预测概率分布为 <em>p</em>，真实标签的 one-hot 向量为 <em>y</em>，则交叉熵损失函数为：</p>
<p>L &#x3D; - Σ <em>y<sub>i</sub></em> log(<em>p<sub>i</sub></em>)</p>
<p>其中，<em>i</em> 表示类别索引。通过对损失函数 <em>L</em> 关于模型的权重和偏置求偏导，可以得到对应的梯度。</p>
<h4 id="3-3-梯度在模型训练中的作用"><a href="#3-3-梯度在模型训练中的作用" class="headerlink" title="3.3 梯度在模型训练中的作用"></a>3.3 梯度在模型训练中的作用</h4><p>梯度提供了模型参数优化的方向。通过不断地沿着梯度的反方向调整参数，模型可以逐步减小损失函数的值，从而提高预测的准确性。梯度的质量（大小和方向）直接影响模型的训练速度和最终性能。</p>
<h3 id="4-深入研究方向：梯度消失与梯度爆炸"><a href="#4-深入研究方向：梯度消失与梯度爆炸" class="headerlink" title="4. 深入研究方向：梯度消失与梯度爆炸"></a>4. 深入研究方向：梯度消失与梯度爆炸</h3><h4 id="4-1-什么是梯度消失与梯度爆炸？"><a href="#4-1-什么是梯度消失与梯度爆炸？" class="headerlink" title="4.1 什么是梯度消失与梯度爆炸？"></a>4.1 什么是梯度消失与梯度爆炸？</h4><p>梯度消失和梯度爆炸是深度神经网络训练中常见的问题，尤其是在大语言模型中，由于其网络层数非常深，这两个问题更容易出现。</p>
<ul>
<li><strong>梯度消失：</strong> 指在反向传播过程中，梯度值变得非常小，接近于零，导致参数更新缓慢甚至停滞。这通常发生在网络的较早层（靠近输入层）。</li>
<li><strong>梯度爆炸：</strong> 指梯度值变得非常大，导致参数更新过大，模型不稳定，甚至发散。这可能导致损失函数变为 NaN（Not a Number）。</li>
</ul>
<p><img src="/2025/02/18/llm_gradient_descent/20250219104514.png" alt="梯度消失与梯度爆炸"></p>
<p><em>图4-1 梯度消失与梯度爆炸：展示了梯度消失和梯度爆炸现象。左图显示了梯度随着反向传播层数的增加而指数级衰减（梯度消失），右图显示了梯度指数级增长（梯度爆炸）。</em> (使用Matplotlib绘制)</p>
<h4 id="4-2-为什么会发生梯度消失-爆炸？"><a href="#4-2-为什么会发生梯度消失-爆炸？" class="headerlink" title="4.2 为什么会发生梯度消失&#x2F;爆炸？"></a>4.2 为什么会发生梯度消失&#x2F;爆炸？</h4><h5 id="4-2-1-激活函数的影响"><a href="#4-2-1-激活函数的影响" class="headerlink" title="4.2.1 激活函数的影响"></a>4.2.1 激活函数的影响</h5><p>某些激活函数（如Sigmoid和Tanh）在其输入值较大或较小时，梯度会趋近于零，导致梯度消失。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219104823.png" alt="激活函数图像"></p>
<p><em>图4-2 激活函数图像：展示了Sigmoid和Tanh激活函数的图像及其导数。可以看出，当输入值较大或较小时，Sigmoid和Tanh函数的导数接近于零，导致梯度消失。</em> (使用Matplotlib绘制)</p>
<p>ReLU（Rectified Linear Unit）激活函数在正区间内的梯度为1，可以有效避免梯度消失。Leaky ReLU和ELU是对ReLU的改进。</p>
<p><img src="/2025/02/18/llm_gradient_descent/20250219104930.png" alt="ReLU, Leaky ReLU, and ELU的函数图像以及他们的导数"></p>
<p><em>图4-3: ReLU, Leaky ReLU, and ELU的函数图像以及他们的导数</em> (使用Matplotlib绘制)</p>
<h5 id="4-2-2-网络层数的影响"><a href="#4-2-2-网络层数的影响" class="headerlink" title="4.2.2 网络层数的影响"></a>4.2.2 网络层数的影响</h5><p>在深层网络中，梯度需要通过多个层进行反向传播。如果每一层的梯度都小于1，那么经过多次连乘后，梯度会迅速衰减，导致梯度消失。反之，如果每一层的梯度都大于1，梯度会迅速增大，导致梯度爆炸。</p>
<h5 id="4-2-3-权重初始化的影响"><a href="#4-2-3-权重初始化的影响" class="headerlink" title="4.2.3 权重初始化的影响"></a>4.2.3 权重初始化的影响</h5><p>如果权重初始化值过大，可能会导致梯度爆炸。如果权重初始化值过小（例如，全部初始化为0），可能会导致梯度消失。合理的权重初始化方法（如Xavier初始化、He初始化）可以缓解这个问题。</p>
<h4 id="4-3-梯度消失-爆炸对大语言模型的影响"><a href="#4-3-梯度消失-爆炸对大语言模型的影响" class="headerlink" title="4.3 梯度消失&#x2F;爆炸对大语言模型的影响"></a>4.3 梯度消失&#x2F;爆炸对大语言模型的影响</h4><p>梯度消失和爆炸会严重影响大语言模型的训练：</p>
<ul>
<li><strong>梯度消失：</strong> 导致模型无法学习长距离依赖关系，影响模型的性能。例如，在文本生成中，模型可能无法生成连贯的长文本。</li>
<li><strong>梯度爆炸：</strong> 导致模型训练不稳定，难以收敛，甚至出现NaN错误。这会使得训练过程无法进行。</li>
</ul>
<h4 id="4-4-应对梯度消失-爆炸的方法"><a href="#4-4-应对梯度消失-爆炸的方法" class="headerlink" title="4.4 应对梯度消失&#x2F;爆炸的方法"></a>4.4 应对梯度消失&#x2F;爆炸的方法</h4><h5 id="4-4-1-梯度裁剪（Gradient-Clipping）"><a href="#4-4-1-梯度裁剪（Gradient-Clipping）" class="headerlink" title="4.4.1 梯度裁剪（Gradient Clipping）"></a>4.4.1 梯度裁剪（Gradient Clipping）</h5><p>梯度裁剪是一种简单有效的方法，它通过设置一个阈值来限制梯度的大小。当梯度的范数（L2范数）超过阈值时，将其缩放到阈值范围内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码</span></span><br><span class="line"><span class="keyword">if</span> norm(gradient) &gt; threshold:</span><br><span class="line">    gradient = gradient * (threshold / norm(gradient))</span><br></pre></td></tr></table></figure>

<h5 id="4-4-2-权重正则化（Weight-Regularization）"><a href="#4-4-2-权重正则化（Weight-Regularization）" class="headerlink" title="4.4.2 权重正则化（Weight Regularization）"></a>4.4.2 权重正则化（Weight Regularization）</h5><p>权重正则化通过在损失函数中添加一个惩罚项来限制权重的大小，从而防止梯度爆炸。常用的正则化方法包括L1正则化和L2正则化。</p>
<ul>
<li><strong>L1正则化：</strong> 惩罚项是权重的绝对值之和。</li>
<li><strong>L2正则化：</strong> 惩罚项是权重的平方和。</li>
</ul>
<h5 id="4-4-3-使用不同的激活函数"><a href="#4-4-3-使用不同的激活函数" class="headerlink" title="4.4.3 使用不同的激活函数"></a>4.4.3 使用不同的激活函数</h5><p>如前所述，ReLU、Leaky ReLU、ELU、GELU等激活函数可以在一定程度上缓解梯度消失问题。</p>
<h5 id="4-4-4-Batch-Normalization"><a href="#4-4-4-Batch-Normalization" class="headerlink" title="4.4.4 Batch Normalization"></a>4.4.4 Batch Normalization</h5><p>Batch Normalization通过对每一层的输入进行归一化，使其均值为0、方差为1，可以加速训练过程，并缓解梯度消失&#x2F;爆炸问题。它还有助于减少内部协变量偏移（Internal Covariate Shift）。</p>
<h5 id="4-4-5-残差连接（Residual-Connections）"><a href="#4-4-5-残差连接（Residual-Connections）" class="headerlink" title="4.4.5 残差连接（Residual Connections）"></a>4.4.5 残差连接（Residual Connections）</h5><p>残差连接通过在网络层之间添加“捷径”来允许梯度直接传播，从而避免梯度消失。ResNet和Transformer等现代网络架构都广泛使用了残差连接。</p>
<h5 id="4-4-6-LSTM和GRU"><a href="#4-4-6-LSTM和GRU" class="headerlink" title="4.4.6 LSTM和GRU"></a>4.4.6 LSTM和GRU</h5><p>对于循环神经网络（RNN），长短期记忆网络（LSTM）和门控循环单元（GRU）通过引入门控机制来控制信息的流动，可以有效缓解梯度消失问题。</p>
<h4 id="4-5-案例分析：不同大语言模型（如BERT、GPT）如何处理梯度消失-爆炸问题"><a href="#4-5-案例分析：不同大语言模型（如BERT、GPT）如何处理梯度消失-爆炸问题" class="headerlink" title="4.5 案例分析：不同大语言模型（如BERT、GPT）如何处理梯度消失&#x2F;爆炸问题"></a>4.5 案例分析：不同大语言模型（如BERT、GPT）如何处理梯度消失&#x2F;爆炸问题</h4><ul>
<li><p><strong>BERT:</strong></p>
<ul>
<li><strong>GELU激活函数:</strong> BERT使用Gaussian Error Linear Unit (GELU)激活函数。GELU在负数区域也有轻微的梯度，有助于缓解梯度消失。</li>
<li><strong>Layer Normalization:</strong> 与Batch Normalization类似，Layer Normalization对每个样本在所有特征维度上进行归一化。</li>
<li><strong>Transformer架构:</strong> BERT基于Transformer，包含残差连接，允许梯度直接跨层传播。</li>
<li><strong>学习率预热(Warm-up):</strong> BERT在训练初期使用较小的学习率，逐渐增加，防止梯度爆炸。</li>
<li><strong>实验数据:</strong><ul>
<li>原始论文中提到，使用Adam优化器，学习率为1e-4, β1 &#x3D; 0.9, β2 &#x3D; 0.999, L2权重衰减为0.01，并在训练的前10,000步进行学习率预热。</li>
<li>Dropout概率设置为0.1。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>GPT:</strong></p>
<ul>
<li><strong>大规模模型:</strong> GPT系列模型通常具有非常多的参数和层数，更容易受到梯度问题影响。</li>
<li><strong>梯度裁剪:</strong> GPT-3等大型模型明确使用了梯度裁剪，防止梯度爆炸。</li>
<li><strong>Layer Norm:</strong> 和BERT一样，GPT也使用了Layer Norm。</li>
<li><strong>Modified Initialization:</strong> GPT-2论文中提到，他们使用了 modified initialization，将残差层的权重初始化为 1&#x2F;√N, 其中N是残差层的数量。</li>
<li><strong>混合精度训练:</strong> GPT-3等模型采用混合精度训练（FP16&#x2F;FP32），加速训练并缓解梯度消失。</li>
<li><strong>实验数据:</strong><ul>
<li>GPT-3论文中提到，他们使用了Adam优化器，β1 &#x3D; 0.9, β2 &#x3D; 0.95，并使用了梯度裁剪，将梯度的L2范数限制为1.0。</li>
<li>GPT-2使用了与OpenAI GPT相似的训练设置，但对Layer Normalization的位置进行了修改，并在残差层之后添加了一个额外的Layer Normalization。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-应用场景对比"><a href="#5-应用场景对比" class="headerlink" title="5. 应用场景对比"></a>5. 应用场景对比</h3><h4 id="5-1-文本生成场景"><a href="#5-1-文本生成场景" class="headerlink" title="5.1 文本生成场景"></a>5.1 文本生成场景</h4><p>在文本生成场景中，大语言模型需要学习长距离依赖关系，因此梯度消失问题尤为突出。例如，生成一篇长篇小说时，模型需要记住前面的情节和角色设定，才能生成连贯、一致的内容。采用残差连接、LSTM&#x2F;GRU、注意力机制等技术可以有效改善模型性能。</p>
<h4 id="5-2-机器翻译场景"><a href="#5-2-机器翻译场景" class="headerlink" title="5.2 机器翻译场景"></a>5.2 机器翻译场景</h4><p>机器翻译同样需要处理长序列，梯度消失&#x2F;爆炸问题也会影响翻译质量。例如，翻译一篇长文章时，模型需要理解整个句子的语义，才能准确地翻译。梯度裁剪、Batch Normalization、注意力机制等技术可以提高翻译模型的训练稳定性和翻译准确性。</p>
<h4 id="5-3-代码生成场景"><a href="#5-3-代码生成场景" class="headerlink" title="5.3 代码生成场景"></a>5.3 代码生成场景</h4><p>代码生成对模型的精确性要求更高，梯度爆炸可能导致生成的代码无法编译或运行。权重正则化、梯度裁剪、更谨慎的权重初始化等技术可以帮助生成更稳定的代码。此外，代码生成通常需要模型理解代码的语法结构和语义，这可能需要更复杂的模型架构和训练策略。</p>
<h4 id="5-4-对比分析"><a href="#5-4-对比分析" class="headerlink" title="5.4 对比分析"></a>5.4 对比分析</h4><table>
<thead>
<tr>
<th>场景</th>
<th>梯度问题挑战</th>
<th>常用解决方法</th>
</tr>
</thead>
<tbody><tr>
<td>文本生成</td>
<td>长距离依赖关系导致梯度消失</td>
<td>残差连接、LSTM&#x2F;GRU、注意力机制、更深的Transformer</td>
</tr>
<tr>
<td>机器翻译</td>
<td>长序列处理导致梯度消失&#x2F;爆炸</td>
<td>梯度裁剪、Batch Normalization、注意力机制、Transformer</td>
</tr>
<tr>
<td>代码生成</td>
<td>对精确性要求高，梯度爆炸导致代码无法编译或运行</td>
<td>权重正则化、梯度裁剪、更谨慎的权重初始化、语法感知的模型架构</td>
</tr>
</tbody></table>
<h3 id="6-未来趋势与挑战"><a href="#6-未来趋势与挑战" class="headerlink" title="6. 未来趋势与挑战"></a>6. 未来趋势与挑战</h3><ul>
<li><strong>更深的网络：</strong> 随着模型规模的不断扩大（例如，参数量达到数千亿甚至万亿），梯度消失&#x2F;爆炸问题将更加严峻。未来的研究将需要探索更有效的方法来训练这些超大型模型。</li>
<li><strong>新的优化算法：</strong> 研究人员正在不断探索新的优化算法，以更好地处理梯度问题。例如，一些研究尝试将二阶优化方法（如牛顿法）应用于深度学习，但计算成本是一个挑战。</li>
<li><strong>硬件加速：</strong> 利用GPU、TPU等硬件加速器可以加速梯度计算，但仍需解决内存限制等问题。未来的硬件发展可能会为训练超大型模型提供更好的支持。</li>
<li><strong>模型架构创新:</strong> 不断探索新的模型架构是解决梯度问题的关键。例如，注意力机制的改进，以及新的网络结构（如Sparse Transformers）的出现，都有助于缓解梯度问题。</li>
<li><strong>AutoML和NAS：</strong> 自动机器学习（AutoML）和神经架构搜索（NAS）技术可以自动搜索更优的模型架构，可能发现新的、更易于训练的结构。</li>
</ul>
<h3 id="7-网络舆情与用户关注"><a href="#7-网络舆情与用户关注" class="headerlink" title="7. 网络舆情与用户关注"></a>7. 网络舆情与用户关注</h3><p>在网络上，关于梯度消失&#x2F;爆炸的讨论主要集中在以下几个方面：</p>
<ul>
<li><strong>技术论坛和博客（如Stack Overflow、Reddit、Medium）：</strong> 开发者们分享解决梯度消失&#x2F;爆炸问题的经验、技巧和代码示例。常见的讨论包括：<ul>
<li>如何选择合适的激活函数？</li>
<li>如何设置梯度裁剪的阈值？</li>
<li>Batch Normalization和Layer Normalization的区别和选择？</li>
<li>残差连接的具体实现方式？</li>
<li>不同优化器（如Adam、SGD）的优缺点？</li>
</ul>
</li>
<li><strong>社交媒体（如Twitter、Facebook）：</strong> 用户关注大语言模型在特定应用中的表现，讨论模型训练的难点。例如，用户可能会抱怨生成的文本不连贯、翻译质量差、生成的代码无法运行等，这些问题可能与梯度消失&#x2F;爆炸有关。</li>
<li><strong>学术论文（如arXiv）：</strong> 研究人员不断提出新的方法来解决梯度问题。新的激活函数、优化算法、模型架构等不断涌现。</li>
<li><strong>问答社区（知乎）：</strong> 有大量关于梯度消失和梯度爆炸的原理、原因和解决方法的问题和讨论。</li>
</ul>
<h3 id="8-结论与建议"><a href="#8-结论与建议" class="headerlink" title="8. 结论与建议"></a>8. 结论与建议</h3><p>梯度是大语言模型训练的核心概念。理解梯度、解决梯度消失&#x2F;爆炸问题对于提高模型性能至关重要。梯度问题不是一个孤立的问题，它与模型架构、激活函数、优化算法、初始化方法等多个因素密切相关。</p>
<p><strong>建议：</strong></p>
<ul>
<li><strong>对于研究人员：</strong> 继续探索新的优化算法、模型架构和训练技术，特别关注超大型模型（如万亿参数模型）的训练挑战。</li>
<li><strong>对于开发者：</strong> 熟悉并掌握各种应对梯度问题的方法，并根据具体应用场景选择合适的技术。在实践中，需要综合考虑模型的性能、训练速度和资源消耗。</li>
<li><strong>对于用户：</strong> 了解大语言模型的基本原理，关注模型在实际应用中的表现，并理解模型可能存在的局限性。</li>
</ul>
<h3 id="9-参考文献"><a href="#9-参考文献" class="headerlink" title="9. 参考文献"></a>9. 参考文献</h3><ol>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep learning</em>. MIT press.</li>
<li>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>nature</em>, <em>323</em>(6088), 533-536.</li>
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <em>Neural computation</em>, <em>9</em>(8), 1735-1780.</li>
<li>Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013, February). On the difficulty of training recurrent neural networks. In <em>International conference on machine learning</em> (pp. 1310-1318). PMLR.</li>
<li>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).</li>
<li>Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In <em>International conference on machine learning</em> (pp. 448-456). PMLR.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</li>
<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</li>
<li>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</li>
<li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Amodei, D. (2020). Language models are few-shot learners. In <em>Advances in neural information processing systems</em> (pp. 1877-1901).</li>
<li>Glorot, X., &amp; Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em> (pp. 249-256). JMLR Workshop and Conference Proceedings.</li>
<li>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.</li>
<li>Hendrycks, D., &amp; Gimpel, K. (2016). Gaussian error linear units (gelus). <em>arXiv preprint arXiv:1606.08415</em>.</li>
<li>Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). Layer normalization. <em>arXiv preprint arXiv:1607.06450</em>.</li>
<li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. <em>OpenAI blog</em>, <em>1</em>(8), 9.</li>
</ol>
<p><strong>免责声明</strong></p>
<p>本报告（“爬虫框架、自动化爬虫、AI爬虫分析报告”）由[ViniJack.SJX] 根据公开可获得的信息以及作者的专业知识和经验撰写，旨在提供关于网络爬虫技术、相关框架和工具的分析和信息。</p>
<p><strong>1. 信息准确性与完整性：</strong></p>
<ul>
<li><p>作者已尽最大努力确保报告中信息的准确性和完整性，但不对其绝对准确性、完整性或及时性做出任何明示或暗示的保证。</p>
</li>
<li><p>报告中的信息可能随时间推移而发生变化，作者不承担更新报告内容的义务。</p>
</li>
<li><p>报告中引用的第三方信息（包括但不限于网站链接、项目描述、数据统计等）均来自公开渠道，作者不对其真实性、准确性或合法性负责。</p>
</li>
</ul>
<p><strong>2. 报告用途与责任限制：</strong></p>
<ul>
<li><p>本报告仅供参考和学习之用，不构成任何形式的投资建议、技术建议、法律建议或其他专业建议。</p>
</li>
<li><p>读者应自行判断和评估报告中的信息，并根据自身情况做出决策。</p>
</li>
<li><p>对于因使用或依赖本报告中的信息而导致的任何直接或间接损失、损害或不利后果，作者不承担任何责任。</p>
</li>
</ul>
<p><strong>3. 技术使用与合规性：</strong></p>
<ul>
<li><p>本报告中提及的任何爬虫框架、工具或技术，读者应自行负责其合法合规使用。</p>
</li>
<li><p>在使用任何爬虫技术时，读者应遵守相关法律法规（包括但不限于数据隐私保护法、知识产权法、网络安全法等），尊重网站的服务条款和robots协议，不得侵犯他人合法权益。</p>
</li>
<li><p>对于因读者违反相关法律法规或不当使用爬虫技术而导致的任何法律责任或纠纷，作者不承担任何责任。</p>
</li>
</ul>
<p><strong>4. 知识产权：</strong></p>
<ul>
<li><p>本报告的版权归作者所有，未经作者书面许可，任何人不得以任何形式复制、传播、修改或使用本报告的全部或部分内容。</p>
</li>
<li><p>报告中引用的第三方内容，其知识产权归原作者所有。</p>
</li>
</ul>
<p><strong>5. 其他：</strong></p>
<ul>
<li><p>本报告可能包含对未来趋势的预测，这些预测基于作者的判断和假设，不构成任何形式的保证。</p>
</li>
<li><p>作者保留随时修改本免责声明的权利。</p>
</li>
</ul>
<p><strong>请在使用本报告前仔细阅读并理解本免责声明。如果您不同意本免责声明的任何条款，请勿使用本报告。</strong></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>大语言模型中的梯度值：深入理解与应用</p><p><a href="http://acorner.ac.cn/2025/02/18/llm_gradient_descent/">http://acorner.ac.cn/2025/02/18/llm_gradient_descent/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ViniJack.SJX</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-02-18</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-02-19</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E5%8E%9F%E7%90%86/">原理</a><a class="link-muted mr-2" rel="tag" href="/tags/LLM/">LLM</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/02/18/Embed%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/"><span class="level-item">Embedding 模型入门级研究报告</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="A-Corner"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">A-Corner</p><p class="is-size-6 is-block">信息的一角</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>China（Guangzhou）</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Model/"><span class="level-start"><span class="level-item">Model</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/RAG/"><span class="level-start"><span class="level-item">RAG</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/"><span class="level-start"><span class="level-item">分析报告</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">原理</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">向量数据库</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%B7%A5%E5%85%B7/"><span class="level-start"><span class="level-item">工具</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BE%AE%E8%B0%83/"><span class="level-start"><span class="level-item">微调</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A1%86%E6%9E%B6/"><span class="level-start"><span class="level-item">框架</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="level-start"><span class="level-item">食用文档</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-18T15:22:21.629Z">2025-02-18</time></p><p class="title"><a href="/2025/02/18/llm_gradient_descent/">大语言模型中的梯度值：深入理解与应用</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-18T09:06:39.943Z">2025-02-18</time></p><p class="title"><a href="/2025/02/18/Embed%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/">Embedding 模型入门级研究报告</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/Model/">Model</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-14T16:03:29.233Z">2025-02-15</time></p><p class="title"><a href="/2025/02/15/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E3%80%81%E8%87%AA%E5%8A%A8%E5%8C%96%E7%88%AC%E8%99%AB%E3%80%81AI%E7%88%AC%E8%99%AB%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">爬虫框架、自动化爬虫、AI爬虫分析报告</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-14T14:55:50.802Z">2025-02-14</time></p><p class="title"><a href="/2025/02/14/AI%E8%87%AA%E5%8A%A8%E5%8C%96%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AF%B9%E6%AF%94%E6%8A%A5%E5%91%8A/">AI自动化爬虫项目对比报告</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-14T09:13:11.344Z">2025-02-14</time></p><p class="title"><a href="/2025/02/14/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/">向量数据库调研报告</a></p><p class="categories"><a href="/categories/RAG/">RAG</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E5%B7%A5%E5%85%B7/">工具</a> / <a href="/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/">向量数据库</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model/"><span class="tag">Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RAG/"><span class="tag">RAG</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8E%9F%E7%90%86/"><span class="tag">原理</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">向量数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B7%A5%E5%85%B7/"><span class="tag">工具</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%B0%83/"><span class="tag">微调</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A1%86%E6%9E%B6/"><span class="tag">框架</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"><span class="tag">自动化</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="tag">食用文档</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a><p class="is-size-7"><span>&copy; 2025 ViniJack.SJX</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2025 A-Corner</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>