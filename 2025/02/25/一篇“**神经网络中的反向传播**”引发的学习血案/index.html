<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>一篇“**神经网络中的反向传播**”引发的学习血案 - A-Acorner 信息的一角</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="A-Acorner 信息的一角"><meta name="msapplication-TileImage" content="/img/一角favicon.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="A-Acorner 信息的一角"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="神经网络中的反向传播"><meta property="og:type" content="blog"><meta property="og:title" content="一篇“**神经网络中的反向传播**”引发的学习血案"><meta property="og:url" content="http://acorner.ac.cn/2025/02/25/%E4%B8%80%E7%AF%87%E2%80%9C**%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD**%E2%80%9D%E5%BC%95%E5%8F%91%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A1%80%E6%A1%88/"><meta property="og:site_name" content="A-Acorner 信息的一角"><meta property="og:description" content="神经网络中的反向传播"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://acorner.ac.cn/img/og_image.png"><meta property="article:published_time" content="2025-02-25T03:20:54.429Z"><meta property="article:modified_time" content="2025-02-25T03:46:17.437Z"><meta property="article:author" content="ViniJack.SJX"><meta property="article:tag" content="LLM"><meta property="article:tag" content="原理"><meta property="article:tag" content="微调"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="分析报告"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="阅读报告"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://acorner.ac.cn/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://acorner.ac.cn/2025/02/25/%E4%B8%80%E7%AF%87%E2%80%9C**%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD**%E2%80%9D%E5%BC%95%E5%8F%91%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A1%80%E6%A1%88/"},"headline":"一篇“**神经网络中的反向传播**”引发的学习血案","image":["http://acorner.ac.cn/img/og_image.png"],"datePublished":"2025-02-25T03:20:54.429Z","dateModified":"2025-02-25T03:46:17.437Z","author":{"@type":"Person","name":"ViniJack.SJX"},"publisher":{"@type":"Organization","name":"A-Acorner 信息的一角","logo":{"@type":"ImageObject","url":"http://acorner.ac.cn/img/一角logo.png"}},"description":"神经网络中的反向传播"}</script><link rel="canonical" href="http://acorner.ac.cn/2025/02/25/%E4%B8%80%E7%AF%87%E2%80%9C**%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD**%E2%80%9D%E5%BC%95%E5%8F%91%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A1%80%E6%A1%88/"><link rel="icon" href="/img/%E4%B8%80%E8%A7%92favicon.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link data-pjax rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-02-25T03:20:54.429Z" title="2025/2/25 11:20:54">2025-02-25</time>发表</span><span class="level-item"><time dateTime="2025-02-25T03:46:17.437Z" title="2025/2/25 11:46:17">2025-02-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/LLM/">LLM</a><span> / </span><a class="link-muted" href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a><span> / </span><a class="link-muted" href="/categories/%E5%8E%9F%E7%90%86/">原理</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E5%BE%AE%E8%B0%83/">微调</a><span> / </span><a class="link-muted" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><span> / </span><a class="link-muted" href="/categories/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/">阅读报告</a></span><span class="level-item">3 小时读完 (大约23818个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">一篇“**神经网络中的反向传播**”引发的学习血案</h1><div class="content"><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>最近不停的听到反向传播以及方向传播的优化方法的一些东西，久好奇翻了一番之前的看过的一篇文章</p>
<p>原文url:<a target="_blank" rel="noopener" href="https://serokell.io/blog/understanding-backpropagation">https://serokell.io/blog/understanding-backpropagation</a></p>
<span id="more"></span>

<p>还是先放上译文：</p>
<p>好的，以下是文章内容的中文翻译：  </p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="strong">**神经网络中的反向传播**</span>  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**作者：**</span> Irena Logunova  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**发布日期：**</span> 2023 年 12 月 19 日  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**阅读时长：**</span> 13 分钟  </span><br><span class="line">​  </span><br><span class="line">反向传播是神经网络深度学习的一个基本组成部分。自 21 世纪初以来，它的发展极大地促进了深度学习算法的广泛采用。在这篇文章中，我们将探讨这种方法的基本概念、应用和历史。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**什么是前向传播和反向传播？**</span>  </span><br><span class="line">​  </span><br><span class="line">神经网络中的前向传播是指输入数据通过网络各层进行计算并产生输出的过程。每一层处理数据并将其传递到下一层，直到获得最终输出。在此过程中，网络学习识别数据中的模式和关系，通过反向传播调整其权重，以最小化预测输出和实际输出之间的差异。  </span><br><span class="line">​  </span><br><span class="line">（图片：前向传播和反向传播示意图）  </span><br><span class="line">​  </span><br><span class="line">反向传播过程包括计算预测输出和实际目标输出之间的误差，同时将信息反向传递到前馈网络中，从最后一层开始，一直到第一层。为了计算特定层的梯度，使用微积分的链式法则将所有后续层的梯度组合起来。  </span><br><span class="line">​  </span><br><span class="line">反向传播，也称为误差的反向传播，是一种广泛用于计算深度前馈神经网络中导数的技术。它在用于训练此类网络的各种监督学习算法中起着至关重要的作用。  </span><br><span class="line">​  </span><br><span class="line">神经网络的训练涉及使用梯度下降，这是一种迭代优化算法，用于发现可微函数的局部最小值。在训练过程中，计算损失函数来衡量网络预测与实际值之间的差异。反向传播能够计算损失函数相对于网络中每个权重的梯度。这种能力支持单独的权重更新，在多次训练迭代中逐步减少损失函数。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播过程是什么样的？**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播的目的是通过微调神经网络的权重和偏置来最小化成本函数。这些调整的程度取决于成本函数相对于这些特定参数的梯度。通过链式法则计算梯度，反向传播有效地将误差信息向后传播到网络中。因此，网络可以沿梯度的相反方向迭代更新其参数。这个迭代过程使神经网络能够收敛到改进的性能和准确的预测。  </span><br><span class="line">​  </span><br><span class="line">神经网络中反向传播计算权重的梯度的基本步骤是前向传播和反向传播。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**前向传播**</span>  </span><br><span class="line">​  </span><br><span class="line">在前向传播过程中，输入数据逐层通过网络传播，从输入层开始，一直到输出层。网络中的每个神经元接收输入，计算输入的加权和，应用激活函数，并将输出传递到下一层。这个过程一直持续到获得最终输出。前向传播根据当前权重计算网络的输出。  </span><br><span class="line">​  </span><br><span class="line">在我们进行反向传播之前，我们需要介绍计算最佳权重的最快方法，这对于复杂的多参数网络来说并非易事。这就是计算图发挥作用的地方。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**什么是计算图？**</span>  </span><br><span class="line">​  </span><br><span class="line">计算图是一种有向图，用于表示模型内部执行的计算。该图通常以数据 (X) 和标签 (Y) 等输入开始。当我们从左到右移动图形时，会遇到表示计算函数所涉及的基本计算的节点。例如，存在用于输入 (X) 和权重矩阵 (W) 之间的矩阵乘法的节点、用于合页损失（用于 SVM 分类器）的红色节点以及用于模型中正则化项的绿色节点。该图以表示要在模型训练期间计算的标量损失 (L) 的输出节点结束。虽然这个计算图对于由于操作数量有限的线性模型来说似乎很简单，但对于具有多次计算的复杂模型来说，它变得更加复杂和关键。  </span><br><span class="line">​  </span><br><span class="line">（图片：深度网络示意图）  </span><br><span class="line">​  </span><br><span class="line">当我们向后移动时，为了计算最佳损失函数，计算图是一种实现最佳解决方案的方法，它可以显著减少所需的计算量。  </span><br><span class="line">​  </span><br><span class="line">这个过程在[<span class="string">这篇白皮书</span>](<span class="link">this whitepaper</span>)中有详细解释。  </span><br><span class="line">​  </span><br><span class="line">在每个节点上，反向模式微分会合并源自该节点的所有路径。我们不需要评估权重相互影响的所有可能组合，但由于有导数，我们可以通过仅计算每个节点的反向操作一次来获得正确的系数。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播**</span>  </span><br><span class="line">​  </span><br><span class="line">在反向传播中，通过将误差反向传播到网络中来计算权重的梯度。它从输出层开始，一直到输入层。通过将网络的预测输出与真实输出或目标值进行比较来量化误差。使用微积分的链式法则计算损失函数相对于每个权重的梯度，这涉及计算每个层权重的偏导数。然后使用梯度来更新网络的权重，旨在最小化损失函数。  </span><br><span class="line">​  </span><br><span class="line">反向传播本质上决定了每个权重对总体误差的贡献程度，并相应地调整它们。通过迭代执行前向和反向传播，网络学习调整其权重，提高其做出准确预测的能力。  </span><br><span class="line">​  </span><br><span class="line">观看此视频，了解前向和反向传播的详细说明。  </span><br><span class="line">​  </span><br><span class="line">（视频：Backpropagation, step-by-step | DL3）  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播算法有哪些类型？**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播网络的两种主要类型是静态反向传播（提供即时映射）和循环反向传播（涉及定点学习）。  </span><br><span class="line">​  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**静态反向传播：**</span> 它通常用于前馈神经网络和一些卷积神经网络 (CNN)，其中数据点之间没有时间依赖性。该算法累积一批数据点的损失函数梯度，然后对模型的参数执行一次更新。批处理过程有助于利用现代硬件中的并行处理能力，从而使大型数据集的训练过程更加高效。  </span><br><span class="line">​  </span><br><span class="line">    该算法能够解决静态分类问题，例如光学字符识别 (OCR)。  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**循环反向传播：**</span> 循环反向传播是用于循环神经网络 (RNN) 的反向传播算法的扩展。在 RNN 中，数据通过一系列相互连接的节点循环流动，从而使网络能够保留来自先前时间步的信息。  </span><br><span class="line">​  </span><br><span class="line">    循环反向传播涉及及时传播 RNN 中的误差信号。它计算损失函数相对于模型参数在多个时间步长的梯度，同时考虑到当前时间步长和先前时间步长之间的依赖关系和相互作用。此过程使网络能够学习和更新其参数，以提高其在需要顺序或时间依赖性的任务（例如自然语言处理、语音识别和时间序列预测）中的性能。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**为什么要使用反向传播？**</span>  </span><br><span class="line">​  </span><br><span class="line">在前向传播完成后，将评估网络的误差，理想情况下应将其最小化。  </span><br><span class="line">​  </span><br><span class="line">如果当前误差很大，则表明网络尚未有效地从数据中学习。换句话说，当前的权重集不足以准确地最小化误差并生成精确的预测。因此，有必要更新神经网络权重以减少误差。  </span><br><span class="line">​  </span><br><span class="line">反向传播算法在权重更新中起着至关重要的作用，其目标是最小化误差。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播算法的优点**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播具有以下几个关键优势：  </span><br><span class="line">​  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**内存效率**</span>  </span><br><span class="line">​  </span><br><span class="line">    与遗传算法等替代优化算法相比，它可以有效地计算导数，从而减少内存使用量。这在处理大型神经网络时特别有用。  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**速度**</span>  </span><br><span class="line">​  </span><br><span class="line">    它速度很快，特别是对于中小型神经网络。然而，随着层数和神经元数量的增加，计算更多导数会导致性能变慢。  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**通用性**</span>  </span><br><span class="line">​  </span><br><span class="line">    该算法适用于各种网络架构，包括卷积神经网络、生成对抗网络、全连接网络等。反向传播的通用性使其能够在各种场景中有效发挥作用。  </span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**参数简单性**</span>  </span><br><span class="line">​  </span><br><span class="line">    反向传播不需要调整特定参数，从而减少了开销。该过程中涉及的唯一参数与梯度下降算法相关，例如学习率。  </span><br><span class="line">​  </span><br><span class="line">在使用神经网络时，可以利用不同的算法来减少损失函数的输出和学习率，以提供更精确的结果。有许多替代方法可以修改神经网络的属性，例如 Adam（自适应矩估计），多年来一直是最新技术，Nesterov 加速梯度，AdaGrad 和 AdaDelta。  </span><br><span class="line">​  </span><br><span class="line">如果您想了解更多信息，请查看[<span class="string">不同优化器的详细说明</span>](<span class="link">check out this detailed description</span>)。  </span><br><span class="line">​  </span><br><span class="line">损失函数优化的最先进算法之一是 [<span class="string">Sophia 优化器</span>](<span class="link">Sophia optimizer</span>)，由斯坦福大学研究人员于 2023 年 5 月发布。此类优化器的经典示例是成本函数，我们将在下面解释。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**计算反向传播：成本函数**</span>  </span><br><span class="line">​  </span><br><span class="line">成本函数表示模型输出与所需输出之间差异的平方。  </span><br><span class="line">​  </span><br><span class="line">当将神经网络应用于具有相关像素值的数百万张图像时，我们可以假设预测输出和相应的实际值。  </span><br><span class="line">​  </span><br><span class="line">较小的成本函数表示模型在训练数据上的性能更好。此外，预计具有最小化成本函数的模型在未见过的数据上也表现良好。  </span><br><span class="line">​  </span><br><span class="line">成本函数接受所有输入（可能涉及数百万个参数），并产生一个值。该值作为指南，指示模型中需要多少改进。它通知模型其性能不佳，并且需要调整其权重和偏置。然而，仅仅告知模型其性能是不够的。我们还需要为模型提供一种使其能够最小化误差的方法。  </span><br><span class="line">​  </span><br><span class="line">这就是梯度下降和反向传播发挥作用的地方，它们提供了模型更新其参数和减少成本函数的方法。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**梯度下降**</span>  </span><br><span class="line">​  </span><br><span class="line">为了实现更好的参数调整并最小化实际输出和训练输出之间的差异，我们采用了一种称为梯度下降的直观算法。目前，梯度下降是机器学习和深度学习中最流行的优化策略。该算法识别错误并有效地减少它们。  </span><br><span class="line">​  </span><br><span class="line">从数学上讲，它通过找到最小点来优化[<span class="string">凸函数</span>](<span class="link">convex function</span>)。  </span><br><span class="line">​  </span><br><span class="line">梯度的概念可以理解为衡量函数的输出在其输入略微修改时的变化程度。它也可以可视化为函数的斜率，其中较高的梯度表示更陡的斜率并有助于模型更快地学习。从比喻上讲，您可以将其比作下降到山谷的底部而不是爬上山顶。这是因为它是一种最小化给定函数的优化算法。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**梯度下降的类型**</span>  </span><br><span class="line">​  </span><br><span class="line">现在让我们探讨不同类型的梯度下降。  </span><br><span class="line">​  </span><br><span class="line">（图片：梯度下降类型图）  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**批量梯度下降**</span>  </span><br><span class="line">​  </span><br><span class="line">批量大小是指单个批次中包含的训练示例的总数。由于无法一次性将整个数据集传递到神经网络，因此数据集被分成多个批次或子集。  </span><br><span class="line">​  </span><br><span class="line">在批量梯度下降中，使用完整数据集来计算成本函数的梯度。然而，这种方法可能很慢，因为它需要为每次更新计算整个数据集的梯度。这可能具有挑战性，尤其是在大型数据集的情况下。成本函数是在初始化参数后计算的，并且该过程涉及从磁盘读取所有记录到内存中。在每次迭代之后，采取一个步骤，然后重复该过程。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**小批量梯度下降**</span>  </span><br><span class="line">​  </span><br><span class="line">小批量梯度下降是一种常用的算法，可提供更快、更准确的结果。数据集被分成小组或“n”个训练示例的批次。与批量梯度下降不同，小批量梯度下降不使用整个数据集。在每次迭代中，采用“n”个训练示例的子集来计算成本函数的梯度。这种方法减少了参数更新的方差，从而实现更稳定的收敛。此外，它可以利用优化的[<span class="string">矩阵运算</span>](<span class="link">matrix operations</span>)，从而提高梯度计算的效率。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**随机梯度下降**</span>  </span><br><span class="line">​  </span><br><span class="line">随机梯度下降 (SGD) 根据在每次迭代时为数据的随机子集计算的梯度来更新模型的参数，从而允许更快的计算。在每次训练迭代（或时期）中，从训练数据集中选择一批随机数据点。然后使用所选批次计算损失函数相对于模型参数的梯度。接下来，根据计算出的梯度更新模型的参数。在梯度的相反方向上执行更新，以朝着损失函数的最小值移动。对固定数量的迭代或直到满足收敛标准重复这些步骤。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**Tensorflow 和 Pytorch 中的反向传播**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播算法是用于训练深度学习模型的关键技术。  </span><br><span class="line">​  </span><br><span class="line">在 <span class="strong">**TensorFlow**</span> 中，您可以通过定义神经网络模型、使用优化器和损失函数编译它、准备数据，然后使用 fit 函数训练模型来使用反向传播。TensorFlow 的自动微分处理训练期间梯度的计算，从而更容易地应用反向传播来有效地训练复杂模型。  </span><br><span class="line">​  </span><br><span class="line">要在 <span class="strong">**PyTorch**</span> 中使用反向传播，您需要定义神经网络架构和损失函数。在训练过程中，数据通过网络前向传递以进行预测，然后在通过网络各层反向计算梯度的过程中使用反向函数自动计算梯度。然后使用这些梯度通过优化算法（如随机梯度下降）更新模型的参数。  </span><br><span class="line">​  </span><br><span class="line">要了解有关在 PyTorch 中使用反向传播的更多信息，请观看本教程：  </span><br><span class="line">​  </span><br><span class="line">（视频：PyTorch Tutorial 04 - Backpropagation - Theory With Example）  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播的应用**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播广泛用于训练各种类型的神经网络，并且在最近深度学习的普及中发挥了重要作用。但是反向传播的应用范围更广，从天气预报到分析数值稳定性。以下是其在机器学习中的几个应用示例。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**人脸识别**</span>  </span><br><span class="line">​  </span><br><span class="line">卷积神经网络是深度学习中用于图像处理和识别的首选技术，通常使用反向传播算法进行训练。在 Parkhi、Vedaldi 和 Zisserman 于 2015 年进行的一项[<span class="string">研究</span>](<span class="link">study</span>)中，他们开发了一个使用 18 层 CNN 和名人面部数据库的人脸识别系统。该网络使用反向传播在所有 18 层上进行训练，图像分批处理。研究人员使用了一种称为[<span class="string">三重损失</span>](<span class="link">triplet loss</span>)的损失函数来提高网络区分细微面部细微差别的能力。这涉及通过网络馈送三元组图像（例如，两张 Angelina Jolie 的图像和一张 Nicole Kidman 的图像），惩罚网络将同一人的图像错误分类为不同，以及将不同人的图像分类为相似。这个训练过程持续迭代，更新前一层的权重。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**NLP：语音识别**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播已应用于训练各种 NLP 任务的神经网络，包括情感分析、语言翻译、文本生成和语音识别。使用反向传播训练的循环神经网络 (RNN) 通常用于 NLP 中的顺序数据处理。  </span><br><span class="line">​  </span><br><span class="line">例如，索尼开发了一个能够识别英语和日语有限命令的系统。该系统使用传入的声音信号分成时间窗口，应用[<span class="string">快速傅里叶变换</span>](<span class="link">Fast Fourier Transform</span>)提取基于频率的特征，然后将其输入到具有五层的神经网络中。反向传播用于训练这些层以理解日语命令，使用 softmax 交叉熵损失函数。研究人员能够通过再训练使同一网络适应识别英语命令，展示了迁移学习能力。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**事故预防**</span>  </span><br><span class="line">​  </span><br><span class="line">由于地表资源的枯竭，地下矿井的数量正在增加。[<span class="string">本文</span>](<span class="link">This paper</span>)提供了一种改进地下矿井爆炸后重新进入时间预测的方法，这对于确保工人安全和生产力至关重要。目前，使用的方法（如固定时间间隔和经验公式）具有局限性，并且可能不具有普遍适用性。作者建议，反向传播神经网络可以成为一种解决方案。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**反向传播的历史**</span>  </span><br><span class="line">​  </span><br><span class="line">在 19 世纪，法国数学家 Baron Augustin-Louis Cauchy 开发了一种称为梯度下降的方法来求解联立方程组。他的目标是解决涉及多个变量的复杂天文计算。Cauchy 的想法是求函数的导数并采取小步骤以最小化误差。  </span><br><span class="line">​  </span><br><span class="line">在接下来的一个世纪里，梯度下降方法在各个学科中得到了应用，为原本难以或无法通过代数解决的具有挑战性的问题提供了数值解。  </span><br><span class="line">​  </span><br><span class="line">1970 年，芬兰赫尔辛基大学的一名硕士生 Seppo Linnainmaa 提出了一种用于稀疏连接网络中误差反向传播的有效算法。尽管 Linnainmaa 没有特别提及神经网络，但他的工作为未来的发展奠定了基础。  </span><br><span class="line">​  </span><br><span class="line">在 20 世纪 80 年代，研究人员独立开发了时间反向传播，以支持循环神经网络的训练，进一步扩展了该算法的能力。  </span><br><span class="line">​  </span><br><span class="line">1986 年，美国心理学家 David Rumelhart 和他的同事发表了一篇极具影响力的论文，将 Linnainmaa 的反向传播算法应用于多层神经网络。这标志着一个重大的突破；随后的几年见证了基于该算法的进一步发展。例如，[<span class="string">Yann LeCun 1989 年的论文</span>](<span class="link">Yann LeCun&#x27;s 1989 paper</span>)展示了反向传播在卷积神经网络中用于手写数字识别的应用。  </span><br><span class="line">​  </span><br><span class="line">近年来，反向传播在深度神经网络的高效训练中发挥着至关重要的作用。虽然为了并行化算法和利用多个 GPU 已经进行了修改，但 Linnainmaa 开发并由 Rumelhart 推广的原始反向传播算法仍然是当代基于深度学习的人工智能的基础。  </span><br><span class="line">​  </span><br><span class="line">如果您想了解更多关于反向传播的详细的、基于数学的解释，请查看[<span class="string">这篇文章</span>](<span class="link">this article</span>)。  </span><br><span class="line">​  </span><br><span class="line"><span class="strong">**结论**</span>  </span><br><span class="line">​  </span><br><span class="line">反向传播作为深度学习的核心原理，在神经网络参与的许多领域中发挥着重要作用。通过促进神经网络内权重的微调，它有助于为几乎所有行业生成准确的预测和分类。然而，与任何技术一样，必须平衡效率和复杂性，了解算法的局限性，例如梯度下降与反向传播无法识别它是在误差函数的全局最小值还是局部最小值上工作，以及难以跨越误差函数景观中的平台。  </span><br><span class="line">​  </span><br><span class="line">为了克服这些局限性，已经出现了新的方法和其他优化器，这些方法在最新的科学论文中得到了广泛的体现。查看下面的一些出版物。  </span><br><span class="line">​  </span><br><span class="line"><span class="bullet">*</span>   [<span class="string">符号优化算法发现</span>](<span class="link">Symbolic Discovery of Optimization Algorithms</span>)  </span><br><span class="line"><span class="bullet">*</span>   [<span class="string">Adam 优化器</span>](<span class="link">Adam Optimizer</span>)  </span><br><span class="line"><span class="bullet">*</span>   [<span class="string">没有反向传播的优化</span>](<span class="link">Optimization without backpropagation</span>)  </span><br></pre></td></tr></table></figure>


<p>然后。不小心。。。真的不小心整理了一下反向传播的涉及的知识篇。。。。真的不小心的。。。就整理了下面一堆东西了。😑太难了。以为AI 是风（疯）口。。。就扎进去了。。。现在真的疯了。。。。。。如果你也疯了。。请往下看</p>
<h1 id="疯点目录"><a href="#疯点目录" class="headerlink" title="疯点目录"></a>疯点目录</h1><p>根据提供的文章内容，整理出反向传播算法（Backpropagation）的学习知识目录列表，采用层级结构展示：</p>
<ul>
<li><p><strong>一级知识点 1：神经网络基础</strong></p>
<ul>
<li><p>二级知识点 1.1：神经元模型 (Neuron Model)</p>
<ul>
<li><p>三级知识点 1.1.1：输入和输出</p>
</li>
<li><p>三级知识点 1.1.2：权重 (Weights) 和阈值 (Biases&#x2F;Threshold)</p>
</li>
<li><p>三级知识点 1.1.3：激活函数 (Activation Function)</p>
</li>
</ul>
</li>
<li><p>二级知识点 1.2：神经网络结构 (Neural Network Structure)</p>
<ul>
<li><p>三级知识点 1.2.1：输入层 (Input Layer)</p>
</li>
<li><p>三级知识点 1.2.2：隐藏层 (Hidden Layer)</p>
</li>
<li><p>三级知识点 1.2.3：输出层 (Output Layer)</p>
</li>
<li><p>三级知识点 1.2.4：多层感知器 (Multilayer Perceptron, MLP)</p>
</li>
<li><p>三级知识点1.2.5: 前馈神经网络</p>
</li>
</ul>
</li>
<li><p>二级知识点 1.3：常见的神经网络类型</p>
<ul>
<li><p>三级知识点 1.3.1：卷积神经网络 (CNN)</p>
</li>
<li><p>三级知识点 1.3.2: 循环神经网络(RNN)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>一级知识点 2：前向传播 (Forward Propagation)</strong></p>
<ul>
<li><p>二级知识点 2.1：前向传播过程</p>
</li>
<li><p>二级知识点 2.2：计算图 (Computational Graph)</p>
</li>
</ul>
</li>
<li><p><strong>一级知识点 3：反向传播 (Backpropagation)</strong></p>
<ul>
<li><p>二级知识点 3.1：反向传播的目的：计算梯度，更新权重</p>
</li>
<li><p>二级知识点 3.2：链式法则 (Chain Rule)</p>
</li>
<li><p>二级知识点 3.3：反向传播过程</p>
</li>
<li><p>二级知识点 3.4：反向传播的类型</p>
<ul>
<li><p>三级知识点 3.4.1：静态反向传播 (Static Backpropagation)</p>
</li>
<li><p>三级知识点 3.4.2：循环反向传播 (Recurrent Backpropagation)</p>
</li>
</ul>
</li>
<li><p>二级知识点3.5: BP神经网络</p>
</li>
</ul>
</li>
<li><p><strong>一级知识点 4：优化算法 (Optimization Algorithms)</strong></p>
<ul>
<li><p>二级知识点 4.1：梯度下降 (Gradient Descent)</p>
<ul>
<li><p>三级知识点 4.1.1：批量梯度下降 (Batch Gradient Descent)</p>
</li>
<li><p>三级知识点 4.1.2：小批量梯度下降 (Mini-batch Gradient Descent)</p>
</li>
<li><p>三级知识点 4.1.3：随机梯度下降 (Stochastic Gradient Descent)</p>
</li>
</ul>
</li>
<li><p>二级知识点 4.2：其他优化器 (Optimizers)</p>
<ul>
<li><p>三级知识点 4.2.1：Adam (Adaptive Moment Estimation)</p>
</li>
<li><p>三级知识点 4.2.2：其他 (AdaGrad, AdaDelta, Nesterov Accelerated Gradient, Sophia, etc.)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>一级知识点5: 损失函数</strong></p>
<ul>
<li><p>二级知识点 5.1: 损失函数的定义</p>
</li>
<li><p>二级知识点 5.2: 不同任务的损失函数</p>
<ul>
<li><p>三级知识点 5.2.1: 分类任务</p>
</li>
<li><p>三级知识点 5.2.2: 回归任务</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>一级知识点 6：反向传播的应用</strong></p>
<ul>
<li><p>二级知识点 6.1：人脸识别 (Face Recognition)</p>
</li>
<li><p>二级知识点 6.2：自然语言处理：语音识别 (NLP: Speech Recognition)</p>
</li>
<li><p>二级知识点 6.3: 事故预防</p>
</li>
</ul>
</li>
<li><p><strong>一级知识点 7：反向传播的历史</strong></p>
</li>
<li><p><strong>一级知识点8: 超参数</strong></p>
<ul>
<li><p>二级知识点 8.1: 输入图像大小</p>
</li>
<li><p>二级知识点 8.2: 学习率</p>
</li>
<li><p>二级知识点 8.3: 正则化参数</p>
</li>
<li><p>二级知识点 8.4: 神经网络层数</p>
</li>
<li><p>二级知识点 8.5: 批处理大小(batch size)</p>
</li>
<li><p>二级知识点 8.6: 卷积层参数</p>
</li>
<li><p>二级知识点 8.7: 池化层参数</p>
</li>
<li><p>二级知识点 8.8: 迭代周期</p>
</li>
</ul>
</li>
</ul>
<p>好的！虽然有点疯的目录看起来很鼓噪，那就让我们告别严肃的学术氛围，来点轻松幽默的风格，开启反向传播的学习之旅！</p>
<blockquote>
<p>哇哦！这篇文章真是信息量爆炸，就像一口气吞下了一整本神经网络的“天书”！😵‍💫 别担心，虽然内容有点多，但咱们可以像剥洋葱一样，一层一层地揭开反向传播的神秘面纱。🧅</p>
</blockquote>
<blockquote>
<p>不过，在正式开始“剥洋葱”之前，我们需要先整理一下“工具箱”🧰，也就是梳理一下知识点。毕竟，磨刀不误砍柴工嘛！🔪</p>
</blockquote>
<blockquote>
<p>想象一下，反向传播就像是一位“调皮”的神经网络教练，它会根据网络“学员”的表现（预测结果）来“敲打”它们，让它们不断改进，最终成为“学霸”！👨‍🎓👩‍🎓</p>
</blockquote>
<blockquote>
<p>那么，这位“教练”究竟有哪些“独门秘籍”呢？🤔 让我们一起列个清单，把这些“秘籍”分门别类，变成我们的学习路线图！🗺️</p>
</blockquote>
<p>接下来，我将按照之前提供的层级结构知识目录列表，开始 Step-by-step 的学习引导。准备好了吗？Let’s go! 🚀</p>
<h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><p><strong>一级知识点 1：神经网络基础 🧠</strong></p>
<p>在深入了解反向传播之前，我们需要先熟悉一下神经网络的基础知识。这就像盖房子要先打好地基一样重要！🧱</p>
<p><strong>二级知识点 1.1：神经元模型 💡</strong></p>
<p>神经元是神经网络的基本组成单位。你可以把它想象成一个“小精灵”🧚‍♀️，它接收一些输入信号，然后根据自己的“心情”（权重和偏置）做出反应，产生一个输出信号。</p>
<ul>
<li><p><strong>三级知识点 1.1.1：输入和输出 ➡️⬅️</strong> 神经元接收来自其他神经元的信号作为输入，然后产生一个信号作为输出。就像你听到朋友的笑话（输入），然后哈哈大笑（输出）一样。😆</p>
</li>
<li><p><strong>三级知识点 1.1.2：权重 (Weights) 和阈值 (Biases&#x2F;Threshold) ⚖️</strong> 每个输入信号都有一个权重，表示这个信号的重要性。权重越大，这个信号就越重要。阈值就像是一个“门槛”，只有当输入的加权和超过这个门槛时，神经元才会被“激活”，产生输出。 可以这样理解： 权重好比你对不同朋友的信任度，信任度越高，他们的话就越重要。 阈值好比你的笑点，只有笑话足够好笑（超过笑点），你才会笑出来。😄</p>
</li>
<li><p><strong>三级知识点 1.1.3：激活函数 (Activation Function) 🔥</strong> 激活函数就像是神经元的“情绪调节器”，它将输入的加权和转换成一个输出信号。激活函数有很多种，每种都有不同的“调节”效果。 例如，Sigmoid 函数可以将输入压缩到 0 到 1 之间，就像把神经元的“情绪”控制在一个温和的范围内。😌</p>
</li>
</ul>
<p><strong>二级知识点 1.2：神经网络结构 🏗️</strong></p>
<p>神经网络是由许多神经元相互连接而成的。这些神经元按照不同的层次排列，形成了不同的网络结构。</p>
<ul>
<li><p><strong>三级知识点 1.2.1：输入层 (Input Layer) 📥</strong> 输入层负责接收外部数据。就像你的眼睛 👀 和耳朵 👂 负责接收外界的信息一样。</p>
</li>
<li><p><strong>三级知识点 1.2.2：隐藏层 (Hidden Layer) 🕵️</strong> 隐藏层是神经网络的“大脑”，负责处理输入数据，提取特征。隐藏层可以有很多层，层数越多，网络就越复杂，处理能力也越强。就像福尔摩斯的大脑一样，层层推理，最终找出真相！🔍</p>
</li>
<li><p><strong>三级知识点 1.2.3：输出层 (Output Layer) 📤</strong> 输出层负责产生网络的最终输出。就像你的嘴巴 👄，负责说出你的想法。</p>
</li>
<li><p><strong>三级知识点 1.2.4：多层感知器 (Multilayer Perceptron, MLP) 🏢</strong> 多层感知器是一种常见的神经网络结构，它由多个层次的神经元组成，每一层的神经元都与下一层的所有神经元相连。 你可以把多层感知器想象成一栋大楼，每一层都有很多房间（神经元），每个房间都与下一层的所有房间相连。🏢</p>
</li>
<li><p><strong>三级知识点 1.2.5：前馈神经网络 ➡️</strong> 在前馈神经网络中，信息只能从输入层流向输出层，不能反向流动。就像单行道一样，只能前进，不能后退。🚗</p>
</li>
</ul>
<p><strong>二级知识点1.3: 常见的神经网络类型</strong> * <strong>三级知识点 1.3.1：卷积神经网络 (CNN) 🖼️</strong> 卷积神经网络特别擅长处理图像。 比如识别人脸， 自动驾驶。 * <strong>三级知识点 1.3.2：循环神经网络 (RNN) ✍️</strong> 循环神经网络更适合处理有顺序的信息， 比如分析一段话， 预测下一秒钟的股票价格。</p>
<h1 id="前向传播-Forward-Propagation"><a href="#前向传播-Forward-Propagation" class="headerlink" title="前向传播 (Forward Propagation)"></a>前向传播 (Forward Propagation)</h1><p><strong>一级知识点 2：前向传播 (Forward Propagation) ➡️</strong></p>
<p>前向传播是神经网络处理信息的过程，就像水流顺着管道流动一样自然。💧</p>
<ul>
<li><p><strong>二级知识点 2.1：前向传播过程 🚶</strong></p>
<ol>
<li><p><strong>数据输入:</strong> 首先，我们将数据（例如一张图片 🖼️ 或一段文字 📝）输入到神经网络的输入层。</p>
</li>
<li><p><strong>逐层传递:</strong> 数据从输入层开始，逐层通过隐藏层。每一层的神经元都会对接收到的数据进行加权求和，然后通过激活函数进行处理，产生输出。就像接力赛一样，每一棒的选手（神经元）都会把接力棒（数据）传递给下一棒。🏃‍♀️🏃‍♂️</p>
</li>
<li><p><strong>输出结果:</strong> 最后，数据到达输出层，产生网络的最终输出结果。这个结果可能是对图片的分类、对文字的翻译等等。</p>
</li>
</ol>
</li>
<li><p><strong>二级知识点 2.2：计算图 (Computational Graph) 📊</strong></p>
<p>  为了更好地理解前向传播和后续的反向传播过程，我们可以使用计算图来表示。</p>
<ul>
<li><p>计算图是一种有向图，它将计算过程表示为一系列节点和边。</p>
</li>
<li><p>节点表示计算操作（例如加法、乘法、激活函数等），边表示数据流动的方向。</p>
</li>
<li><p>通过计算图，我们可以清晰地看到数据是如何在神经网络中流动的，以及每个节点是如何参与计算的。</p>
</li>
</ul>
<p>  举个例子：假设我们要计算 <code>y = (x + w) * b</code>，我们可以用计算图表示为：</p>
<p>  x —-(+)—-(*)—- y<br>   |     ^     ^<br>   |     |     |<br>   w —-|     b —-|</p>
<p>  在这个图中，<code>x</code>、<code>w</code>、<code>b</code> 是输入节点，<code>(+)</code> 和 <code>(*)</code> 是计算节点，<code>y</code> 是输出节点。数据从输入节点流向输出节点，完成了计算过程。</p>
</li>
</ul>
<p>前向传播就像是神经网络的“思考”过程，它根据输入数据和当前的权重，一步一步地计算出最终的输出结果。🤔</p>
<p>现在，你已经了解了前向传播的过程了！请你思考一下：</p>
<ul>
<li><p>你能用自己的话描述一下前向传播的过程吗？🗣️</p>
</li>
<li><p>你能画出一个简单的神经网络的计算图吗？✍️</p>
</li>
</ul>
<h2 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h2><p>好的，为了让你更好地理解知识点，我将结合之前的神经网络基础和前向传播内容，给出一些更具体的范例，并配以图示和代码片段（Python + PyTorch）进行说明。</p>
<p><strong>一级知识点 1：神经网络基础 🧠</strong></p>
<p><strong>二级知识点 1.1：神经元模型 💡</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们有一个简单的神经元，用于判断一封邮件是否是垃圾邮件。</p>
<ul>
<li><strong>输入 (Inputs):</strong><ul>
<li>x1: 邮件中包含“免费”一词的次数 (例如：3)</li>
<li>x2: 邮件中包含链接的数量 (例如：5)</li>
<li>x3: 发件人是否在已知的垃圾邮件发送者列表中 (1 表示是，0 表示否) (例如：1)</li>
</ul>
</li>
<li><strong>权重 (Weights):</strong><ul>
<li>w1: 0.8 (表示“免费”一词的重要性)</li>
<li>w2: 0.5 (表示链接数量的重要性)</li>
<li>w3: 1.2 (表示发件人是否在垃圾邮件列表中的重要性)</li>
</ul>
</li>
<li><strong>偏置 (Bias):</strong><ul>
<li>b: -1.0</li>
</ul>
</li>
<li><strong>激活函数 (Activation Function):</strong><ul>
<li>Sigmoid 函数：σ(z) &#x3D; 1 &#x2F; (1 + exp(-z)) （将输出压缩到 0-1 之间）</li>
</ul>
</li>
</ul>
<p><strong>计算过程:</strong></p>
<ol>
<li><strong>加权和 (Weighted Sum):</strong><br>z &#x3D; (x1 * w1) + (x2 * w2) + (x3 * w3) + b<br>z &#x3D; (3 * 0.8) + (5 * 0.5) + (1 * 1.2) + (-1.0) &#x3D; 2.4 + 2.5 + 1.2 - 1.0 &#x3D; 5.1</li>
<li><strong>激活函数 (Activation):</strong><br>output &#x3D; σ(z) &#x3D; 1 &#x2F; (1 + exp(-5.1)) ≈ 0.994</li>
</ol>
<p><strong>结论:</strong> 输出值接近 1，表示这个神经元认为这封邮件很可能是垃圾邮件。</p>
<p><strong>PyTorch 代码片段:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经元 (单层线性模型 + Sigmoid 激活函数)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 3，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sigmoid(<span class="variable language_">self</span>.linear(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建神经元实例</span></span><br><span class="line">neuron = Neuron()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置权重和偏置 (手动设置，实际应用中通过训练学习)</span></span><br><span class="line">neuron.linear.weight.data = torch.tensor([[<span class="number">0.8</span>, <span class="number">0.5</span>, <span class="number">1.2</span>]])</span><br><span class="line">neuron.linear.bias.data = torch.tensor([-<span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>])  <span class="comment"># 注意：输入数据类型需为浮点数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出</span></span><br><span class="line">output = neuron(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;神经元输出: <span class="subst">&#123;output.item():<span class="number">.3</span>f&#125;</span>&quot;</span>)  <span class="comment"># 输出: 神经元输出: 0.994</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二级知识点 1.2：神经网络结构 🏗️</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们要构建一个简单的多层感知器 (MLP) 来识别手写数字（0-9）。</p>
<ul>
<li><strong>输入层 (Input Layer):</strong> 784 个神经元 (28x28 像素的图像)</li>
<li><strong>隐藏层 (Hidden Layer):</strong> 128 个神经元 (使用 ReLU 激活函数)</li>
<li><strong>输出层 (Output Layer):</strong> 10 个神经元 (分别代表 0-9 十个数字，使用 Softmax 激活函数)</li>
</ul>
<p><strong>图示:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[输入层]      [隐藏层]       [输出层]</span><br><span class="line">(784 个)      (128 个)       (10 个)</span><br><span class="line">O O O ... O   O O O ... O    O O O ... O</span><br><span class="line">O O O ... O   O O O ... O    O O O ... O</span><br><span class="line">...          ...            ...</span><br><span class="line">O O O ... O   O O O ... O    O O O ... O</span><br></pre></td></tr></table></figure>

<p><strong>PyTorch 代码片段:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义多层感知器 (MLP)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()  <span class="comment"># 将 28x28 的图像展平为 784 维向量</span></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(<span class="number">784</span>, <span class="number">128</span>)  <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.softmax = nn.Softmax(dim=<span class="number">1</span>)  <span class="comment"># Softmax 激活函数 (dim=1 表示对每一行进行 Softmax)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.linear1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.softmax(<span class="variable language_">self</span>.linear2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 MLP 实例</span></span><br><span class="line">mlp = MLP()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据 (一张 28x28 的手写数字图像)</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># (batch_size, height, width)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出</span></span><br><span class="line">output = mlp(input_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;MLP 输出: <span class="subst">&#123;output&#125;</span>&quot;</span>)  <span class="comment"># 输出一个 1x10 的概率分布</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>一级知识点 2：前向传播 ➡️</strong></p>
<p><strong>二级知识点 2.1：前向传播过程 🚶</strong></p>
<ul>
<li><p><strong>范例:</strong> 沿用上面的 MLP 识别手写数字的例子，我们来看前向传播的具体过程：</p>
<ol>
<li><strong>输入图像:</strong> 将一张 28x28 的手写数字图像 (例如数字 “3”) 输入到 MLP 的输入层。图像的每个像素值 (0-255) 对应输入层的一个神经元。</li>
<li><strong>展平:</strong> 将 28x28 的二维图像展平为 784 维的一维向量。</li>
<li><strong>输入层到隐藏层:</strong><ul>
<li>对 784 维的输入向量进行加权求和：每个输入值乘以对应的权重，然后加上偏置。</li>
<li>应用 ReLU 激活函数：将加权和的结果输入到 ReLU 函数中，得到隐藏层的输出。</li>
</ul>
</li>
<li><strong>隐藏层到输出层:</strong><ul>
<li>对 128 维的隐藏层输出进行加权求和：每个隐藏层输出值乘以对应的权重，然后加上偏置。</li>
<li>应用 Softmax 激活函数：将加权和的结果输入到 Softmax 函数中，得到一个 10 维的概率分布向量。向量的每个元素表示对应数字的概率。</li>
</ul>
</li>
<li><strong>输出结果:</strong> 输出层产生一个 10 维的概率分布向量，其中概率最高的元素对应的数字就是 MLP 的预测结果。例如，如果输出向量中第 4 个元素 (索引为 3) 的概率最高，则 MLP 预测这张图片是数字 “3”。</li>
</ol>
</li>
</ul>
<p><strong>二级知识点 2.2：计算图 📊</strong><br>前向传播流程图与神经网络结构图基本一致.</p>
<h1 id="反向传播-Backpropagation"><a href="#反向传播-Backpropagation" class="headerlink" title="反向传播 (Backpropagation)"></a>反向传播 (Backpropagation)</h1><p><strong>一级知识点 3：反向传播 (Backpropagation) 🔄</strong></p>
<p>如果说前向传播是神经网络的“思考”过程，那么反向传播就是神经网络的“反思”和“学习”过程。🤔</p>
<ul>
<li><p><strong>二级知识点 3.1：反向传播的目的：计算梯度，更新权重 🎯</strong></p>
<ul>
<li><p><strong>计算梯度 (Calculate Gradients):</strong> 反向传播的核心目的是计算损失函数相对于神经网络中每个权重和偏置的梯度。梯度表示了损失函数的变化趋势，指明了权重和偏置应该如何调整才能减少损失。</p>
<ul>
<li>你可以把梯度想象成一个“指南针”🧭，它指引着权重和偏置朝着“损失最小化”的方向前进。</li>
</ul>
</li>
<li><p><strong>更新权重 (Update Weights):</strong> 一旦计算出梯度，就可以使用优化算法（例如梯度下降）来更新神经网络中的权重和偏置。通过不断地调整权重和偏置，神经网络的预测结果会越来越准确。</p>
<ul>
<li>这个过程就像是“雕刻家”👨‍🎨 不断地调整“雕塑”的细节，使其越来越接近理想的形状。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>二级知识点 3.2：链式法则 (Chain Rule) 🔗</strong></p>
<p>  反向传播算法的核心是微积分中的链式法则。链式法则用于计算复合函数的导数。</p>
<ul>
<li><p><strong>简单例子:</strong> 假设你有两个函数：<code>y = f(u)</code> 和 <code>u = g(x)</code>，那么 <code>y</code> 关于 <code>x</code> 的导数可以通过链式法则计算： <code>dy/dx = (dy/du) * (du/dx)</code></p>
</li>
<li><p><strong>在神经网络中:</strong> 神经网络可以看作是一个非常复杂的复合函数。每一层都可以看作是一个函数，将前一层的输出作为输入，产生当前层的输出。反向传播利用链式法则，从输出层开始，逐层计算损失函数相对于每个权重和偏置的梯度。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 3.3：反向传播过程 ⏪</strong></p>
<ol>
<li><p><strong>前向传播:</strong> 首先，进行一次前向传播，计算出网络的输出和损失。</p>
</li>
<li><p><strong>计算输出层梯度:</strong> 计算损失函数相对于输出层神经元输出的梯度。</p>
</li>
<li><p><strong>反向传播梯度:</strong> 从输出层开始，逐层向前计算梯度。</p>
<ul>
<li><p>使用链式法则，计算损失函数相对于每一层权重和偏置的梯度。</p>
</li>
<li><p>将梯度传递到前一层。</p>
</li>
</ul>
</li>
<li><p><strong>更新权重和偏置:</strong> 使用优化算法（例如梯度下降），根据计算出的梯度更新权重和偏置。</p>
</li>
</ol>
<p>  <strong>形象比喻:</strong> 想象你正在玩一个“猜数字”游戏。🤖</p>
<ol>
<li><p><strong>前向传播:</strong> 你猜一个数字 (输入)，然后朋友告诉你猜的数字是大了还是小了 (输出&#x2F;损失)。</p>
</li>
<li><p><strong>反向传播:</strong> 你根据朋友的反馈 (损失)，反思自己猜数字的策略 (权重)，并调整自己的策略 (更新权重)，以便下次猜得更准。</p>
</li>
</ol>
</li>
<li><p><strong>二级知识点 3.4：反向传播的类型</strong></p>
<ul>
<li><p><strong>三级知识点 3.4.1：静态反向传播 (Static Backpropagation)</strong> 处理每一次输入都是独立的，没有前后关联的情况。</p>
</li>
<li><p><strong>三级知识点 3.4.2：循环反向传播 (Recurrent Backpropagation)</strong> 用于处理序列数据，如文本或时间序列，其中当前输入与之前的输入有关联。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点3.5: BP神经网络</strong> 使用反向传播算法的神经网络。</p>
</li>
</ul>
<p>反向传播是神经网络学习的核心，它使得神经网络能够从错误中学习，不断提高自己的性能。💪</p>
<p>现在，你已经了解了反向传播的基本原理和过程。请思考：</p>
<ul>
<li><p>你能用自己的话解释反向传播的目的和过程吗？</p>
</li>
<li><p>链式法则在反向传播中起到了什么作用？</p>
</li>
</ul>
<h2 id="范例-1"><a href="#范例-1" class="headerlink" title="范例"></a>范例</h2><p>好的，我们来为反向传播 (Backpropagation) 这一关键概念提供更具体的范例，并结合 PyTorch 代码进行说明，让它更易于理解。</p>
<p><strong>一级知识点 3：反向传播 (Backpropagation) 🔄</strong></p>
<p><strong>二级知识点 3.1：反向传播的目的：计算梯度，更新权重 🎯</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们有一个非常简单的神经网络，只有一个输入 <code>x</code>、一个权重 <code>w</code>、一个偏置 <code>b</code> 和一个输出 <code>y</code>。我们的目标是预测 <code>y_true</code>。</p>
<ul>
<li><strong>模型:</strong> <code>y = w * x + b</code></li>
<li><strong>损失函数 (Loss Function):</strong> 均方误差 (Mean Squared Error, MSE)：<code>L = (y - y_true)^2</code></li>
<li><strong>目标:</strong> 找到合适的 <code>w</code> 和 <code>b</code>，使得损失函数 <code>L</code> 最小化。</li>
</ul>
<p><strong>计算梯度 (Calculate Gradients):</strong></p>
<ol>
<li><strong>∂L&#x2F;∂y:</strong> 损失函数 <code>L</code> 相对于模型输出 <code>y</code> 的梯度。<br><code>∂L/∂y = 2 * (y - y_true)</code></li>
<li><strong>∂y&#x2F;∂w:</strong> 模型输出 <code>y</code> 相对于权重 <code>w</code> 的梯度。<br><code>∂y/∂w = x</code></li>
<li><strong>∂y&#x2F;∂b:</strong> 模型输出 <code>y</code> 相对于偏置 <code>b</code> 的梯度。<br><code>∂y/∂b = 1</code></li>
<li><strong>∂L&#x2F;∂w:</strong> 损失函数 <code>L</code> 相对于权重 <code>w</code> 的梯度 (使用链式法则)。<br><code>∂L/∂w = (∂L/∂y) * (∂y/∂w) = 2 * (y - y_true) * x</code></li>
<li><strong>∂L&#x2F;∂b:</strong> 损失函数 <code>L</code> 相对于偏置 <code>b</code> 的梯度 (使用链式法则)。<br><code>∂L/∂b = (∂L/∂y) * (∂y/∂b) = 2 * (y - y_true) * 1 = 2 * (y - y_true)</code></li>
</ol>
<p><strong>更新权重 (Update Weights):</strong></p>
<ul>
<li><strong>学习率 (Learning Rate):</strong>  <code>lr</code> (例如：0.1)</li>
<li><strong>权重更新:</strong> <code>w = w - lr * (∂L/∂w)</code></li>
<li><strong>偏置更新:</strong> <code>b = b - lr * (∂L/∂b)</code></li>
</ul>
<p><strong>PyTorch 代码片段:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入、权重、偏置、真实值</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># requires_grad=True 表示需要计算梯度</span></span><br><span class="line">w = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">0.5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_true = torch.tensor(<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">y = w * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = (y - y_true)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播 (自动计算梯度)</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;∂L/∂w: <span class="subst">&#123;w.grad&#125;</span>&quot;</span>)  <span class="comment"># ∂L/∂w: -8.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;∂L/∂b: <span class="subst">&#123;b.grad&#125;</span>&quot;</span>)  <span class="comment"># ∂L/∂b: -4.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新权重和偏置 (手动更新，实际应用中通常使用优化器)</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在更新权重和偏置时，不需要计算梯度</span></span><br><span class="line">    w -= lr * w.grad</span><br><span class="line">    b -= lr * b.grad</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清空梯度 (重要！否则梯度会累积)</span></span><br><span class="line">    w.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;更新后的权重 w: <span class="subst">&#123;w.item()&#125;</span>&quot;</span>)  <span class="comment"># 更新后的权重 w: 1.8</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;更新后的偏置 b: <span class="subst">&#123;b.item()&#125;</span>&quot;</span>)  <span class="comment"># 更新后的偏置 b: 0.9</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二级知识点 3.2：链式法则 (Chain Rule) 🔗</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们有一个稍微复杂一点的神经网络：</p>
<ul>
<li><strong>第一层:</strong> <code>y1 = w1 * x + b1</code></li>
<li><strong>第二层:</strong> <code>y2 = w2 * y1 + b2</code></li>
<li><strong>损失函数:</strong> <code>L = (y2 - y_true)^2</code></li>
</ul>
<p>我们要计算损失函数 <code>L</code> 相对于 <code>w1</code> 的梯度 <code>∂L/∂w1</code>。</p>
<p><strong>链式法则分解:</strong></p>
<ol>
<li><strong>∂L&#x2F;∂y2:</strong> 损失函数 <code>L</code> 相对于第二层输出 <code>y2</code> 的梯度。<br><code>∂L/∂y2 = 2 * (y2 - y_true)</code></li>
<li><strong>∂y2&#x2F;∂y1:</strong> 第二层输出 <code>y2</code> 相对于第一层输出 <code>y1</code> 的梯度。<br><code>∂y2/∂y1 = w2</code></li>
<li><strong>∂y1&#x2F;∂w1:</strong> 第一层输出 <code>y1</code> 相对于权重 <code>w1</code> 的梯度。<br><code>∂y1/∂w1 = x</code></li>
<li><strong>∂L&#x2F;∂w1:</strong> 损失函数 <code>L</code> 相对于权重 <code>w1</code> 的梯度 (使用链式法则)。<br><code>∂L/∂w1 = (∂L/∂y2) * (∂y2/∂y1) * (∂y1/∂w1) = 2 * (y2 - y_true) * w2 * x</code></li>
</ol>
<p><strong>PyTorch 代码片段 (演示自动微分):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入、权重、偏置、真实值</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w1 = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.tensor(<span class="number">0.5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_true = torch.tensor(<span class="number">8.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">y1 = w1 * x + b1</span><br><span class="line">y2 = w2 * y1 + b2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = (y2 - y_true)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播 (自动计算梯度)</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;∂L/∂w1: <span class="subst">&#123;w1.grad&#125;</span>&quot;</span>)  <span class="comment"># ∂L/∂w1: -24.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (省略权重和偏置的更新步骤，与前面的例子类似)</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二级知识点 3.3：反向传播过程 ⏪</strong></p>
<ul>
<li><p><strong>结合 MLP 的范例:</strong></p>
<p>回顾一下我们之前用于识别手写数字的 MLP（输入层 784 个神经元，隐藏层 128 个神经元，输出层 10 个神经元）。</p>
<p><strong>反向传播过程:</strong></p>
<ol>
<li><strong>前向传播:</strong> 输入一张手写数字图像，经过 MLP 的前向传播，得到输出层的预测概率分布。</li>
<li><strong>计算损失:</strong> 使用交叉熵损失函数 (Cross-Entropy Loss) 计算预测概率分布与真实标签 (one-hot 编码) 之间的差异。</li>
<li><strong>计算输出层梯度:</strong> 计算损失函数相对于输出层神经元输出的梯度。</li>
<li><strong>反向传播梯度 (隐藏层到输出层):</strong><ul>
<li>计算损失函数相对于输出层权重和偏置的梯度 (使用链式法则)。</li>
<li>计算损失函数相对于隐藏层输出的梯度 (使用链式法则)。</li>
</ul>
</li>
<li><strong>反向传播梯度 (输入层到隐藏层):</strong><ul>
<li>计算损失函数相对于隐藏层权重和偏置的梯度 (使用链式法则)。</li>
<li>计算损失函数相对于输入层输出的梯度 (使用链式法则，但通常不需要更新输入层的权重)。</li>
</ul>
</li>
<li><strong>更新权重和偏置:</strong> 使用优化算法 (例如 Adam)，根据计算出的梯度更新所有层的权重和偏置。</li>
</ol>
</li>
</ul>
<h1 id="优化算法-Optimization-Algorithms"><a href="#优化算法-Optimization-Algorithms" class="headerlink" title="优化算法 (Optimization Algorithms)"></a>优化算法 (Optimization Algorithms)</h1><p>OK！准备好迎接优化算法了吗？它们可是反向传播的“神助攻”！💪</p>
<p><strong>一级知识点 4：优化算法 (Optimization Algorithms) ⚙️</strong></p>
<p>优化算法在神经网络训练中扮演着至关重要的角色。它们利用反向传播计算出的梯度，来更新网络的权重和偏置，目标是找到使损失函数最小化的参数值。</p>
<ul>
<li><p><strong>二级知识点 4.1：梯度下降 (Gradient Descent) ⛰️</strong></p>
<p>  梯度下降是最基本、最常用的优化算法。它就像一个“探险家”🚶，沿着梯度的反方向（最陡峭的下坡方向）前进，一步一步地寻找“山谷”（损失函数的最小值）。</p>
<ul>
<li><p><strong>核心思想:</strong></p>
<ul>
<li><p>计算损失函数相对于每个参数（权重和偏置）的梯度。</p>
</li>
<li><p>沿着梯度的反方向更新参数：<code>参数 = 参数 - 学习率 * 梯度</code></p>
</li>
<li><p>学习率 (Learning Rate) 控制着每次更新的步长。学习率太大可能会“跨过”最小值，太小则可能导致收敛速度过慢。</p>
</li>
</ul>
</li>
<li><p><strong>三级知识点 4.1.1：批量梯度下降 (Batch Gradient Descent) 🐢</strong></p>
<ul>
<li><p>每次迭代使用整个训练数据集来计算梯度和更新参数。</p>
</li>
<li><p><strong>优点:</strong> 稳定，能够保证收敛到局部最小值（对于凸函数，可以收敛到全局最小值）。</p>
</li>
<li><p><strong>缺点:</strong> 速度慢，特别是对于大型数据集，计算量巨大。</p>
</li>
</ul>
</li>
<li><p><strong>三级知识点 4.1.2：小批量梯度下降 (Mini-batch Gradient Descent) 🐇</strong></p>
<ul>
<li><p>每次迭代使用训练数据集的一个子集（称为“批次” batch）来计算梯度和更新参数。</p>
</li>
<li><p><strong>优点:</strong> 速度比批量梯度下降快，计算量较小，同时又能保持一定的稳定性。</p>
</li>
<li><p><strong>缺点:</strong> 可能会在最小值附近震荡。</p>
</li>
</ul>
</li>
<li><p><strong>三级知识点 4.1.3：随机梯度下降 (Stochastic Gradient Descent) 🚀</strong></p>
<ul>
<li><p>每次迭代只使用一个训练样本来计算梯度和更新参数。</p>
</li>
<li><p><strong>优点:</strong> 速度非常快，适合于大型数据集和在线学习。</p>
</li>
<li><p><strong>缺点:</strong> 非常不稳定，可能会在最小值附近剧烈震荡，甚至无法收敛。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>二级知识点 4.2：其他优化器 (Optimizers) ✨</strong></p>
<p>  除了梯度下降，还有许多更高级、更有效的优化算法。它们通常在梯度下降的基础上进行改进，以解决梯度下降的局限性，例如收敛速度慢、容易陷入局部最小值等问题。</p>
<ul>
<li><p><strong>三级知识点 4.2.1：Adam (Adaptive Moment Estimation) 🤖</strong></p>
<ul>
<li><p>Adam 是一种自适应学习率优化算法，它结合了动量法 (Momentum) 和 RMSprop 的思想。</p>
</li>
<li><p><strong>优点:</strong> 收敛速度快，对超参数的设置不敏感，通常表现良好。</p>
</li>
<li><p><strong>原理 (简要):</strong></p>
<ul>
<li><p><strong>动量 (Momentum):</strong> 考虑了之前梯度的累积效应，有助于加速收敛并减少震荡。</p>
</li>
<li><p><strong>RMSprop:</strong> 对每个参数使用不同的学习率，根据参数梯度的历史平方均值进行调整，有助于处理稀疏梯度和非平稳目标函数。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>三级知识点 4.2.2：其他 (AdaGrad, AdaDelta, Nesterov Accelerated Gradient, Sophia, etc.) 🤓</strong></p>
<ul>
<li><p><strong>AdaGrad:</strong> 自适应学习率算法，对不频繁更新的参数使用更大的学习率，对频繁更新的参数使用更小的学习率。</p>
</li>
<li><p><strong>AdaDelta:</strong> AdaGrad 的改进版，解决了 AdaGrad 学习率单调递减的问题。</p>
</li>
<li><p><strong>Nesterov Accelerated Gradient (NAG):</strong> 在动量法的基础上进行改进，通过在计算梯度时“向前看一步”，提高了收敛速度。</p>
</li>
<li><p><strong>Sophia:</strong> 一种新的二阶优化算法, 在一些情况下能获得比Adam更好的结果.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>优化算法的选择对于神经网络的训练至关重要。选择合适的优化算法可以加速训练过程，提高模型性能。</p>
<p>现在，你已经了解了优化算法的基本概念和常见类型。请思考：</p>
<ul>
<li><p>你能用自己的话解释梯度下降算法的原理吗？</p>
</li>
<li><p>不同的梯度下降变体（批量、小批量、随机）有什么区别？</p>
</li>
<li><p>Adam 优化器有哪些优点？</p>
</li>
</ul>
<h2 id="范例-2"><a href="#范例-2" class="headerlink" title="范例"></a>范例</h2><p>好的，我们来为优化算法 (Optimization Algorithms) 提供更具体的范例，并结合 PyTorch 代码进行说明，让它们更加生动易懂。</p>
<p><strong>一级知识点 4：优化算法 (Optimization Algorithms) ⚙️</strong></p>
<p><strong>二级知识点 4.1：梯度下降 (Gradient Descent) ⛰️</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们要使用梯度下降来解决一个简单的线性回归问题。</p>
<ul>
<li><strong>模型:</strong> <code>y = w * x + b</code></li>
<li><strong>数据集:</strong> 假设我们有一些 (x, y) 数据点，例如：<ul>
<li>(x&#x3D;1, y&#x3D;2)</li>
<li>(x&#x3D;2, y&#x3D;4)</li>
<li>(x&#x3D;3, y&#x3D;5)</li>
<li>(x&#x3D;4, y&#x3D;4)</li>
<li>(x&#x3D;5, y&#x3D;5)</li>
</ul>
</li>
<li><strong>损失函数:</strong> 均方误差 (MSE)：<code>L = (1/n) * Σ(y_pred - y_true)^2</code> (n 是样本数量)</li>
<li><strong>目标:</strong> 找到合适的 <code>w</code> 和 <code>b</code>，使得损失函数 <code>L</code> 最小化。</li>
</ul>
<p><strong>批量梯度下降 (Batch Gradient Descent):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重和偏置</span></span><br><span class="line">w = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播 (自动计算梯度)</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重和偏置 (使用梯度下降)</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w -= lr * w.grad</span><br><span class="line">        b -= lr * b.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清空梯度 (重要！)</span></span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>, w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终的 w 和 b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终的 w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, 最终的 b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>小批量梯度下降 (Mini-batch Gradient Descent):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重和偏置</span></span><br><span class="line">w = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批次大小</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 随机打乱数据</span></span><br><span class="line">    indices = np.arange(<span class="built_in">len</span>(x))</span><br><span class="line">    np.random.shuffle(indices)</span><br><span class="line">    x_shuffled = x[indices]</span><br><span class="line">    y_shuffled = y[indices]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x), batch_size):</span><br><span class="line">        <span class="comment"># 获取当前批次的数据</span></span><br><span class="line">        x_batch = x_shuffled[i:i+batch_size]</span><br><span class="line">        y_batch = y_shuffled[i:i+batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        y_pred = w * x_batch + b</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = torch.mean((y_pred - y_batch)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新权重和偏置</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            w -= lr * w.grad</span><br><span class="line">            b -= lr * b.grad</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            w.grad.zero_()</span><br><span class="line">            b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>, w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终的 w 和 b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终的 w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, 最终的 b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>随机梯度下降 (Stochastic Gradient Descent):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重和偏置</span></span><br><span class="line">w = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代次数 (每个样本都迭代一次)</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">      <span class="comment"># 获取当前样本</span></span><br><span class="line">      x_sample = x[i]</span><br><span class="line">      y_sample = y[i]</span><br><span class="line">      <span class="comment">#前向传播</span></span><br><span class="line">      y_pred = w* x_sample + b</span><br><span class="line"></span><br><span class="line">      <span class="comment">#计算损失</span></span><br><span class="line">      loss = (y_pred - y_sample)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">#反向传播</span></span><br><span class="line">      loss.backward()</span><br><span class="line"></span><br><span class="line">      <span class="comment">#更新权重和偏置</span></span><br><span class="line">      <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w -= lr* w.grad</span><br><span class="line">        b -= lr* b.grad</span><br><span class="line">        <span class="comment">#清空梯度</span></span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终的 w: <span class="subst">&#123;w.item():<span class="number">.4</span>f&#125;</span>, 最终的 b: <span class="subst">&#123;b.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二级知识点 4.2：其他优化器 (Optimizers) ✨</strong></p>
<ul>
<li><p><strong>范例：</strong> 使用 PyTorch 内置的优化器 (Adam)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集 (同上)</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 1，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器 (Adam)</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代次数</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = model(x.unsqueeze(<span class="number">1</span>))  <span class="comment"># unsqueeze(1) 将 x 从 [5] 变为 [5, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred.squeeze() - y)**<span class="number">2</span>)  <span class="comment"># squeeze() 将 y_pred 从 [5, 1] 变为 [5]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数 (使用优化器)</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>, w: <span class="subst">&#123;model.linear.weight.item():<span class="number">.4</span>f&#125;</span>, b: <span class="subst">&#123;model.linear.bias.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终的 w 和 b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终的 w: <span class="subst">&#123;model.linear.weight.item():<span class="number">.4</span>f&#125;</span>, 最终的 b: <span class="subst">&#123;model.linear.bias.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>通过这些范例和代码，应该对梯度下降的不同变体以及如何使用 PyTorch 内置的优化器有了更深入的理解。请注意，实际应用中，我们通常会使用 PyTorch 提供的 <code>nn.Module</code>、<code>nn.Linear</code>、<code>optim</code> 等模块来构建模型和优化器，而不需要手动实现梯度下降的细节。</p>
<p>在下一阶段，我们将学习损失函数, 这是反向传播的”指挥官”！👨‍✈️</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>好的！让我们进入下一个关键环节：损失函数！🎯</p>
<p><strong>一级知识点 5：损失函数 (Loss Functions) 📉</strong></p>
<p>损失函数是神经网络训练的“指挥官”，它告诉神经网络当前的表现如何，以及距离“完美”还有多远。</p>
<ul>
<li><p><strong>二级知识点 5.1：损失函数的定义 📝</strong></p>
<ul>
<li><p>损失函数是一个衡量模型预测输出与真实标签之间差异的函数。</p>
</li>
<li><p>损失函数的值越小，表示模型的预测结果越接近真实标签，模型的性能越好。</p>
</li>
<li><p>损失函数的选择取决于具体的任务类型（例如分类、回归等）。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 5.2：不同任务的损失函数 ➗➖➕</strong></p>
<ul>
<li><p><strong>三级知识点 5.2.1：分类任务 (Classification) 🐱🐶</strong></p>
<ul>
<li><p><strong>交叉熵损失 (Cross-Entropy Loss):</strong> 用于多分类问题，衡量预测概率分布与真实标签分布之间的差异。</p>
<ul>
<li><p><strong>公式 (二分类):</strong> <code>- (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))</code></p>
<ul>
<li><p><code>y_true</code>: 真实标签 (0 或 1)</p>
</li>
<li><p><code>y_pred</code>: 预测概率 (0-1 之间)</p>
</li>
</ul>
</li>
<li><p><strong>公式 (多分类):</strong> <code>- Σ(y_true_i * log(y_pred_i))</code></p>
<ul>
<li><p><code>y_true_i</code>: 真实标签的 one-hot 编码的第 i 个元素</p>
</li>
<li><p><code>y_pred_i</code>: 预测概率分布的第 i 个元素</p>
</li>
</ul>
</li>
<li><p><strong>PyTorch:</strong> <code>torch.nn.CrossEntropyLoss()</code> (多分类，内部会自动计算 Softmax) 或 <code>torch.nn.BCELoss()</code> (二分类，需要手动计算 Sigmoid)</p>
</li>
</ul>
</li>
<li><p><strong>Hinge Loss:</strong> 常用于支持向量机 (SVM)，目标是最大化分类边界。</p>
<ul>
<li><strong>PyTorch:</strong> <code>torch.nn.HingeEmbeddingLoss()</code></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>三级知识点 5.2.2：回归任务 (Regression) 🏠📈</strong></p>
<ul>
<li><p><strong>均方误差 (Mean Squared Error, MSE):</strong> 计算预测值与真实值之间差的平方的平均值。</p>
<ul>
<li><p><strong>公式:</strong> <code>(1/n) * Σ(y_pred - y_true)^2</code></p>
</li>
<li><p><strong>PyTorch:</strong> <code>torch.nn.MSELoss()</code></p>
</li>
</ul>
</li>
<li><p><strong>平均绝对误差 (Mean Absolute Error, MAE):</strong> 计算预测值与真实值之间差的绝对值的平均值。</p>
<ul>
<li><p><strong>公式:</strong> <code>(1/n) * Σ|y_pred - y_true|</code></p>
</li>
<li><p><strong>PyTorch:</strong> <code>torch.nn.L1Loss()</code></p>
</li>
</ul>
</li>
<li><p><strong>Huber Loss:</strong> MSE 和 MAE 的结合，对异常值更鲁棒。</p>
<ul>
<li><strong>PyTorch:</strong> <code>torch.nn.SmoothL1Loss()</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>形象比喻:</strong></p>
<ul>
<li><p><strong>分类任务:</strong> 想象你在玩一个“猜动物”的游戏。</p>
<ul>
<li><strong>交叉熵损失:</strong> 就像猜错的“惩罚”，猜得越离谱，“惩罚”越大。</li>
</ul>
</li>
<li><p><strong>回归任务:</strong> 想象你在玩一个“扔飞镖”的游戏。</p>
<ul>
<li><p><strong>MSE:</strong> 就像计算所有飞镖偏离靶心的距离的平方的平均值。</p>
</li>
<li><p><strong>MAE:</strong> 就像计算所有飞镖偏离靶心的距离的平均值。</p>
</li>
</ul>
</li>
</ul>
<p>损失函数为神经网络提供了学习的目标，反向传播算法利用损失函数的梯度来更新网络参数，优化算法则负责具体的更新过程。</p>
<p>现在，你已经了解了损失函数的作用和常见类型。请思考：</p>
<ul>
<li><p>你能用自己的话解释损失函数的作用吗？</p>
</li>
<li><p>对于分类任务和回归任务，分别应该选择什么样的损失函数？</p>
</li>
</ul>
<h2 id="范例-3"><a href="#范例-3" class="headerlink" title="范例"></a>范例</h2><p>好的，让我们通过一些具体的例子来加深对损失函数的理解，并结合 PyTorch 代码进行演示。</p>
<p><strong>一级知识点 5：损失函数 (Loss Functions) 📉</strong></p>
<p><strong>二级知识点 5.1：损失函数的定义 📝</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们正在训练一个模型来预测房价。</p>
<ul>
<li><strong>模型输出:</strong> 模型的预测房价 (例如：350,000 美元)</li>
<li><strong>真实标签:</strong> 房屋的实际价格 (例如：380,000 美元)</li>
<li><strong>损失函数:</strong> 我们可以使用均方误差 (MSE) 来衡量模型预测的准确性。<ul>
<li><strong>计算:</strong> <code>(350,000 - 380,000)^2 = 900,000,000</code> (这里为了简化，我们只计算了一个样本的损失)</li>
</ul>
</li>
</ul>
<p>这个例子中，损失函数的值 (900,000,000) 越大，表示模型的预测结果与真实价格之间的差距越大，模型的性能越差。</p>
</li>
</ul>
<p><strong>二级知识点 5.2：不同任务的损失函数 ➗➖➕</strong></p>
<ul>
<li><p><strong>三级知识点 5.2.1：分类任务 (Classification) 🐱🐶</strong></p>
<ul>
<li><p><strong>范例：</strong> 我们要构建一个图像分类器，将图像分为猫或狗两类。</p>
<ul>
<li><strong>模型输出:</strong> 模型输出一个包含两个元素的向量，分别表示图像是猫和狗的概率。例如：<code>[0.8, 0.2]</code> (表示模型认为这张图片是猫的概率为 80%，是狗的概率为 20%)</li>
<li><strong>真实标签:</strong><ul>
<li>如果图片是猫，则真实标签为 <code>[1, 0]</code> (one-hot 编码)</li>
<li>如果图片是狗，则真实标签为 <code>[0, 1]</code> (one-hot 编码)</li>
</ul>
</li>
<li><strong>损失函数:</strong> 我们可以使用交叉熵损失 (Cross-Entropy Loss)。</li>
</ul>
<p><strong>PyTorch 代码 (二分类):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出 (假设模型已经输出了预测概率)</span></span><br><span class="line">y_pred = torch.tensor([<span class="number">0.8</span>, <span class="number">0.2</span>])  <span class="comment"># 预测为猫的概率为 0.8，狗的概率为 0.2</span></span><br><span class="line">y_pred = torch.sigmoid(y_pred) <span class="comment"># 需要手动计算 Sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签 (假设这张图片是猫)</span></span><br><span class="line">y_true = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>])  <span class="comment"># one-hot 编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算二元交叉熵损失 (Binary Cross-Entropy Loss)</span></span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line">loss = bce_loss(y_pred, y_true)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;二元交叉熵损失: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>PyTorch 代码 (多分类):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出 (假设模型已经输出了预测概率)</span></span><br><span class="line">y_pred = torch.tensor([[<span class="number">0.8</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, <span class="number">0.7</span>]])  <span class="comment"># 两张图片，第一张预测为猫的概率高，第二张预测为狗的概率高</span></span><br><span class="line"><span class="comment">#y_pred = torch.softmax(y_pred, dim=1)  # nn.CrossEntropyLoss 已经包含了 softmax，所以这里不需要</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签 (假设第一张图片是猫，第二张图片是狗)</span></span><br><span class="line">y_true = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 类别索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交叉熵损失 (Cross-Entropy Loss)</span></span><br><span class="line">ce_loss = nn.CrossEntropyLoss()</span><br><span class="line">loss = ce_loss(y_pred, y_true)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;交叉熵损失: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>三级知识点 5.2.2：回归任务 (Regression) 🏠📈</strong></p>
<ul>
<li><p><strong>范例：</strong> 我们要构建一个模型来预测房价 (与之前的例子相同)。</p>
<ul>
<li><strong>模型输出:</strong> 模型预测的房价 (例如：350,000 美元)</li>
<li><strong>真实标签:</strong> 房屋的实际价格 (例如：380,000 美元)</li>
<li><strong>损失函数:</strong><ul>
<li><strong>均方误差 (MSE):</strong> <code>(350,000 - 380,000)^2 = 900,000,000</code></li>
<li><strong>平均绝对误差 (MAE):</strong> <code>|350,000 - 380,000| = 30,000</code></li>
</ul>
</li>
</ul>
<p><strong>PyTorch 代码 (MSE):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出 (假设模型已经输出了预测值)</span></span><br><span class="line">y_pred = torch.tensor([<span class="number">350000.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = torch.tensor([<span class="number">380000.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差 (MSE)</span></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line">loss = mse_loss(y_pred, y_true)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>PyTorch 代码 (MAE):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出 (假设模型已经输出了预测值)</span></span><br><span class="line">y_pred = torch.tensor([<span class="number">350000.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = torch.tensor([<span class="number">380000.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算平均绝对误差 (MAE)</span></span><br><span class="line">mae_loss = nn.L1Loss()</span><br><span class="line">loss = mae_loss(y_pred, y_true)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;平均绝对误差: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>通过这些范例和代码，你可以看到不同类型的损失函数是如何计算的，以及如何在 PyTorch 中使用它们。在实际应用中，你需要根据你的任务类型选择合适的损失函数。<br>在下一阶段，我们将学习反向传播的一些实际应用！🌟</p>
<h1 id="实际应用（一小部分）"><a href="#实际应用（一小部分）" class="headerlink" title="实际应用（一小部分）"></a>实际应用（一小部分）</h1><p>太好了！接下来，让我们一起探索反向传播在现实世界中的应用，看看它如何大显身手！🌟</p>
<p><strong>一级知识点 6：反向传播的应用 🌍</strong></p>
<p>反向传播算法不仅仅是理论上的概念，它已经在许多领域取得了显著的成果。</p>
<ul>
<li><p><strong>二级知识点 6.1：人脸识别 (Face Recognition) 👤📸</strong></p>
<ul>
<li><p><strong>原理:</strong> 卷积神经网络 (CNN) 结合反向传播算法，可以学习人脸图像的特征，从而实现人脸识别。</p>
</li>
<li><p><strong>过程:</strong></p>
<ol>
<li><p><strong>数据收集:</strong> 收集大量人脸图像，并标注每个图像对应的人物身份。</p>
</li>
<li><p><strong>构建 CNN 模型:</strong> 设计一个 CNN 模型，通常包含多个卷积层、池化层和全连接层。</p>
</li>
<li><p><strong>前向传播:</strong> 将人脸图像输入到 CNN 模型中，进行前向传播，得到模型的预测输出（例如，属于每个人物身份的概率）。</p>
</li>
<li><p><strong>计算损失:</strong> 使用交叉熵损失函数等，计算模型预测输出与真实标签之间的差异。</p>
</li>
<li><p><strong>反向传播:</strong> 使用反向传播算法，计算损失函数相对于模型中每个权重和偏置的梯度。</p>
</li>
<li><p><strong>更新参数:</strong> 使用优化算法（例如 Adam），根据计算出的梯度更新模型的权重和偏置。</p>
</li>
<li><p><strong>重复步骤 3-6:</strong> 不断迭代，直到模型收敛或达到预定的训练轮数。</p>
</li>
</ol>
</li>
<li><p><strong>应用:</strong> 手机解锁、门禁系统、安防监控等。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 6.2：自然语言处理：语音识别 (NLP: Speech Recognition) 🗣️📱</strong></p>
<ul>
<li><p><strong>原理:</strong> 循环神经网络 (RNN) 或 Transformer 模型结合反向传播算法，可以将语音信号转换为文本。</p>
</li>
<li><p><strong>过程:</strong></p>
<ol>
<li><p><strong>数据收集:</strong> 收集大量语音数据，并标注每个语音片段对应的文本内容。</p>
</li>
<li><p><strong>构建模型:</strong> 设计一个 RNN 或 Transformer 模型。</p>
</li>
<li><p><strong>特征提取:</strong> 将语音信号转换为声学特征（例如梅尔频率倒谱系数 MFCC）。</p>
</li>
<li><p><strong>前向传播:</strong> 将声学特征输入到模型中，进行前向传播，得到模型的预测输出（例如，每个时间步的文本预测概率）。</p>
</li>
<li><p><strong>计算损失:</strong> 使用交叉熵损失函数等，计算模型预测输出与真实标签之间的差异。</p>
</li>
<li><p><strong>反向传播:</strong> 使用反向传播算法（例如 BPTT，随时间反向传播），计算损失函数相对于模型中每个权重和偏置的梯度。</p>
</li>
<li><p><strong>更新参数:</strong> 使用优化算法（例如 Adam），根据计算出的梯度更新模型的权重和偏置。</p>
</li>
<li><p><strong>重复步骤 4-7:</strong> 不断迭代，直到模型收敛或达到预定的训练轮数。</p>
</li>
</ol>
</li>
<li><p><strong>应用:</strong> 语音助手（如 Siri、Alexa）、语音输入法、实时字幕等。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 6.3： 事故预防</strong></p>
<ul>
<li><p><strong>原理:</strong> 利用反向传播算法训练的模型能够预测潜在的危险情况，从而采取预防措施，减少事故发生的可能性。</p>
</li>
<li><p><strong>应用</strong>: 通过分析数据来预测矿井事故， 提前采取措施。</p>
</li>
</ul>
</li>
</ul>
<p>反向传播的应用远不止这些，它还广泛应用于图像生成、机器翻译、推荐系统、自动驾驶、医疗诊断、金融预测等众多领域。随着深度学习技术的不断发展，反向传播算法将在更多领域发挥重要作用。</p>
<p>现在，你已经了解了反向传播的一些实际应用。请思考：</p>
<ul>
<li><p>你能举出其他一些反向传播的应用例子吗？</p>
</li>
<li><p>你认为反向传播算法在未来还可能有哪些应用？</p>
</li>
</ul>
<h2 id="范例-4"><a href="#范例-4" class="headerlink" title="范例"></a>范例</h2><p>好的，让我们通过一些具体的例子和代码片段，来展示反向传播在实际应用中的强大功能。</p>
<p><strong>一级知识点 6：反向传播的应用 🌍</strong></p>
<p><strong>二级知识点 6.1：人脸识别 (Face Recognition) 👤📸</strong></p>
<ul>
<li><p><strong>范例：</strong> 使用 PyTorch 和预训练的 CNN 模型 (例如 FaceNet) 进行人脸识别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的 FaceNet 模型 (这里使用 torchvision 提供的 resnet18 作为示例)</span></span><br><span class="line"><span class="comment"># 实际应用中，你可能需要下载并加载更专业的 FaceNet 模型</span></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.fc = torch.nn.Linear(<span class="number">512</span>, <span class="number">128</span>)  <span class="comment"># 修改最后一层，输出 128 维的特征向量</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像预处理步骤</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">160</span>, <span class="number">160</span>)),  <span class="comment"># 调整图像大小</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转换为 Tensor</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载两张人脸图像 (例如，image1.jpg 和 image2.jpg)</span></span><br><span class="line">image1 = Image.<span class="built_in">open</span>(<span class="string">&quot;image1.jpg&quot;</span>)</span><br><span class="line">image2 = Image.<span class="built_in">open</span>(<span class="string">&quot;image2.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图像进行预处理</span></span><br><span class="line">image1_tensor = transform(image1)</span><br><span class="line">image2_tensor = transform(image2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像输入到模型中，获取特征向量</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    feature_vector1 = model(image1_tensor.unsqueeze(<span class="number">0</span>))  <span class="comment"># unsqueeze(0) 增加批次维度</span></span><br><span class="line">    feature_vector2 = model(image2_tensor.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算两个特征向量之间的距离 (例如，欧氏距离)</span></span><br><span class="line">distance = torch.norm(feature_vector1 - feature_vector2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置一个阈值，判断两张图片是否属于同一个人</span></span><br><span class="line">threshold = <span class="number">1.0</span>  <span class="comment"># 这个阈值需要根据实际情况调整</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> distance &lt; threshold:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;这两张图片可能是同一个人。&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;这两张图片可能不是同一个人。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;两张图片特征向量之间的距离: <span class="subst">&#123;distance.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>代码解释:</strong></p>
<ol>
<li><strong>加载预训练模型:</strong> 我们使用 <code>torchvision</code> 提供的预训练 <code>resnet18</code> 模型作为示例。实际应用中，你可能需要下载并加载更专业的 FaceNet 模型，例如在 VGGFace2 数据集上预训练的模型。</li>
<li><strong>修改最后一层:</strong> 我们将 <code>resnet18</code> 模型的最后一层 (全连接层) 修改为输出 128 维的特征向量。这是 FaceNet 模型常用的特征向量维度。</li>
<li><strong>图像预处理:</strong> 我们定义了一系列图像预处理步骤，包括调整图像大小、转换为 Tensor 和标准化。这些步骤对于提高模型的性能非常重要。</li>
<li><strong>特征提取:</strong> 我们将预处理后的图像输入到模型中，获取 128 维的特征向量。</li>
<li><strong>距离计算:</strong> 我们计算两个特征向量之间的欧氏距离。距离越小，表示两张人脸图像越相似。</li>
<li><strong>阈值判断:</strong> 我们设置一个阈值，如果距离小于阈值，则认为两张图片属于同一个人。</li>
</ol>
</li>
</ul>
<p><strong>二级知识点 6.2：自然语言处理：语音识别 (NLP: Speech Recognition) 🗣️📱</strong></p>
<ul>
<li><p><strong>范例：</strong> 使用 PyTorch 和预训练的语音识别模型 (例如 Wav2Vec2) 进行语音识别。由于从头训练一个语音识别模型非常复杂，并且需要大量的计算资源，一般情况下我们都会使用预训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Wav2Vec2ForCTC, Wav2Vec2Processor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的 Wav2Vec2 模型和处理器</span></span><br><span class="line">processor = Wav2Vec2Processor.from_pretrained(<span class="string">&quot;facebook/wav2vec2-base-960h&quot;</span>)</span><br><span class="line">model = Wav2Vec2ForCTC.from_pretrained(<span class="string">&quot;facebook/wav2vec2-base-960h&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载音频文件 (例如，audio.wav，需要是单声道、16kHz 采样率的音频)</span></span><br><span class="line"><span class="comment"># 这里我们使用 torchaudio 来加载音频，你需要先安装 torchaudio：pip install torchaudio</span></span><br><span class="line"><span class="keyword">import</span> torchaudio</span><br><span class="line">waveform, sample_rate = torchaudio.load(<span class="string">&quot;audio.wav&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果音频不是 16kHz 采样率，进行重采样</span></span><br><span class="line"><span class="keyword">if</span> sample_rate != <span class="number">16000</span>:</span><br><span class="line">    waveform = torchaudio.functional.resample(waveform, sample_rate, <span class="number">16000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将音频数据转换为模型所需的输入格式</span></span><br><span class="line">input_values = processor(waveform, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="string">&quot;longest&quot;</span>).input_values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将音频数据输入到模型中，获取 logits</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    logits = model(input_values).logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对 logits 进行解码，获取预测的文本</span></span><br><span class="line">predicted_ids = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">transcription = processor.batch_decode(predicted_ids)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;语音识别结果: <span class="subst">&#123;transcription&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>代码解释:</strong></p>
<ol>
<li><strong>加载预训练模型和处理器:</strong> 我们使用 Hugging Face Transformers 库提供的 <code>Wav2Vec2ForCTC</code> 和 <code>Wav2Vec2Processor</code>。<code>Wav2Vec2ForCTC</code> 是一个预训练的语音识别模型，<code>Wav2Vec2Processor</code> 用于将音频数据转换为模型所需的输入格式。</li>
<li><strong>加载音频文件:</strong> 我们使用 <code>torchaudio</code> 库加载音频文件。你需要确保音频文件是单声道、16kHz 采样率的。如果不是，需要进行重采样。</li>
<li><strong>数据预处理:</strong> 我们使用 <code>processor</code> 将音频数据转换为模型所需的输入格式。</li>
<li><strong>模型预测:</strong> 我们将预处理后的音频数据输入到模型中，获取 <code>logits</code>。<code>logits</code> 是模型输出的原始值，还没有经过 Softmax 转换。</li>
<li><strong>解码:</strong> 我们使用 <code>torch.argmax</code> 对 <code>logits</code> 进行解码，获取预测的文本 ID。然后，我们使用 <code>processor.batch_decode</code> 将文本 ID 转换为文本。</li>
</ol>
</li>
</ul>
<p>这些范例展示了反向传播在人脸识别和语音识别中的应用。请注意，这些只是简化的示例，实际应用中可能需要更复杂的模型、更多的数据预处理步骤以及更精细的调参。</p>
<h1 id="反向传播📣的历史"><a href="#反向传播📣的历史" class="headerlink" title="反向传播📣的历史"></a>反向传播📣的历史</h1><p>说到这里，那就顺带看看，一起回顾反向传播算法的发展历程，了解这段精彩的历史！📜</p>
<p><strong>一级知识点 7：反向传播的历史 🕰️</strong></p>
<p>反向传播算法并非一蹴而就，它的发展历经了数十年的时间，凝聚了许多研究者的智慧和努力。</p>
<ul>
<li><p><strong>19 世纪：梯度下降法的雏形 📉</strong></p>
<ul>
<li><strong>Baron Augustin-Louis Cauchy (法国数学家):</strong> 提出了梯度下降法 (Gradient Descent) 的思想，用于解决复杂的数学问题。这为反向传播算法奠定了基础。</li>
</ul>
</li>
<li><p><strong>1970 年：反向传播的早期探索 🌱</strong></p>
<ul>
<li><strong>Seppo Linnainmaa (芬兰硕士生):</strong> 提出了一种用于稀疏连接网络的误差反向传播算法。虽然他没有明确提到神经网络，但他的工作为反向传播算法的提出奠定了基础。</li>
</ul>
</li>
<li><p><strong>20 世纪 80 年代：反向传播的突破 🚀</strong></p>
<ul>
<li><p><strong>多位研究者独立研究:</strong> 独立开发了时间反向传播 (Backpropagation Through Time, BPTT) 算法，用于训练循环神经网络 (RNN)。</p>
</li>
<li><p><strong>1986 年：David Rumelhart 及其同事:</strong> 发表了一篇具有里程碑意义的论文，将 Linnainmaa 的反向传播算法应用于多层神经网络，并证明了其有效性。这篇论文被认为是反向传播算法的正式提出，极大地推动了神经网络的发展。</p>
</li>
<li><p><strong>1989 年：Yann LeCun:</strong> 将反向传播算法应用于卷积神经网络 (CNN)，用于手写数字识别，取得了显著的成果。</p>
</li>
</ul>
</li>
<li><p><strong>21 世纪：反向传播的广泛应用和发展 🌐</strong></p>
<ul>
<li><p>随着深度学习的兴起，反向传播算法成为训练各种类型神经网络的核心算法。</p>
</li>
<li><p>研究者们不断对反向传播算法进行改进和优化，例如：</p>
<ul>
<li><p><strong>并行化计算:</strong> 利用 GPU 等硬件加速反向传播的计算速度。</p>
</li>
<li><p><strong>优化算法:</strong> 提出各种新的优化算法（如 Adam、RMSprop 等），以提高训练效率和模型性能。</p>
</li>
<li><p><strong>梯度消失&#x2F;爆炸问题:</strong> 提出各种解决方案（如 ReLU 激活函数、Batch Normalization、残差连接等），以缓解梯度消失&#x2F;爆炸问题。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>反向传播算法的发展历程是一个不断探索、不断创新的过程。正是由于这些研究者的努力，我们才能够利用深度学习技术解决各种复杂的问题。</p>
<p>现在，你已经了解了反向传播算法的历史。请思考：</p>
<ul>
<li><p>反向传播算法的发展历程中，有哪些关键的里程碑事件？</p>
</li>
<li><p>反向传播算法的提出对神经网络的发展产生了什么影响？</p>
</li>
</ul>
<p>我们还剩下最后一个知识点了：超参数！</p>
<h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><p>好的！让我们来了解神经网络中的“神秘”参数——超参数！🧙‍♂️</p>
<p><strong>一级知识点 8：超参数 (Hyperparameters) ⚙️</strong></p>
<p>超参数是神经网络中非常重要的概念。它们不是通过训练数据学习得到的，而是在训练之前由人工设置的参数。超参数的选择对模型的性能有很大的影响。</p>
<ul>
<li><p><strong>二级知识点 8.1：输入图像大小 (Input Image Size) 🖼️</strong></p>
<ul>
<li><p><strong>定义:</strong> 输入到神经网络的图像的尺寸（宽度和高度）。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>较大的图像通常包含更多的细节信息，可能有助于提高模型的性能，但也会增加计算量和内存消耗。</p>
</li>
<li><p>较小的图像计算量较小，训练速度更快，但可能会丢失一些细节信息。</p>
</li>
</ul>
</li>
<li><p><strong>选择:</strong> 需要根据具体的任务和数据集进行权衡。通常需要进行实验来确定最佳的输入图像大小。</p>
</li>
<li><p><strong>示例:</strong> 对于 ImageNet 数据集，常用的输入图像大小为 224x224 或 299x299。对于 MNIST 数据集，输入图像大小为 28x28。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.2：学习率 (Learning Rate) 🏃</strong></p>
<ul>
<li><p><strong>定义:</strong> 控制权重更新的步长。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>学习率过大可能导致模型在最小值附近震荡，甚至无法收敛。</p>
</li>
<li><p>学习率过小可能导致模型收敛速度过慢，需要更长的训练时间。</p>
</li>
</ul>
</li>
<li><p><strong>选择:</strong> 通常需要通过实验来确定最佳的学习率。可以使用学习率衰减策略，在训练过程中逐渐减小学习率。</p>
</li>
<li><p><strong>示例:</strong> 常用的学习率范围为 0.1 到 0.0001。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.3：正则化参数 (Regularization Parameter) 🏋️</strong></p>
<ul>
<li><p><strong>定义:</strong> 用于控制模型复杂度，防止过拟合。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>正则化参数越大，对模型复杂度的惩罚越大，模型越倾向于选择更简单的模型。</p>
</li>
<li><p>正则化参数越小，对模型复杂度的惩罚越小，模型可能更容易过拟合。</p>
</li>
</ul>
</li>
<li><p><strong>选择:</strong> 需要根据具体的任务和数据集进行权衡。通常需要进行实验来确定最佳的正则化参数。</p>
</li>
<li><p><strong>示例:</strong> 常用的正则化方法包括 L1 正则化和 L2 正则化。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.4：神经网络层数 (Number of Layers) 🧱</strong></p>
<ul>
<li><p><strong>定义:</strong> 神经网络中隐藏层的数量。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>层数越多，模型的表示能力越强，但也越容易过拟合。</p>
</li>
<li><p>层数越少，模型的表示能力越弱，可能无法很好地拟合数据。</p>
</li>
</ul>
</li>
<li><p><strong>选择:</strong> 需要根据具体的任务和数据集进行权衡。通常需要进行实验来确定最佳的层数。</p>
</li>
<li><p><strong>示例:</strong> 对于简单的任务，可以使用较少的层数（例如 1-2 层）。对于复杂的任务，可能需要使用较多的层数（例如几十层甚至上百层）。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.5：批处理大小 (Batch Size) 📦</strong></p>
<ul>
<li><p><strong>定义:</strong> 每次迭代中使用的训练样本数量。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>批处理大小越大，梯度估计越准确，训练越稳定，但内存消耗越大，每次迭代的时间越长。</p>
</li>
<li><p>批处理大小越小，内存消耗越小，每次迭代的时间越短，但梯度估计可能不够准确，训练可能不稳定。</p>
</li>
</ul>
</li>
<li><p><strong>选择:</strong> 需要根据具体的硬件条件和数据集大小进行权衡。通常需要进行实验来确定最佳的批处理大小。</p>
</li>
<li><p><strong>示例:</strong> 常用的批处理大小范围为 32 到 1024。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.6：卷积层参数 (Convolutional Layer Parameters) 🧱</strong></p>
<ul>
<li><p><strong>卷积核大小 (Kernel Size):</strong> 卷积核的尺寸（例如 3x3、5x5）。</p>
</li>
<li><p><strong>步长 (Stride):</strong> 卷积核在图像上移动的步长（例如 1、2）。</p>
</li>
<li><p><strong>填充 (Padding):</strong> 在图像边缘填充像素的方式（例如 “valid”、“same”）。</p>
</li>
<li><p><strong>卷积核数量 (Number of Filters):</strong> 卷积层中卷积核的数量。</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.7：池化层参数 (Pooling Layer Parameters) 🧱</strong></p>
<ul>
<li><p><strong>池化核大小 (Kernel Size):</strong> 池化核的尺寸（例如 2x2、3x3）。</p>
</li>
<li><p><strong>步长 (Stride):</strong> 池化核移动的步长（例如 2、3）。</p>
</li>
<li><p><strong>池化类型</strong>: 最大池化， 均值池化等</p>
</li>
</ul>
</li>
<li><p><strong>二级知识点 8.8：迭代周期 (Epochs) 🔁</strong></p>
<ul>
<li><p><strong>定义:</strong> 将所有训练数据过一遍叫做一个周期。</p>
</li>
<li><p><strong>影响:</strong></p>
<ul>
<li><p>周期数越多， 模型训练时间越长， 容易过拟合。</p>
</li>
<li><p>周期数越少， 模型可能无法完全学习到数据中的规律</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="范例-5"><a href="#范例-5" class="headerlink" title="范例"></a>范例</h1><p>好的，我们来为超参数提供一些更具体的范例，并结合 PyTorch 代码进行说明。</p>
<p><strong>一级知识点 8：超参数 (Hyperparameters) ⚙️</strong></p>
<p><strong>二级知识点 8.1：输入图像大小 (Input Image Size) 🖼️</strong></p>
<ul>
<li><p><strong>范例：</strong> 假设我们要训练一个图像分类模型，使用 CIFAR-10 数据集。CIFAR-10 数据集中的图像大小为 32x32 像素。</p>
<ul>
<li><strong>情况 1：</strong> 我们将输入图像大小设置为 32x32（原始大小）。<ul>
<li><strong>优点:</strong> 不需要对图像进行额外的缩放操作，保留了原始图像的所有信息。</li>
<li><strong>缺点:</strong> 如果模型比较复杂，计算量可能会比较大。</li>
</ul>
</li>
<li><strong>情况 2：</strong> 我们将输入图像大小设置为 64x64。<ul>
<li><strong>优点:</strong> 可能会提高模型的性能（如果模型能够学习到更精细的特征）。</li>
<li><strong>缺点:</strong> 增加了计算量和内存消耗。需要对原始图像进行上采样操作，可能会引入一些噪声。</li>
</ul>
</li>
<li><strong>情况 3：</strong> 我们将输入图像大小设置为 16x16。<ul>
<li><strong>优点:</strong> 减少了计算量和内存消耗。</li>
<li><strong>缺点:</strong> 可能会降低模型的性能（丢失了一些细节信息）。需要对原始图像进行下采样操作，可能会导致信息损失。</li>
</ul>
</li>
</ul>
<p><strong>PyTorch 代码 (调整输入图像大小):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像预处理步骤</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">64</span>, <span class="number">64</span>)),  <span class="comment"># 将图像大小调整为 64x64</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 CIFAR-10 数据集，并应用预处理步骤</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>二级知识点 8.2：学习率 (Learning Rate) 🏃</strong></p>
<ul>
<li><p><strong>范例：</strong> 我们使用 PyTorch 构建一个简单的线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (省略数据加载和模型定义部分，参考之前的线性回归例子) ...</span></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 1，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment"># 情况 1：学习率设置为 0.1 (较大)</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 2：学习率设置为 0.01 (适中)</span></span><br><span class="line"><span class="comment"># optimizer = optim.SGD(model.parameters(), lr=0.01)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 3：学习率设置为 0.001 (较小)</span></span><br><span class="line"><span class="comment"># optimizer = optim.SGD(model.parameters(), lr=0.001)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (省略训练循环部分，参考之前的线性回归例子) ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>情况 1 (lr&#x3D;0.1):</strong> 模型可能会在最小值附近震荡，甚至无法收敛。</li>
<li><strong>情况 2 (lr&#x3D;0.01):</strong> 模型可能会比较平稳地收敛到最小值。</li>
<li><strong>情况 3 (lr&#x3D;0.001):</strong> 模型可能会收敛得很慢，需要更长的训练时间。</li>
</ul>
</li>
</ul>
<p><strong>二级知识点 8.3：正则化参数 (Regularization Parameter) 🏋️</strong></p>
<ul>
<li><p><strong>范例：</strong> 我们在 PyTorch 中构建一个带有 L2 正则化的线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (省略数据加载和模型定义部分) ...</span></span><br><span class="line"> <span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 1，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment"># 定义优化器，并设置 L2 正则化参数 (weight_decay)</span></span><br><span class="line"><span class="comment"># 情况 1：正则化参数设置为 0.1 (较大)</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 2：正则化参数设置为 0.01 (适中)</span></span><br><span class="line"><span class="comment"># optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 3：正则化参数设置为 0 (无正则化)</span></span><br><span class="line"><span class="comment"># optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (省略训练循环部分) ...</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>情况 1 (weight_decay&#x3D;0.1):</strong> 模型更倾向于选择较小的权重，有助于防止过拟合。</li>
<li><strong>情况 2 (weight_decay&#x3D;0.01):</strong> 正则化的效果适中。</li>
<li><strong>情况 3 (weight_decay&#x3D;0):</strong> 没有正则化，模型可能更容易过拟合。</li>
</ul>
</li>
</ul>
<p><strong>二级知识点 8.4：神经网络层数 (Number of Layers) 🧱</strong></p>
<ul>
<li><strong>范例：</strong> 我们使用 PyTorch 构建不同层数的 MLP。</li>
<li><strong>二级知识点 8.5：批处理大小</strong></li>
<li><strong>二级知识点 8.6： 卷积层参数</strong></li>
<li><strong>二级知识点 8.7： 池化层参数</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 1：单层 MLP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 2：两层 MLP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.linear1(x))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear2(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况 3：三层 MLP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.relu1 = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.relu2 = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.linear3 = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.relu1(<span class="variable language_">self</span>.linear1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.relu2(<span class="variable language_">self</span>.linear2(x))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear3(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情况4： 含有卷积层和池化层的神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>) <span class="comment"># 卷积层</span></span><br><span class="line">      <span class="variable language_">self</span>.relu1 = nn.ReLU() <span class="comment">#激活函数</span></span><br><span class="line">      <span class="variable language_">self</span>.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>) <span class="comment">#最大池化层</span></span><br><span class="line">      <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">      <span class="variable language_">self</span>.relu2 = nn.ReLU()</span><br><span class="line">      <span class="variable language_">self</span>.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">      <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">      <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">32</span>*<span class="number">8</span>*<span class="number">8</span>, <span class="number">10</span>) <span class="comment">#全连接层</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      x = <span class="variable language_">self</span>.relu1(<span class="variable language_">self</span>.conv1(x)) <span class="comment"># [batch_size, 3, 32, 32] -&gt; [batch_size, 16, 32, 32]</span></span><br><span class="line">      x = <span class="variable language_">self</span>.pool1(x) <span class="comment"># [batch_size, 16, 32, 32] -&gt; [batch_size, 16, 16, 16]</span></span><br><span class="line">      x = <span class="variable language_">self</span>.relu2(<span class="variable language_">self</span>.conv2(x)) <span class="comment"># [batch_size, 16, 16, 16] -&gt; [batch_size, 32, 16, 16]</span></span><br><span class="line">      x = <span class="variable language_">self</span>.pool2(x) <span class="comment"># [batch_size, 32, 16, 16] -&gt; [batch_size, 32, 8, 8]</span></span><br><span class="line">      x = <span class="variable language_">self</span>.flatten(x) <span class="comment"># [batch_size, 32, 8, 8] -&gt; [batch_size, 32*8*8]</span></span><br><span class="line">      x = <span class="variable language_">self</span>.fc(x) <span class="comment"># [batch_size, 32*8*8] -&gt; [batch_size, 10]</span></span><br><span class="line">      <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>情况 1 (MLP1):</strong> 模型比较简单，可能无法很好地拟合复杂的数据。</li>
<li><strong>情况 2 (MLP2):</strong> 模型的表示能力有所增强。</li>
<li><strong>情况 3 (MLP3):</strong> 模型的表示能力更强，但也更容易过拟合。</li>
<li><strong>情况4：</strong><br>*  卷积核大小为 3x3， 步长为1， 填充为1<br>*  池化核大小为 2x2, 步长为2</li>
</ul>
</li>
</ul>
<p><strong>二级知识点 8.8：迭代周期 (Epochs) 🔁</strong></p>
<p>假设其他参数不变的情况下， 神经网络训练5个周期和500个周期，最终模型的效果可能会有显著差别。</p>
<p>这些范例展示了不同超参数对模型的影响。在实际应用中，你需要根据具体的任务、数据集和硬件条件，通过实验来确定最佳的超参数组合。通常会使用网格搜索、随机搜索或贝叶斯优化等方法来进行超参数调优。</p>
<hr>
<p>为了让疯🉐更全面，我将为你提供一个综合性的代码示例，涵盖以下内容：</p>
<p>注意：⚠️ 这只是一个很简单 ⚠️ 的一个架构示例代码</p>
<ol>
<li><strong>构建一个简单的多层感知器 (MLP) 模型 (PyTorch)</strong></li>
<li><strong>使用模拟数据进行训练</strong></li>
<li><strong>展示前向传播、反向传播、损失函数计算和优化器更新的过程</strong></li>
<li><strong>包含对关键步骤的注释</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义 MLP 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_size, hidden_size)  <span class="comment"># 全连接层 1</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()  <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_size, output_size)  <span class="comment"># 全连接层 2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.fc1(x)  <span class="comment"># 第一层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)  <span class="comment"># ReLU 激活</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fc2(out)  <span class="comment"># 第二层</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 准备数据 (模拟数据)</span></span><br><span class="line">input_size = <span class="number">10</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机输入数据 (模拟 100 个样本)</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, input_size)</span><br><span class="line"><span class="comment"># 生成随机标签 (二分类问题，0 或 1)</span></span><br><span class="line">y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,)).long() <span class="comment">#需要时long 类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义模型、损失函数和优化器</span></span><br><span class="line">hidden_size = <span class="number">64</span></span><br><span class="line">model = MLP(input_size, hidden_size, output_size)</span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数 (适用于多分类问题)</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># Adam 优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 训练模型</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 将数据分成批次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>, batch_size):</span><br><span class="line">        <span class="comment"># 获取当前批次的输入和标签</span></span><br><span class="line">        x_batch = x[i:i+batch_size]</span><br><span class="line">        y_batch = y[i:i+batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = model(x_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs, y_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播 (计算梯度)</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数 (使用优化器)</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清空梯度 (重要！)</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印每个 epoch 的损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练完成!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. (可选) 在测试集上评估模型</span></span><br><span class="line"><span class="comment"># ... (这里省略了测试集的代码，你需要准备一个测试集并进行类似的前向传播和损失计算) ...</span></span><br></pre></td></tr></table></figure>

<p><strong>代码解释:</strong></p>
<ul>
<li><strong>模型定义 (MLP 类):</strong><ul>
<li><code>__init__</code>: 定义了模型的结构，包括两个全连接层 (fc1 和 fc2) 和一个 ReLU 激活函数。</li>
<li><code>forward</code>: 定义了前向传播的过程，数据依次经过 fc1、ReLU 和 fc2。</li>
</ul>
</li>
<li><strong>数据准备:</strong><ul>
<li><code>x</code>: 模拟的输入数据，形状为 (100, 10)，表示 100 个样本，每个样本有 10 个特征。</li>
<li><code>y</code>: 模拟的标签，形状为 (100,)，表示 100 个样本的类别标签 (0 或 1)。</li>
</ul>
</li>
<li><strong>模型、损失函数和优化器:</strong><ul>
<li><code>model</code>: 创建 MLP 模型实例。</li>
<li><code>criterion</code>: 使用交叉熵损失函数 (CrossEntropyLoss)，适用于多分类问题（这里我们是二分类，但 CrossEntropyLoss 也可以用）。</li>
<li><code>optimizer</code>: 使用 Adam 优化器，并传入模型的参数和学习率。</li>
</ul>
</li>
<li><strong>训练循环:</strong><ul>
<li><code>epochs</code>: 训练的轮数。</li>
<li><strong>批次划分:</strong> 将数据分成多个批次，每次迭代只使用一个批次的数据。</li>
<li><strong>前向传播:</strong> <code>outputs = model(x_batch)</code>，将输入数据传入模型，得到输出。</li>
<li><strong>计算损失:</strong> <code>loss = criterion(outputs, y_batch)</code>，计算模型输出与真实标签之间的损失。</li>
<li><strong>反向传播:</strong> <code>loss.backward()</code>，自动计算损失函数相对于模型参数的梯度。</li>
<li><strong>更新参数:</strong> <code>optimizer.step()</code>，使用优化器根据梯度更新模型的参数。</li>
<li><strong>清空梯度:</strong> <code>optimizer.zero_grad()</code>，在每次更新参数后，需要清空梯度，否则梯度会累积。</li>
</ul>
</li>
<li><strong>代码涵盖了所有之前的知识点范例</strong></li>
</ul>
<p>这个代码示例展示了一个完整的神经网络训练流程，包括模型定义、数据准备、前向传播、反向传播、损失函数计算和优化器更新。你可以运行这段代码，观察模型的训练过程和损失的变化。</p>
<p>请注意，这只是一个非常基础的示例。在实际应用中，你可能需要处理更复杂的数据集、构建更复杂的模型、调整更多的超参数，并使用更高级的技术来提高模型的性能。</p>
<p>恭喜你！你已经完成了所有知识点的学习！🎉🎉🎉 你现在对神经网络、反向传播算法以及相关的概念有了更深入的了解。也正式开始风（疯）口浪尖般的生活了。如果你有任何问题，或者想进一步探索某个方面，请随时告诉我！😊</p>
<p><strong>免责声明</strong></p>
<p>本报告（“一篇“<strong>神经网络中的反向传播</strong>”引发的学习血案”）由[ViniJack.SJX] 根据公开可获得的信息以及作者的专业知识和经验撰写，旨在提供关于原理、技术、相关框架和工具的分析和信息。</p>
<p><strong>1. 信息准确性与完整性：</strong></p>
<ul>
<li><p>作者已尽最大努力确保报告中信息的准确性和完整性，但不对其绝对准确性、完整性或及时性做出任何明示或暗示的保证。</p>
</li>
<li><p>报告中的信息可能随时间推移而发生变化，作者不承担更新报告内容的义务。</p>
</li>
<li><p>报告中引用的第三方信息（包括但不限于网站链接、项目描述、数据统计等）均来自公开渠道，作者不对其真实性、准确性或合法性负责。</p>
</li>
</ul>
<p><strong>2. 报告用途与责任限制：</strong></p>
<ul>
<li><p>本报告仅供参考和学习之用，不构成任何形式的投资建议、技术建议、法律建议或其他专业建议。</p>
</li>
<li><p>读者应自行判断和评估报告中的信息，并根据自身情况做出决策。</p>
</li>
<li><p>对于因使用或依赖本报告中的信息而导致的任何直接或间接损失、损害或不利后果，作者不承担任何责任。</p>
</li>
</ul>
<p><strong>3. 技术使用与合规性：</strong></p>
<ul>
<li><p>本报告中提及的任何爬虫框架、工具或技术，读者应自行负责其合法合规使用。</p>
</li>
<li><p>在使用任何爬虫技术时，读者应遵守相关法律法规（包括但不限于数据隐私保护法、知识产权法、网络安全法等），尊重网站的服务条款和robots协议，不得侵犯他人合法权益。</p>
</li>
<li><p>对于因读者违反相关法律法规或不当使用爬虫技术而导致的任何法律责任或纠纷，作者不承担任何责任。</p>
</li>
</ul>
<p><strong>4. 知识产权：</strong></p>
<ul>
<li><p>本报告的版权归作者所有，未经作者书面许可，任何人不得以任何形式复制、传播、修改或使用本报告的全部或部分内容。</p>
</li>
<li><p>报告中引用的第三方内容，其知识产权归原作者所有。</p>
</li>
</ul>
<p><strong>5. 其他：</strong></p>
<ul>
<li><p>本报告可能包含对未来趋势的预测，这些预测基于作者的判断和假设，不构成任何形式的保证。</p>
</li>
<li><p>作者保留随时修改本免责声明的权利。</p>
</li>
</ul>
<p><strong>请在使用本报告前仔细阅读并理解本免责声明。如果您不同意本免责声明的任何条款，请勿使用本报告。</strong></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>一篇“**神经网络中的反向传播**”引发的学习血案</p><p><a href="http://acorner.ac.cn/2025/02/25/一篇“**神经网络中的反向传播**”引发的学习血案/">http://acorner.ac.cn/2025/02/25/一篇“**神经网络中的反向传播**”引发的学习血案/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ViniJack.SJX</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-02-25</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-02-25</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/LLM/">LLM</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%8E%9F%E7%90%86/">原理</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%BE%AE%E8%B0%83/">微调</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="link-muted mr-2" rel="tag" href="/tags/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/">阅读报告</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/02/25/%E5%A4%8D%E6%9D%82%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%9AChain-of-Thought%20vs%20Tree-of-Thought%20%E8%8C%83%E5%BC%8F%E5%AF%B9%E6%AF%94/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">复杂提示工程：Chain-of-Thought vs Tree-of-Thought 范式对比</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/02/21/Michael%20Luo%20....%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"><span class="level-item">关于“DeepScaleR：通过扩展强化学习，用1.5B模型超越O1-Preview”（译文）阅读报告</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="A-Corner"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">A-Corner</p><p class="is-size-6 is-block">信息的一角</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>China（Guangzhou）</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/A-Corner/a-corner.github.io" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Bert/"><span class="level-start"><span class="level-item">Bert</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Model/"><span class="level-start"><span class="level-item">Model</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/RAG/"><span class="level-start"><span class="level-item">RAG</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/"><span class="level-start"><span class="level-item">分析报告</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8E%9F%E7%90%86/"><span class="level-start"><span class="level-item">原理</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">向量数据库</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"><span class="level-start"><span class="level-item">对比分析</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%B7%A5%E5%85%B7/"><span class="level-start"><span class="level-item">工具</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">强化学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BE%AE%E8%B0%83/"><span class="level-start"><span class="level-item">微调</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/"><span class="level-start"><span class="level-item">提示工程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A1%86%E6%9E%B6/"><span class="level-start"><span class="level-item">框架</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"><span class="level-start"><span class="level-item">阅读报告</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="level-start"><span class="level-item">食用文档</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-28T07:08:16.614Z">2025-02-28</time></p><p class="title"><a href="/2025/02/28/Bert%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%AC%E8%B4%A8/">Bert模型的本质</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/">对比分析</a> / <a href="/categories/Bert/">Bert</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-25T05:51:16.647Z">2025-02-25</time></p><p class="title"><a href="/2025/02/25/%E5%A4%8D%E6%9D%82%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%9AChain-of-Thought%20vs%20Tree-of-Thought%20%E8%8C%83%E5%BC%8F%E5%AF%B9%E6%AF%94/">复杂提示工程：Chain-of-Thought vs Tree-of-Thought 范式对比</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/">对比分析</a> / <a href="/categories/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/">提示工程</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-25T03:20:54.429Z">2025-02-25</time></p><p class="title"><a href="/2025/02/25/%E4%B8%80%E7%AF%87%E2%80%9C**%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD**%E2%80%9D%E5%BC%95%E5%8F%91%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A1%80%E6%A1%88/">一篇“**神经网络中的反向传播**”引发的学习血案</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E5%BE%AE%E8%B0%83/">微调</a> / <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a> / <a href="/categories/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/">阅读报告</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-21T03:26:59.380Z">2025-02-21</time></p><p class="title"><a href="/2025/02/21/Michael%20Luo%20....%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/">关于“DeepScaleR：通过扩展强化学习，用1.5B模型超越O1-Preview”（译文）阅读报告</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/">分析报告</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E5%BE%AE%E8%B0%83/">微调</a> / <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a> / <a href="/categories/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/">阅读报告</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-20T06:44:10.490Z">2025-02-20</time></p><p class="title"><a href="/2025/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/">深度学习数学基础入门</a></p><p class="categories"><a href="/categories/LLM/">LLM</a> / <a href="/categories/%E5%8E%9F%E7%90%86/">原理</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert/"><span class="tag">Bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model/"><span class="tag">Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RAG/"><span class="tag">RAG</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%A5%E9%97%A8/"><span class="tag">入门</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/"><span class="tag">分析报告</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8E%9F%E7%90%86/"><span class="tag">原理</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">向量数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"><span class="tag">对比分析</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B7%A5%E5%85%B7/"><span class="tag">工具</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="tag">强化学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%B0%83/"><span class="tag">微调</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/"><span class="tag">提示工程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A1%86%E6%9E%B6/"><span class="tag">框架</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87/"><span class="tag">概率</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"><span class="tag">自动化</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"><span class="tag">阅读报告</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A3%9F%E7%94%A8%E6%96%87%E6%A1%A3/"><span class="tag">食用文档</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/%E4%B8%80%E8%A7%92logo.png" alt="A-Acorner 信息的一角" height="28"></a><p class="is-size-7"><span>&copy; 2025 ViniJack.SJX</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2025 A-Corner</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/A-Corner/a-corner.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>